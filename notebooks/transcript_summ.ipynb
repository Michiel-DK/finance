{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "from chromadb.config import Settings\n",
    "\n",
    "chroma_client = chromadb.HttpClient(host='localhost', port = 8083, settings=Settings(allow_reset=True, anonymized_telemetry=False))\n",
    "\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(name=\"transcripts_mililm_l6_v3\", embedding_function=sentence_transformer_ef)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query one document\n",
    "results = collection.query(\n",
    "    query_texts=[\"Tesla transcripts for the year 2024\"],\n",
    "    n_results=10,\n",
    "    where = {'$and':[\n",
    "              {'symbol': {\n",
    "                       \"$in\": ['TSLA']}\n",
    "              }, \n",
    "              {'year': {\n",
    "                        \"$gt\": 2023}\n",
    "              }]\n",
    "         },\n",
    "    include=['metadatas', 'documents', 'embeddings'],\n",
    "    where_document={\"$contains\":\"2024\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['2024Q2TSLA', '2024Q1TSLA']],\n",
       " 'distances': None,\n",
       " 'embeddings': [[[-0.0822194516658783,\n",
       "    0.009658602997660637,\n",
       "    0.052224915474653244,\n",
       "    0.02644718624651432,\n",
       "    0.04375709965825081,\n",
       "    -0.012698511593043804,\n",
       "    -0.000993350287899375,\n",
       "    0.07763923704624176,\n",
       "    0.046971507370471954,\n",
       "    -0.026099272072315216,\n",
       "    0.003186175599694252,\n",
       "    0.0008456492214463651,\n",
       "    0.04624375328421593,\n",
       "    -0.07265063375234604,\n",
       "    0.011303565464913845,\n",
       "    0.02751617692410946,\n",
       "    0.0003328380116727203,\n",
       "    -0.1211201548576355,\n",
       "    -0.06751690059900284,\n",
       "    0.05124247446656227,\n",
       "    -0.03266194090247154,\n",
       "    -0.013144985772669315,\n",
       "    -0.0125283133238554,\n",
       "    0.002807856537401676,\n",
       "    0.04040118679404259,\n",
       "    -0.01640661247074604,\n",
       "    -0.03554452955722809,\n",
       "    0.05132472142577171,\n",
       "    -0.026458900421857834,\n",
       "    -0.032147157937288284,\n",
       "    -0.019565070047974586,\n",
       "    0.045829273760318756,\n",
       "    0.03245055675506592,\n",
       "    0.0004907945403829217,\n",
       "    -0.02981671132147312,\n",
       "    0.030537785962224007,\n",
       "    -0.007292246911674738,\n",
       "    -0.05821411684155464,\n",
       "    -0.008607913739979267,\n",
       "    -0.059230078011751175,\n",
       "    0.030443087220191956,\n",
       "    -0.11919261515140533,\n",
       "    -0.07551616430282593,\n",
       "    -0.0005850792513228953,\n",
       "    0.019453857094049454,\n",
       "    -0.032944127917289734,\n",
       "    0.08154849708080292,\n",
       "    -0.05723084509372711,\n",
       "    -0.04144402593374252,\n",
       "    -0.03174900263547897,\n",
       "    -0.08152539283037186,\n",
       "    -0.053428489714860916,\n",
       "    0.09085691720247269,\n",
       "    -0.0448368638753891,\n",
       "    0.004793393891304731,\n",
       "    0.005334584508091211,\n",
       "    -0.0018369618337601423,\n",
       "    0.015501528047025204,\n",
       "    0.06758385896682739,\n",
       "    -0.00734243169426918,\n",
       "    -0.0005309555563144386,\n",
       "    -0.046153474599123,\n",
       "    -0.03369005769491196,\n",
       "    0.05287567526102066,\n",
       "    0.038130275905132294,\n",
       "    -0.03168613091111183,\n",
       "    -0.007008484564721584,\n",
       "    0.04990419000387192,\n",
       "    -0.0794747918844223,\n",
       "    0.03157037869095802,\n",
       "    0.040152259171009064,\n",
       "    -0.0033110990189015865,\n",
       "    -0.061134468764066696,\n",
       "    0.006686572916805744,\n",
       "    -0.025685995817184448,\n",
       "    0.04243863746523857,\n",
       "    0.048618216067552567,\n",
       "    0.0699104592204094,\n",
       "    -0.0033185728825628757,\n",
       "    -0.08736414462327957,\n",
       "    0.04655887931585312,\n",
       "    -0.08944731205701828,\n",
       "    -0.09301792085170746,\n",
       "    -0.043260715901851654,\n",
       "    -0.036524735391139984,\n",
       "    -0.0012551062973216176,\n",
       "    0.04354994744062424,\n",
       "    0.01758374646306038,\n",
       "    -0.04686374589800835,\n",
       "    0.02481449395418167,\n",
       "    -0.002361844526603818,\n",
       "    -0.050202734768390656,\n",
       "    0.05779697746038437,\n",
       "    0.027248986065387726,\n",
       "    0.03662337735295296,\n",
       "    0.06788437813520432,\n",
       "    -0.0014159363927319646,\n",
       "    -0.06972535699605942,\n",
       "    -0.0184149369597435,\n",
       "    0.07326146960258484,\n",
       "    0.07680892199277878,\n",
       "    0.16513320803642273,\n",
       "    -0.03529040887951851,\n",
       "    -0.0666118711233139,\n",
       "    -0.057760756462812424,\n",
       "    -0.04677576571702957,\n",
       "    -0.07224321365356445,\n",
       "    0.026119165122509003,\n",
       "    -0.03415466845035553,\n",
       "    -0.02577124908566475,\n",
       "    -0.005172401666641235,\n",
       "    -0.022824198007583618,\n",
       "    -0.050321370363235474,\n",
       "    -0.03786255046725273,\n",
       "    -0.03661470115184784,\n",
       "    -1.2743509614665527e-05,\n",
       "    0.0019839718006551266,\n",
       "    0.09418757259845734,\n",
       "    0.0788290724158287,\n",
       "    0.014120456762611866,\n",
       "    0.08594293892383575,\n",
       "    0.05751679092645645,\n",
       "    -0.014863895252346992,\n",
       "    0.04719037562608719,\n",
       "    0.05354054644703865,\n",
       "    -0.0009020774159580469,\n",
       "    -0.07279165089130402,\n",
       "    6.269616669545753e-33,\n",
       "    -0.018261607736349106,\n",
       "    0.05523209273815155,\n",
       "    -0.07853640615940094,\n",
       "    0.09759778529405594,\n",
       "    -0.022602280601859093,\n",
       "    0.03831557556986809,\n",
       "    -0.027963852509856224,\n",
       "    0.018782014027237892,\n",
       "    -0.03148431330919266,\n",
       "    -0.0846608579158783,\n",
       "    0.04103758558630943,\n",
       "    0.07797662168741226,\n",
       "    0.07694743573665619,\n",
       "    -0.017574714496731758,\n",
       "    -0.0421234630048275,\n",
       "    -0.045074209570884705,\n",
       "    -0.1043899729847908,\n",
       "    -0.024978967383503914,\n",
       "    -0.0003074128180742264,\n",
       "    -0.015490234829485416,\n",
       "    0.060889631509780884,\n",
       "    -0.04725818708539009,\n",
       "    -0.019489945843815804,\n",
       "    0.05993514508008957,\n",
       "    0.0608966089785099,\n",
       "    -0.04591009020805359,\n",
       "    0.05830144137144089,\n",
       "    -0.056938111782073975,\n",
       "    -0.030181489884853363,\n",
       "    0.03001328371465206,\n",
       "    -0.1108831912279129,\n",
       "    0.03251883015036583,\n",
       "    0.004920820705592632,\n",
       "    0.0101693794131279,\n",
       "    0.04569326713681221,\n",
       "    0.05168178677558899,\n",
       "    -0.06028658151626587,\n",
       "    -0.07152839004993439,\n",
       "    -0.03418966010212898,\n",
       "    -0.07768622785806656,\n",
       "    -0.033973563462495804,\n",
       "    0.020222332328557968,\n",
       "    -0.07717299461364746,\n",
       "    -0.01534964982420206,\n",
       "    -0.009635859169065952,\n",
       "    -0.0017287597293034196,\n",
       "    0.017104826867580414,\n",
       "    0.01763090118765831,\n",
       "    0.05517502501606941,\n",
       "    -0.05278465896844864,\n",
       "    0.01092127151787281,\n",
       "    -0.0120823560282588,\n",
       "    0.03758780285716057,\n",
       "    -0.07597605139017105,\n",
       "    0.02090461552143097,\n",
       "    -0.046508725732564926,\n",
       "    -0.036830201745033264,\n",
       "    -0.07593125849962234,\n",
       "    -0.06753644347190857,\n",
       "    0.003968549892306328,\n",
       "    0.06819020211696625,\n",
       "    0.06514070928096771,\n",
       "    -0.043663982301950455,\n",
       "    -0.0004422865167725831,\n",
       "    -0.16060009598731995,\n",
       "    0.06917282193899155,\n",
       "    -0.03039013035595417,\n",
       "    0.034337058663368225,\n",
       "    -0.01473288331180811,\n",
       "    0.07863257080316544,\n",
       "    0.03869082033634186,\n",
       "    -0.03322175517678261,\n",
       "    -0.012251926586031914,\n",
       "    -0.04114435613155365,\n",
       "    0.023315442726016045,\n",
       "    0.005399802699685097,\n",
       "    -0.06572834402322769,\n",
       "    0.017999080941081047,\n",
       "    0.0703570544719696,\n",
       "    -0.009727664291858673,\n",
       "    0.034798186272382736,\n",
       "    -0.03177013620734215,\n",
       "    0.017956452444195747,\n",
       "    0.010296575725078583,\n",
       "    0.055199407041072845,\n",
       "    -0.05340778827667236,\n",
       "    0.03455904871225357,\n",
       "    0.04117896780371666,\n",
       "    -0.04056495800614357,\n",
       "    0.04206041991710663,\n",
       "    0.00897500105202198,\n",
       "    0.03886953368782997,\n",
       "    0.05546724796295166,\n",
       "    0.07153420895338058,\n",
       "    -0.048874709755182266,\n",
       "    -7.832803750948616e-33,\n",
       "    -0.04453554004430771,\n",
       "    0.029403112828731537,\n",
       "    -0.0038646995089948177,\n",
       "    0.014520623721182346,\n",
       "    -0.01985781267285347,\n",
       "    0.013214546255767345,\n",
       "    0.023860881105065346,\n",
       "    0.0377291776239872,\n",
       "    0.08155439794063568,\n",
       "    -0.028121575713157654,\n",
       "    0.031066711992025375,\n",
       "    0.0647110790014267,\n",
       "    0.008016054518520832,\n",
       "    -0.030607430264353752,\n",
       "    -0.015962975099682808,\n",
       "    -0.06307824701070786,\n",
       "    0.0643051341176033,\n",
       "    -0.10515784472227097,\n",
       "    -0.020390046760439873,\n",
       "    -0.0017212682869285345,\n",
       "    0.043423086404800415,\n",
       "    0.04069341719150543,\n",
       "    -0.05943591147661209,\n",
       "    0.05421331152319908,\n",
       "    0.02912592887878418,\n",
       "    0.01983155682682991,\n",
       "    0.055439263582229614,\n",
       "    0.009063079953193665,\n",
       "    -0.048906486481428146,\n",
       "    -0.08470318466424942,\n",
       "    -0.11829663813114166,\n",
       "    -0.12406333535909653,\n",
       "    -0.05259865149855614,\n",
       "    0.05252799764275551,\n",
       "    0.06478428840637207,\n",
       "    0.07411458343267441,\n",
       "    0.06173132359981537,\n",
       "    -0.025167478248476982,\n",
       "    -0.03154860436916351,\n",
       "    -0.013286733999848366,\n",
       "    0.019219962880015373,\n",
       "    0.03326708450913429,\n",
       "    -0.03521926328539848,\n",
       "    0.018479155376553535,\n",
       "    -0.03625073283910751,\n",
       "    -0.0008724139188416302,\n",
       "    0.041927408427000046,\n",
       "    0.040219876915216446,\n",
       "    -0.004136150702834129,\n",
       "    0.04404311999678612,\n",
       "    0.03564972057938576,\n",
       "    0.06295794993638992,\n",
       "    -0.014207675121724606,\n",
       "    0.07388288527727127,\n",
       "    -0.05445251613855362,\n",
       "    0.027989856898784637,\n",
       "    0.011569265276193619,\n",
       "    0.0355946384370327,\n",
       "    -0.0515008345246315,\n",
       "    0.06996653974056244,\n",
       "    -0.02877601608633995,\n",
       "    0.07831589132547379,\n",
       "    0.04908832535147667,\n",
       "    -0.038995418697595596,\n",
       "    -0.003135651582852006,\n",
       "    -0.08045363426208496,\n",
       "    0.00012137110024923459,\n",
       "    -0.06302474439144135,\n",
       "    -0.018155427649617195,\n",
       "    -0.022411394864320755,\n",
       "    0.06847167015075684,\n",
       "    -0.04292677342891693,\n",
       "    -0.10244361311197281,\n",
       "    -0.11806114763021469,\n",
       "    -4.033009099657647e-05,\n",
       "    0.1023106575012207,\n",
       "    0.03187068551778793,\n",
       "    -0.09031421691179276,\n",
       "    -0.042772095650434494,\n",
       "    -0.06776145100593567,\n",
       "    0.004901639185845852,\n",
       "    0.09909424185752869,\n",
       "    0.04178405553102493,\n",
       "    0.049128394573926926,\n",
       "    0.0007728695054538548,\n",
       "    0.060704343020915985,\n",
       "    -0.0024120972957462072,\n",
       "    0.0026321818586438894,\n",
       "    0.08253207802772522,\n",
       "    0.0033874833025038242,\n",
       "    -0.030021995306015015,\n",
       "    -0.0713510811328888,\n",
       "    -0.02897382155060768,\n",
       "    0.04515034332871437,\n",
       "    -0.0332769900560379,\n",
       "    -7.186430650563125e-08,\n",
       "    0.006161579396575689,\n",
       "    0.016375495120882988,\n",
       "    -0.02072213962674141,\n",
       "    0.019249847158789635,\n",
       "    0.12310638278722763,\n",
       "    -0.015912197530269623,\n",
       "    -0.0037464993074536324,\n",
       "    0.020920731127262115,\n",
       "    -0.04968007281422615,\n",
       "    0.043093591928482056,\n",
       "    0.06409812718629837,\n",
       "    0.06710250675678253,\n",
       "    -0.057919859886169434,\n",
       "    0.005400545429438353,\n",
       "    0.048362813889980316,\n",
       "    0.03471607342362404,\n",
       "    -0.011375644244253635,\n",
       "    0.009263227693736553,\n",
       "    -0.05335906893014908,\n",
       "    -0.007628303952515125,\n",
       "    0.04848351329565048,\n",
       "    0.03112439252436161,\n",
       "    -0.04157270863652229,\n",
       "    0.058584924787282944,\n",
       "    0.025057556107640266,\n",
       "    -0.004129720851778984,\n",
       "    -0.07864116877317429,\n",
       "    0.05194821208715439,\n",
       "    0.03248806297779083,\n",
       "    -0.04453794285655022,\n",
       "    -0.02653496339917183,\n",
       "    -0.035715602338314056,\n",
       "    -0.07165143638849258,\n",
       "    -0.051854152232408524,\n",
       "    -0.0990554541349411,\n",
       "    -0.05036401003599167,\n",
       "    -0.04709694907069206,\n",
       "    0.018662096932530403,\n",
       "    0.0753193348646164,\n",
       "    0.02526242658495903,\n",
       "    0.014264818280935287,\n",
       "    0.00795206893235445,\n",
       "    0.030356112867593765,\n",
       "    0.05858776718378067,\n",
       "    -0.0729464739561081,\n",
       "    -0.013505258597433567,\n",
       "    -0.12372315675020218,\n",
       "    -0.07886135578155518,\n",
       "    -0.014433644711971283,\n",
       "    -0.0194643996655941,\n",
       "    -0.02613871358335018,\n",
       "    -0.11632455885410309,\n",
       "    0.049148257821798325,\n",
       "    0.060561444610357285,\n",
       "    0.06435759365558624,\n",
       "    0.003986949101090431,\n",
       "    -0.023553015664219856,\n",
       "    -0.01916712149977684,\n",
       "    0.0024841483682394028,\n",
       "    -0.012075072154402733,\n",
       "    0.0910082757472992,\n",
       "    -0.05059279128909111,\n",
       "    -0.024829452857375145,\n",
       "    0.11950274556875229],\n",
       "   [-0.10578782111406326,\n",
       "    0.0279950350522995,\n",
       "    0.06110762432217598,\n",
       "    0.008655339479446411,\n",
       "    0.02837900072336197,\n",
       "    0.02511715702712536,\n",
       "    -0.0009924423648044467,\n",
       "    0.05591000244021416,\n",
       "    0.06650242209434509,\n",
       "    -0.006881937850266695,\n",
       "    -0.0218966044485569,\n",
       "    0.005451471544802189,\n",
       "    0.017506223171949387,\n",
       "    -0.08112288266420364,\n",
       "    0.012227186001837254,\n",
       "    0.014148930087685585,\n",
       "    -0.008501814678311348,\n",
       "    -0.12598684430122375,\n",
       "    -0.09730600565671921,\n",
       "    0.09277161955833435,\n",
       "    0.005388058256357908,\n",
       "    0.0066676572896540165,\n",
       "    -0.031333811581134796,\n",
       "    0.034454524517059326,\n",
       "    0.0190053042024374,\n",
       "    -0.052210137248039246,\n",
       "    -0.055838972330093384,\n",
       "    0.03990521654486656,\n",
       "    -0.04021831229329109,\n",
       "    -0.038006193935871124,\n",
       "    -0.049822866916656494,\n",
       "    0.0275623369961977,\n",
       "    0.0016176061471924186,\n",
       "    0.04498564079403877,\n",
       "    0.025101739913225174,\n",
       "    0.039336755871772766,\n",
       "    -0.0017540290718898177,\n",
       "    -0.05786220729351044,\n",
       "    -0.016564438119530678,\n",
       "    0.006181451957672834,\n",
       "    0.02739790827035904,\n",
       "    -0.09999078512191772,\n",
       "    -0.08488050103187561,\n",
       "    0.0030612717382609844,\n",
       "    0.04208457097411156,\n",
       "    -0.0469123050570488,\n",
       "    0.035303495824337006,\n",
       "    -0.048396337777376175,\n",
       "    -0.02597426250576973,\n",
       "    -0.01733325608074665,\n",
       "    -0.10853911191225052,\n",
       "    -0.03988101705908775,\n",
       "    0.04735406115651131,\n",
       "    -0.044781915843486786,\n",
       "    -0.0200495608150959,\n",
       "    0.01817597821354866,\n",
       "    0.013327529653906822,\n",
       "    0.0279767494648695,\n",
       "    0.05470665544271469,\n",
       "    0.0004594521888066083,\n",
       "    -0.0005257312441244721,\n",
       "    -0.05576436594128609,\n",
       "    -0.04968775436282158,\n",
       "    0.07130298018455505,\n",
       "    0.004187160637229681,\n",
       "    -0.06766802817583084,\n",
       "    -0.018687373027205467,\n",
       "    0.013760029338300228,\n",
       "    -0.06166182458400726,\n",
       "    0.0603245384991169,\n",
       "    0.022158488631248474,\n",
       "    -0.004286868032068014,\n",
       "    -0.05746059492230415,\n",
       "    -0.008669146336615086,\n",
       "    0.0008419944206252694,\n",
       "    0.036761727184057236,\n",
       "    0.06043890118598938,\n",
       "    0.06256472319364548,\n",
       "    0.019964583218097687,\n",
       "    -0.06341268867254257,\n",
       "    0.06558971852064133,\n",
       "    -0.09841357916593552,\n",
       "    -0.1327938586473465,\n",
       "    -5.4309504776028916e-05,\n",
       "    -0.025155792012810707,\n",
       "    0.014779046177864075,\n",
       "    -0.002997427945956588,\n",
       "    0.03303176164627075,\n",
       "    0.03205781430006027,\n",
       "    -0.02141784131526947,\n",
       "    -0.010385900735855103,\n",
       "    -0.07791438698768616,\n",
       "    0.03389707952737808,\n",
       "    0.061852287501096725,\n",
       "    0.0337640605866909,\n",
       "    0.05203584209084511,\n",
       "    0.01339457742869854,\n",
       "    -0.03468344733119011,\n",
       "    -0.014557262882590294,\n",
       "    0.08963913470506668,\n",
       "    0.08326991647481918,\n",
       "    0.16060644388198853,\n",
       "    0.008933967910706997,\n",
       "    -0.06722065806388855,\n",
       "    -0.027091117575764656,\n",
       "    -0.023109260946512222,\n",
       "    -0.036461301147937775,\n",
       "    0.07199853658676147,\n",
       "    -0.027691176161170006,\n",
       "    -0.07142744213342667,\n",
       "    0.009535069577395916,\n",
       "    -0.03620629385113716,\n",
       "    0.031464140862226486,\n",
       "    -0.06072913482785225,\n",
       "    -0.05493788793683052,\n",
       "    0.04164627939462662,\n",
       "    -0.014963890425860882,\n",
       "    0.07296017557382584,\n",
       "    0.040288046002388,\n",
       "    -0.009883753955364227,\n",
       "    0.1129218265414238,\n",
       "    0.02427586168050766,\n",
       "    0.015914693474769592,\n",
       "    0.0423162467777729,\n",
       "    0.01585107110440731,\n",
       "    -0.0024150607641786337,\n",
       "    -0.035836346447467804,\n",
       "    5.064062356779047e-33,\n",
       "    0.03220738098025322,\n",
       "    0.04374659061431885,\n",
       "    -0.03734346106648445,\n",
       "    0.04466370865702629,\n",
       "    0.04445405676960945,\n",
       "    0.06709445267915726,\n",
       "    -0.03664923831820488,\n",
       "    0.0010981486411765218,\n",
       "    -0.07311433553695679,\n",
       "    -0.09248621016740799,\n",
       "    0.05637272819876671,\n",
       "    0.05177002400159836,\n",
       "    0.051808856427669525,\n",
       "    -0.02296609804034233,\n",
       "    -0.08725918084383011,\n",
       "    -0.01670456863939762,\n",
       "    -0.07840830087661743,\n",
       "    -0.013957721181213856,\n",
       "    -0.0533389113843441,\n",
       "    -0.030251501128077507,\n",
       "    0.09421145170927048,\n",
       "    -0.0623592808842659,\n",
       "    -0.03065468557178974,\n",
       "    0.02394017018377781,\n",
       "    0.05281282216310501,\n",
       "    -0.025925202295184135,\n",
       "    0.060687534511089325,\n",
       "    0.003240917343646288,\n",
       "    -0.04488096386194229,\n",
       "    0.02001364715397358,\n",
       "    -0.10112510621547699,\n",
       "    0.05920860171318054,\n",
       "    0.042216215282678604,\n",
       "    0.008265421725809574,\n",
       "    0.07117071002721786,\n",
       "    0.05884399265050888,\n",
       "    -0.06378193944692612,\n",
       "    -0.07368087768554688,\n",
       "    -0.03297063708305359,\n",
       "    -0.07276642322540283,\n",
       "    -0.029768845066428185,\n",
       "    0.033356375992298126,\n",
       "    -0.06534305214881897,\n",
       "    -0.03880174458026886,\n",
       "    -0.0196892898529768,\n",
       "    -0.05285496264696121,\n",
       "    -0.008319119922816753,\n",
       "    0.03816508501768112,\n",
       "    0.04537514969706535,\n",
       "    -0.0798432007431984,\n",
       "    0.018156928941607475,\n",
       "    0.020930858328938484,\n",
       "    -0.003913079388439655,\n",
       "    -0.04364084452390671,\n",
       "    -0.02076556719839573,\n",
       "    -0.037504490464925766,\n",
       "    0.03036547638475895,\n",
       "    -0.06428550183773041,\n",
       "    -0.06634015589952469,\n",
       "    0.019300837069749832,\n",
       "    0.06523294001817703,\n",
       "    0.06594102084636688,\n",
       "    -0.04586242511868477,\n",
       "    0.023721592500805855,\n",
       "    -0.14007093012332916,\n",
       "    0.05183417722582817,\n",
       "    -0.04078327864408493,\n",
       "    0.08497745543718338,\n",
       "    0.011171365156769753,\n",
       "    0.06395626068115234,\n",
       "    0.031853120774030685,\n",
       "    -0.03486136347055435,\n",
       "    -0.0035135657526552677,\n",
       "    -0.033492036163806915,\n",
       "    0.011894864030182362,\n",
       "    -0.0027704553212970495,\n",
       "    -0.09566156566143036,\n",
       "    0.024945763871073723,\n",
       "    0.023403162136673927,\n",
       "    -0.004949420690536499,\n",
       "    0.06830547004938126,\n",
       "    -0.04452436789870262,\n",
       "    0.012578163295984268,\n",
       "    0.03529549017548561,\n",
       "    0.028140190988779068,\n",
       "    -0.05135384574532509,\n",
       "    0.07238399237394333,\n",
       "    0.044926103204488754,\n",
       "    -0.04732009395956993,\n",
       "    0.03192613273859024,\n",
       "    -0.011562284082174301,\n",
       "    0.06749223917722702,\n",
       "    0.040694937109947205,\n",
       "    0.05302241072058678,\n",
       "    -0.053621795028448105,\n",
       "    -6.525741460226575e-33,\n",
       "    -0.06152687966823578,\n",
       "    0.03324209153652191,\n",
       "    -0.0347064733505249,\n",
       "    -0.007534225471317768,\n",
       "    -0.020763667300343513,\n",
       "    0.0036010872572660446,\n",
       "    0.025646602734923363,\n",
       "    0.0311142411082983,\n",
       "    0.059352658689022064,\n",
       "    -0.06121581420302391,\n",
       "    0.004231561906635761,\n",
       "    0.013844422996044159,\n",
       "    -0.013996437191963196,\n",
       "    -0.0254046767950058,\n",
       "    0.005085054785013199,\n",
       "    -0.010275585576891899,\n",
       "    0.04828866943717003,\n",
       "    -0.09471262246370316,\n",
       "    -0.07752785831689835,\n",
       "    0.029314525425434113,\n",
       "    0.05109792575240135,\n",
       "    0.0019196029752492905,\n",
       "    -0.08437211811542511,\n",
       "    0.04084906354546547,\n",
       "    0.029435329139232635,\n",
       "    -0.002313531469553709,\n",
       "    0.09556935727596283,\n",
       "    0.018141252920031548,\n",
       "    -0.05356346815824509,\n",
       "    -0.08768530189990997,\n",
       "    -0.09971431642770767,\n",
       "    -0.143388569355011,\n",
       "    -0.07054353505373001,\n",
       "    0.035686053335666656,\n",
       "    0.06438816338777542,\n",
       "    0.08759495615959167,\n",
       "    0.06631551682949066,\n",
       "    -0.03534865379333496,\n",
       "    0.00558509211987257,\n",
       "    0.0035093624610453844,\n",
       "    0.07188042998313904,\n",
       "    -0.002821307862177491,\n",
       "    -0.00692338403314352,\n",
       "    0.04922232776880264,\n",
       "    -0.015412603504955769,\n",
       "    -0.01157663855701685,\n",
       "    -0.005307410843670368,\n",
       "    -0.012776887975633144,\n",
       "    -0.04884103313088417,\n",
       "    0.04916217178106308,\n",
       "    -0.004796150140464306,\n",
       "    -0.011907808482646942,\n",
       "    0.012672509998083115,\n",
       "    0.08777344971895218,\n",
       "    -0.04799284785985947,\n",
       "    0.06933949142694473,\n",
       "    -0.028547514230012894,\n",
       "    0.05428021401166916,\n",
       "    -0.014429484494030476,\n",
       "    0.003652825253084302,\n",
       "    -0.007957756519317627,\n",
       "    0.0682058185338974,\n",
       "    -0.01334294956177473,\n",
       "    -0.04326272010803223,\n",
       "    0.06792643666267395,\n",
       "    -0.05197795480489731,\n",
       "    -0.006732819136232138,\n",
       "    -0.036944013088941574,\n",
       "    0.017758870497345924,\n",
       "    -0.03356996178627014,\n",
       "    0.031211011111736298,\n",
       "    -0.01603742130100727,\n",
       "    -0.08008787035942078,\n",
       "    -0.10679370909929276,\n",
       "    0.024435093626379967,\n",
       "    0.08447369188070297,\n",
       "    -0.04647356644272804,\n",
       "    -0.04408682510256767,\n",
       "    -0.047248389571905136,\n",
       "    0.0038594852667301893,\n",
       "    0.018771329894661903,\n",
       "    0.05022458732128143,\n",
       "    0.05943914130330086,\n",
       "    0.0815814658999443,\n",
       "    0.030465444549918175,\n",
       "    0.0665791854262352,\n",
       "    0.04767511785030365,\n",
       "    0.031987451016902924,\n",
       "    0.05240790918469429,\n",
       "    0.02955840341746807,\n",
       "    -0.04959404468536377,\n",
       "    -0.04374691843986511,\n",
       "    -0.014469355344772339,\n",
       "    0.07467583566904068,\n",
       "    -0.03414241597056389,\n",
       "    -6.853844070064952e-08,\n",
       "    0.0007524913526140153,\n",
       "    0.010280152782797813,\n",
       "    -0.07525410503149033,\n",
       "    0.026845121756196022,\n",
       "    0.1168445572257042,\n",
       "    -0.05133751034736633,\n",
       "    0.008830619975924492,\n",
       "    -0.012969208881258965,\n",
       "    -0.05128553882241249,\n",
       "    0.05297192931175232,\n",
       "    0.03948540985584259,\n",
       "    0.05814630538225174,\n",
       "    -0.04545026645064354,\n",
       "    0.010933979414403439,\n",
       "    0.024840550497174263,\n",
       "    -0.011808904819190502,\n",
       "    -0.039929937571287155,\n",
       "    0.010279873386025429,\n",
       "    -0.04499046131968498,\n",
       "    -0.02451682649552822,\n",
       "    0.032367635518312454,\n",
       "    0.04340093582868576,\n",
       "    -0.022453343495726585,\n",
       "    0.09430884569883347,\n",
       "    0.0271706972271204,\n",
       "    -0.01091616228222847,\n",
       "    -0.09336480498313904,\n",
       "    0.09884615987539291,\n",
       "    0.007814372889697552,\n",
       "    -0.0340321809053421,\n",
       "    -0.0450655035674572,\n",
       "    -0.04635936766862869,\n",
       "    -0.08103647083044052,\n",
       "    -0.04102134332060814,\n",
       "    -0.10985303670167923,\n",
       "    0.006092818919569254,\n",
       "    -0.0554569810628891,\n",
       "    0.000343647989211604,\n",
       "    0.09116710722446442,\n",
       "    0.03515610843896866,\n",
       "    0.023806443437933922,\n",
       "    0.019169354811310768,\n",
       "    0.04254411533474922,\n",
       "    0.06672576814889908,\n",
       "    -0.018956448882818222,\n",
       "    -0.027485216036438942,\n",
       "    -0.11408992856740952,\n",
       "    -0.09760145097970963,\n",
       "    0.005097333807498217,\n",
       "    -0.05940262973308563,\n",
       "    -0.036853961646556854,\n",
       "    -0.09587553143501282,\n",
       "    0.04519364982843399,\n",
       "    0.05282231792807579,\n",
       "    0.06603512167930603,\n",
       "    -0.002604118548333645,\n",
       "    -0.013699260540306568,\n",
       "    0.004641999956220388,\n",
       "    -2.9982315936649684e-06,\n",
       "    0.0006674490286968648,\n",
       "    0.06130949407815933,\n",
       "    -0.03838138282299042,\n",
       "    -0.03743300959467888,\n",
       "    0.11600421369075775]]],\n",
       " 'metadatas': [[{'date': '2024-07-24 01:45:26',\n",
       "    'quarter': 2,\n",
       "    'seq_num': 1002,\n",
       "    'source': '/Users/michieldekoninck/code/Michiel-DK/finance/json_data/transcripts_2024.json',\n",
       "    'symbol': 'TSLA',\n",
       "    'year': 2024},\n",
       "   {'date': '2024-04-23 22:30:10',\n",
       "    'quarter': 1,\n",
       "    'seq_num': 1003,\n",
       "    'source': '/Users/michieldekoninck/code/Michiel-DK/finance/json_data/transcripts_2024.json',\n",
       "    'symbol': 'TSLA',\n",
       "    'year': 2024}]],\n",
       " 'documents': [[\"Travis Axelrod: Good afternoon, everyone and welcome to Tesla's Second Quarter 2024 Q&A Webcast. My name is Travis Axelrod, Head of Investor Relations and I’m joined today by Elon Musk, Vaibhav Taneja, and a number of other executives. Our Q2 results were announced at about 3.00 p.m. Central Time and the Update Deck we published at the same link as this webcast. During this call, we will discuss our business outlook and make forward-looking statements. These comments are based on our predictions and expectations as of today. Actual events or results could differ materially due to a number of risks and uncertainties, including those mentioned in our most recent filings with the SEC. During the question-and-answer portion of today's call, please limit yourself to one question and one follow-up. Please use the raise hand button to join the question queue. Before we jump into Q&A, Elon has some opening remarks. Elon?\\nElon Musk: Thank you. So to recap, we saw large adoption exploration in EVs, and then a bit of a hangover as others struggle to make compelling EVs. So there are quite a few competing electric vehicles that have entered the market. And mostly they’ve not done well, but they’ve discounted their EVs very substantially, which has made it a bit more difficult for Tesla. We don’t see this as long-term issue, but really -- fairly short-term. And we still obviously firmly believe that EVs are best for customers and that the world is headed for a fully electrified transport, not just the cars, but also aircrafts and boats. Despite many challenges the Tesla team did a great job executing and we did achieve record quarterly revenues. Energy storage deployments reached an all-time high in Q2, leading to record profits for the energy business. And we are investing in many future projects, including AI training and inference and great deal of infrastructure to support future products. We won't get too much into the product roadmap here, because that is reserved for product announcement events. But we are on track to deliver a more affordable model in the first half of next year. The big -- really by far the biggest differentiator for Tesla is autonomy. In addition to that, we've scale economies and we're the most efficient electric vehicle producer in the world. So, this, anyway -- while others are pursuing different parts of the AI robotic stack, we are pursuing all of them. This allows for better cost control, more scale, quicker time to market, and a superior product, applying not to -- not just to autonomous vehicles, but to autonomous humanoid robots like Optimus. Regarding Full Self-Driving and Robotaxi, we've made a lot of progress with Full Self-Driving in Q2 and with version 12.5 beginning rollout, we think customers will experience a step change improvement in how well supervised full self-driving works. Version 12.5 has 5x the parameters of 12.4 and will finally merge the highway and city stacks. So the highway stack is still at this point is pretty old. So often the issues people encounter are on highway, but with 12.5, we are finally merged the two stacks. I still find that most people actually don't know how good the system is, and I would encourage anyone to understand the system better, to simply try it out and let the car drive you around. One of the things we're going to be doing just to make sure people actually understand the capabilities of the car is when delivering a new car and when picking up a car for service to just show people how to use it and just drive them around the block. Once people use it at all they tend to continue using it. So it's very compelling. And then this I think will be a massive demand driver, even unsupervised full self-driving will be a massive demand driver. And as we increase the miles between intervention, it will transition from supervised full self-driving to unsupervised full self-driving, and we can unlock massive potential in [V3] (ph). We postponed the sort of Robotaxi the sort of product unveil by a couple of months where it were -- it shifted to 10/10 to the 10th October -end because I wanted to make some important changes that I think would improve the vehicle -- sort of Robotaxi, the thing that we are -- the main thing that we are going to show and we are also going to show off a couple of other things. So moving it back a few months allowed us to improve the Robotaxi as well as add in a couple other things for the product unveil. We're also nearing completion of the South expansion of Giga Texas, which will house our largest training cluster to date. So it will be an incremental for 50,000 H100s plus 20,000 of our hardware 4 AI5 Tesla AI computer. With Optimus, Optimus is already performing tasks in our factory. And we expect to have Optimus production Version 1 in limited production starting early next year. This will be for Tesla consumption. It's just better for us to iron out the issues ourselves. But we expect to have several thousand Optimus robots produced and doing useful things by the end of next year in the Tesla factories. And then in 2026, ramping up production quite a bit, and at that point we'll be providing Optimus robots to outside customers. That will be Production Version 2 of Optimus. For the energy business, this is growing faster than anything else. This is -- we are really demand constrained rather than production constrained. So we are ramping up production in our U.S. factory as well as building the Megapack factory in China that should roughly double our output, maybe more than double -- maybe triple potentially. So in conclusion, we are super excited about the progress across the board. We are changing the energy system, how people move around, how people approach the economy. The undertaking is massive, but I think the future is incredibly bright. I really just can't emphasize just the importance of autonomy for the vehicle side and for Optimus. Although the numbers sound crazy, I think Tesla producing at volume with unsupervised FSD essentially enabling the fleet to operate like a giant autonomous fleet. And it takes the valuation, I think, to some pretty crazy number. ARK Invest thinks, on the order of $5 trillion, I think they are probably not wrong. And long-term Optimus, I think, it achieves a valuation several times that number. I want to thank the Tesla team for a strong execution and looking forward to exciting years ahead.\\nTravis Axelrod: Great. Thank you very much, Elon, and Vaibhav has opening remarks as well.\\nVaibhav Taneja: Thanks. As Elon mentioned, the Tesla team rose to the occasion yet again and delivered on all fronts with some notable records. In addition to those records, we saw our automotive deliveries go sequentially. I would like to thank the entire Tesla team for their efforts in delivering a great quarter. On the auto business front, affordability remains a top of mind for customers, and in response in Q2, we offered attractive financing options to offset sustained high interest rates. These programs had an impact on revenue per unit in the quarter. These impacts will persist into Q3 as we have already launched similar programs. We are now offering extremely competitive financing rates in most parts of the world. This is the best time to buy a Tesla, I mean, if you are waiting on the sidelines, come out and get your car. We had a record quarter on regulatory credits, revenues, and as well. On net, our auto margins remained flat sequentially. It is important to note that the demand for regulatory credits is dependent on other OEMs plans for the kind of vehicles they are manufacturing and selling as well as changes in regulations. We pride ourselves to be the company with the most American-made cars and are continuing our journey to further localize our supply chain, not just in the U.S., but in Europe and China as well for the respective factories. As always, our focus is on providing the most compelling products at a reasonable price. We have stepped up our efforts to provide more trims that have estimated range of more than 300 miles on a single charge. We believe this, along with the expansion of our supercharging network, is the right strategy to combat range anxiety. Since the revision of FSD pricing in North America, we've seen production rates increase meaningfully and expect this to be a driver of vehicle sales as the feature set improves further. Cost per vehicle declined sequentially when we removed the impact of Cybertruck. While we are experiencing material costs trending down, note that there is latency on the cost side and such reductions would show up in the P&L when the vehicles built with these materials get delivered. Additionally, as we get into the second half of the year, it is important to note that we are still ramping Cybertruck and Model 3 and are also getting impacted by varying amounts of tariffs on both raw materials and finished goods. While our teams are working feverishly to offset these, unfortunately it may have an impact on the cost in the near-term. We previously talked about the potential of the energy business and now feel excited that the foundation that was laid over time is bearing the expected results. Energy storage deployments more than doubled with contribution not just from Megapack, but also Powerwall, resulting in record revenues and profit for the energy business. Energy storage backlog is strong. As discussed before, deployments will fluctuate from period to period with some quarters seeing large increases and others seeing a decline. Recognition of storage gigawatt hours is dependent on a variety of factors, including logistics timing as we send units from a single factory to markets across the world, customer readiness and in case of EPC projects on construction activities. Moving on to the other parts of the business, service and other gross profits also improved sequentially from the improvement in service utilization and growth in our collision repair business. The impact of our recent reorg is reflected in restructuring other - on the income statement. Just to level set, this was about $622 million of charge, which got recorded in the period. And I want people to remember that we've called it out separately on the financials. Sequentially, our operating expenses excluding surcharges reduced despite an increase in spend for AI-related activities and higher legal and other costs. On the CapEx front, while we saw a sequential decline in Q2, we still expect the year to be over $10 billion in CapEx as we increase our spend to bring a 50k GPU cluster online. This new cluster will immensely increase our capabilities to scale FSD and other AI initiatives. We reverted to positive free cash flow of $1.3 billion in Q2. This was despite restructuring payments being made in the quarter and we ended the quarter with over $30 billion of cash and investments. Once again, we've begun the journey towards the next phase for the company with the building blocks being placed. It will take some time, but will be a rewarding experience for everyone involved. Once again, I would like to thank the entire Tesla team for their efforts.\\nA - Travis Axelrod: Great. Thank you very much, Vaibhav. Now let's go to investor questions. The first question is, what is the status on the Roadster?\\nElon Musk: With respect to Roadster, we've completed most of the engineering. And I think there's still some upgrades we want to make to it, but we expect to be in production with Roadster next year. It will be something special, like the whole thing [Indiscernible].\\nTravis Axelrod: Fantastic. The next question is about timing of Robotaxi event, which we've already covered. So we'll go to the next question, when do you expect the first Robotaxi ride?\\nElon Musk: I guess that, that's really just a question of when can we expect the first -- or when can we do unsupervised full self-driving. It's difficult, obviously, my predictions on this have been overly optimistic in the past. So I mean, based on the current trend, it seems as though we should get miles between interventions to be high enough that -- to be far enough in excess of humans that you could do unsupervised possibly by the end of this year. I would be shocked if we cannot do it next year. So next year seems highly probable to me based on [quite simply] (ph) plus the points of the curve of miles between intervention. That trend exceeds humans for sure next year, so yes.\\nTravis Axelrod: Thank you very much. Our third question is, the Cybertruck is an iconic product that wows everyone who sees it. Do you have plans to expand the cyber vehicle lineup to a cyber SUV or cyber van?\\nElon Musk: I think we want to limit product announcements to when we have a special -- specific product announcement event, rather than earnings calls.\\nTravis Axelrod: Great, thank you. Our next question is, what is the current status of 4680 battery cell production and how is the ramp up progressing?\\nLars Moravy: Yes, 4680 production ramped strongly in Q2, delivering 51% more cells than Q1 while reducing COGS significantly. We currently produce more than 1,400 Cybertrucks of 4680 cells per week, and we'll continue to ramp output as we drive cost down further towards the cost parity target we set for the end of the year. We've built our first validation Cybertruck with dry cathode process made on our mass production equipment, which is a huge technical milestone and we're super proud of that. We're on track for production launch with dry cathode in Q4, and this will enable cell cost to be significantly below available alternatives, which was the original goal of the 4680 program.\\nTravis Axelrod: Great. Thank you very much. The next question is any update on Dojo?\\nElon Musk: Yes, so Dojo, I should preface this by saying I'm incredibly impressed by NVIDIA's execution and the capability of their hardware. And what we are seeing is that the demand for NVIDIA hardware is so high that it's often difficult to get the GPUs. And there just seems this, I guess I'm quite concerned about actually being able to get state-of-the-art NVIDIA GPUs when we want them. And I think this therefore requires that we put a lot more effort on Dojo in order to have -- in order to ensure that we've got the training capability that we need. So we are going to double down on Dojo, and we do see a path to being competitive with NVIDIA with Dojo. And I think we kind of have no choice because the demand for NVIDIA is so high and the -- it's obviously their obligation essentially to raise the price of GPUs to whatever the market will bear, which is very high. So, I think we've really got to make Dojo work and we will.\\nTravis Axelrod: Right. The next question is what type of accessories will be offered with Optimus?\\nElon Musk: There's -- Optimus is intended to be a generalized humanoid robot with a lot of intelligence. So it's like saying what kind of accessories will be offered with a human. It's just really intended to be able to be backward compatible with human tasks. So it would use any accessories that a human would use. Yes.\\nTravis Axelrod: Thank you. The next question is, do you feel you're cheating people out of the joys of owning a Tesla by not advertising?\\nElon Musk: We are doing some advertising, so, want to say something?\\nVaibhav Taneja: Yes, I would say something. Our fundamental belief is that we need to be providing the best products at a reasonable price to the consumers. Just to give you a fact, in U.S. alone in Q2, over two-thirds of our sales were to -- deliveries were to people who had never owned a Tesla before and which is encouraging. We've spent money on advertising and other awareness programs and we have adjusted our strategy. We're not saying no to advertising, but this is a dynamic play and we know that we have not exhausted all our options and therefore plan to keep adjusting, but in the latter half of this year as well.\\nTravis Axelrod: Great. Thank you very much. The next question is on energy growth, which we already covered in opening remarks, so we'll move on to the next one. What is the updated timeline for Giga Mexico and what will be the primary vehicles produced initially?\\nElon Musk: Well, we currently are paused on Giga Mexico. I think we need to see just where things stand after the election. Trump has said that he will put heavy tariffs on vehicles produced in Mexico. So it doesn't make sense to invest a lot in Mexico if that is going to be the case. So we kind of need to see where the things play out politically. However, we are increasing capacity at our existing factories quite significantly. And I should say that the Cybertaxi or Robotaxi will be produced here at our headquarters at Giga Texas.\\nTravis Axelrod: All right. Thank you.\\nElon Musk: And as well Optimus towards the end of next year for Optimus production Version 2, the high volume version of Optimus will also be produced here in Texas.\\nTravis Axelrod: Great. Thank you. Just a couple more. Is Tesla still in talks with an OEM to license FSD?\\nElon Musk: There are a few major OEMs that have expressed interest in licensing Tesla full self-driving. And I suspect there will be more over time. But we can't comment on the details of those discussions.\\nTravis Axelrod: All right. Thank you. And the last one, any updates on investing in xAI and integrating Grok into Tesla software?\\nElon Musk: I should say Tesla is learning quite a bit from xAI. It's been actually helpful in advancing full self-driving and in building up the new Tesla data center. With -- regarding investing in xAI, I think, we need to have a shareholder approval of any such investment. But I'm certainly supportive of that if shareholders are, the group -- probably, I think we need a vote on that. And I think there are opportunities to integrate Grok into Tesla's software, yes.\\nTravis Axelrod: All right. Thanks very much. And now we will move on to analyst questions. The first question comes from Will Stein from Truist. Will, please go ahead and unmute yourself.\\nWill Stein: Great. Thanks so much for taking my question. And this relates a little bit to the last one that was asked. Elon, I share your strong enthusiasm about AI and I recognize Tesla's opportunity to do some great things with the technology. But there are some concerns I have about Tesla's commercialization and that's what I'd like to ask about specifically. There were some news stories through the quarter that indicated that you redirected some AI compute systems that were destined for Tesla instead to xAI or perhaps it was to X, I'm not sure. And similarly, a few quarters ago, if you recall, I asked about your ability to hire engineers in this area, and you noted that there was a great desire for some of these engineers to work on projects that you were involved with, but some of them weren't at Tesla, they were instead at xAI or perhaps even X again. So the question is, when it comes to your capital investments, your AI R&D, your AI engineers, how do you make allocation decisions among these various ventures and how do you make Tesla owners comfortable that you're doing it in a way that really benefits them? Thank you.\\nElon Musk: Yes, I mean, I think you're referring to a very -- like an old article, regarding GPUs. I think that's like 6 or 7 months old. At Tesla, we had no place to try them on, so it would've been a waste of Tesla capital because we would just have to order H100 and have no place to try them on. So it was just -- there was -- this wasn't a, let's pick xAI of Tesla. There's -- there was no -- the Tesla data centers were full. There was no place to actually put them. The -- we've been working 24/7 to complete the South extension on the Tesla Giga factory in Texas. That South extension is what will house 50,000 H100s and we're beginning to move the H100 server racks into place there. But we really needed -- we needed that to complete physically. You can't just order compute -- order GPUs and turn them on, you need a data center, it's not possible. So I want to be clear, that was in Tesla's interest, not contrary to Tesla's interest. Does Tesla no good to have GPUs that it can't turn on. That South extension is able to take GPUs, which is really just this week. We are moving the GPUs in there and we'll bring them online. With regard to xAI, there are a few that only want to work on AGI. So what I was finding was that when trying to recruit people to Tesla, they were only interested in working on AGI and not on Tesla's specific problems and they want to start -- do a start-up. So it was a case of either they go to a start-up or -- and I am involved or they do a start-up and I am not involved. Those are the two choices. This wasn't they would come to Tesla. They were not going to come to Tesla under any circumstances. So, yes.\\nVaibhav Taneja: Yes, I mean, I would even add that AI is a broad spectrum and there are a lot of things which we are focused on full time driving as Tesla and also Optimus, but there's the other spectrum of AI which we're not working on, and that's the kind of work which other companies are trying to do in this case, xAI. So you have to keep that in mind that it's a broad spectrum. It's not just one specific thing.\\nElon Musk: Yes. And once again, I want to just repeat myself here. I tried to recruit them to Tesla, including to say like, you can work on AGI, I if you want and they refused. Only then was xAI created.\\nWill Stein: I really appreciate that clarification. If I can ask one follow-up, it relates to the new vehicles that you're planning to introduce next year. I understand this is not the venue for product announcements, but when we think about the focus, I've heard on the one hand that the focus is on cost reduction. On the other hand, you also said that the Roadster would come out. Should we expect other maybe more limited variants like, similar to the cars that you make today, but with some changes or improvements or different, some other variability in the form factors. It should -- we expect that to be a significant part of the strategy in the next year or two?\\nElon Musk: I don't want to get into details of product announcements. And we have to be careful of the Osborne effect here. So, if you start announcing some great thing, it affects our near-term sales. We're going to make great products in future just like we have in the past, end of story.\\nTravis Axelrod: Right. The next question comes from Ben Kallo from Baird. Ben, please go ahead and unmute yourself.\\nBen Kallo: Hi. Thanks for taking my question. When we think about revenue contribution and with energy growing so quickly and Optimus on the come, how do we think about the overall segments longer term? And then do you think that auto revenue will fall below 50% of your overall revenue? And then my follow-up is just on the last call you talked about, distributed compute on your new hardware. Could you just update us and talk a little bit more about that, the timeline for it and how you would reward customers for letting you use their compute power and their cars? Thanks.\\nElon Musk: Yes, I mean, as I've said a few times, I think the long-term value of Optimus will exceed that of everything else that Tesla combined. So, it's simply -- just simply consider the usefulness utility of a humanoid robot that can do pretty much anything you ask of it. I think everyone on earth is going to want one. There's 8 billion people on earth, so it's 8 billion right there. Then you've got, all of the industrial uses, which is probably at least as much, if not way more. So I suspect that the long-term demand for general purpose humanoid robots is in excess of 20 billion units. And Tesla is -- that has the most advanced humanoid robot in the world, and is also very good at manufacturing, which these other companies are not. And we've got a lot of experience -- with the most experienced with the world leaders in real world AI. So we have all of the ingredients. I think we are unique in having all of the ingredients necessary for large scale, high utility, generalized humanoid robots. That's why my rough estimate long-term is in accordance with the ARK [ph] Invest analysis of market cap on the order of $5 trillion for -- maybe more for autonomous transport, and it's several times that number for general purpose humanoid robots. I mean, at that point, I'm not sure what money even means, but in the benign AI scenario, we are headed for an age of abundance where there is no shortage of goods and services. Anyone can have pretty much anything they want. It's a wild -- very wild future we're heading for.\\nBen Kallo: On the distributed compute?\\nElon Musk: Yes, distributed compute, that seems like a pretty obvious thing to do. I think the -- where this distributed compute becomes interesting is with our next generation Tesla AI truck, which is hardware viable or what we're calling AI5, which is -- from the standpoint of inference capability comparable toB200 -- and a bit of B200. And we are aiming to have that in production at the end of next year and scale production in '26. So it just seemed like if you've got -- even if you've got autonomous vehicles that are operating for 50 or 60 hours a week, there's a 168 hours in a week. So you have somewhere above I think a 100 [indiscernible] net computing. I think we need a better word than GPU because GPU means graph express in unit. So there's a 100 hours plus per week of AI compute, AI advanced compute from the fleet, from the vehicles and probably some percentage from the humanoid robots that it would make sense to do distributed inference. And if you're -- if there's a fleet of at some point a 100 million vehicles with AI5 and beyond, because you have AI 6 and 7 and whatnot, and there may be billions of humanoid robots that is just a staggering amount of inference compute or that could be used for general purposes at computing. It doesn't have to be used for, the humanoid robot or for the car. So I think, that's just -- that -- that's a pretty obvious thing to say, like, well, it's more useful than having to do nothing.\\nTravis Axelrod: All right. Thank you. The next question comes from Alex Potter from Piper Alex. Alex, please go ahead and unmute yourself.\\nAlex Potter: Perfect. Thanks. I wanted to ask a question on FSD licensing. You mentioned that in passing previously, was just wondering if you can elaborate maybe on the mechanics of how that would work. I guess presumably this would not be some sort of simple plug and play proposition that presumably an OEM would need, I don't know, several years to develop its own vehicle platform that's based on FSD. I imagine they would need to adopt Tesla's electrical architecture, compute, sensor stack. So I, correct me if I'm sort of misunderstanding this, but if you had a cooperative agreement of some kind with another OEM, then presumably it would take you several years before you'd be able to recognize licensing revenue from that agreement. Is that the right way to think about that?\\nElon Musk: Yes. The OEMs not real fast. There's not really a sensor suite, it's just cameras. But they would have to integrate our AI computer and have cameras with a 360 degree view. And at least the gateway, like the what talks to the internet, and communicates with the Tesla system, what that you need kind of a gateway computer too. So it's really gateway computer with the cellular and Wi-Fi connectivity, the Tesla AI computer, and seven cameras, or not cameras, again, a 360 degree view. But this will -- given the speed at which, the auto industry moves, it would be several years before you would see this in volume.\\nAlex Potter: Okay, good. That's more or less what I expected. So then the follow-up here is, if you did sign an FSD licensing agreement with another automaker, when do you think you would disclose that? Would you do it right when you signed the agreement or only after that multiple years has passed and the vehicle is ready to be rolled out? think it depends on the OEM. I guess we'd be happy either way. Yes, it depends on, what kind of arrangement we enter into. A lot of those things are, we are not resolved yet, so we'll make that determination as and when we get to that point.\\nElon Musk: And the kind of deals that are obviously relevant are only if, some OEM is willing to do this in a million cars a year or something significant. It's not -- if it's like 10,000 or a 100,000 cars a year. We can just make that ourselves.\\nTravis Axelrod: All right, thank you. The next question comes from Dan Levy from Barclays. Dan, please go ahead and unmute yourself.\\nDan Levy: Hi, good evening. Thanks for taking the questions. First, wanted to start with a question on Shanghai. You've leveraged Shanghai as an export center really due its low cost, and that makes sense. But maybe you can just give us a sense of, of how the strategy changes, if at all, given, the implementation of tariffs in Europe. Also to what extent, your import of batteries from China into the U.S., how that might change given the tariffs. Thank you.\\nElon Musk: Yes. I think I covered some part of it in my opening remarks, but just to give you a little bit more, just on the tariff side, the European authorities did sample certain other OEMs in the first round to establish the tariffs for cars being imported from China into Europe. While we were not picked up in our individual examination in the first round, they did pick us up in the second round. They visited our factory. They -- we worked with them, provided them all the information. As a result, we were adjusting our import strategy out of China into Europe. But -- and one other thing to note is in Q2 itself, we started building right hand from model wise out of Berlin and we also delivered it in U.K. And we're adjusting as needed, but we will keep adjust. We're still importing Model 3s into Europe, out of Shanghai. And we are still evaluating what is the best alternate manage all this just on the examination by the European authorities. Like I said, we cooperated with them. Well, we are confident that they, we should get a better rate than what they have imposed for now. But this is literally evolving and we are adjusting as fast as we can with this. It is -- I would also add that, because of this, you've seen the impact that Berlin is doing more imports into places like Taiwan as well as, U.K I just mentioned. So it will keep changing and we will keep adapting as we go about it.\\nDan Levy: Great. Thanks. Yes, thank you. As a follow-up, wanted to ask about the Robotaxi strategy and specifically the shareholder deck here notes that the release is going to be -- one of the gating factors is regulatory approval. So maybe you could help us understand which regulations specifically are the ones that we should be looking for? Is it FMVSS, that's standard? And then to what extent does the strategy shift? You've done with FSD more of a nationwide, no boundary approach. Is the Robotaxi approach one that's more geofenced, so to speak, and is more driven by a state by state approach?\\nElon Musk: I mean, our solution is a generalized solution like what everybody else has. They, if you see like Waymo has one of it, they have a very localized solution that requires high density mapping. It's not -- it's quite fragile. So, their ability to expand rapidly is limited. Our solution is a general solution that works anywhere. It would even work on a different earth. So if you're rendered a new Earth, it would work on a new earth. So it's -- there's this capability I think in our experience, once we demonstrate that something is safe enough or significantly safer than human. We are fine that regulators are supportive of deploying deployment of that capability. It's difficult to argue with if you -- if you've got a large number of -- yes, if you've got billions of miles that show that in the future unsupervised FSD is safer than human. What regulator could really stand in the way of that? They would -- they're morally obligated to approve. So I don't think regulatory approval will be a limiting factor. I should also say that the self-driving capabilities of this are deployed outside of North America are far behind that in, in North America. So with the -- with Version 12.5, and maybe a 12.6, but pretty soon we will ask for regular regulatory approval of the Tesla supervised FSD in Europe, China, and other countries. And I, I think we're likely to receive that before the end of the year, which will be a helpful demand driver in those regions obviously.\\nTravis Axelrod: Thank you. Just to …\\nElon Musk: Go ahead, Travis.\\nTravis Axelrod: In terms of like, as Elon said, in terms of regulatory approval, the vehicles are governed by FMVSS in U.S., which is the same across all 50 states. The road rules are the same across all 50 states. So creating a generalized solution gives us the best opportunity to deploy in all 50 states, reasonably. Of course there are state and even local and municipal level regulations that may apply to, being a transportation company or deploying taxes. But as far as getting the vehicle on the road, that's all federal and that's very much in line with what you was just suggesting about the data and the vehicle itself.\\nVaibhav Taneja: And to add to the technology point, the end-to-end network basically makes no assumption about the location. Like you could add data from different countries and it just like perform equally well there, just like almost like close to zero US specific, um, code in there. It's all just the data that comes from the U.S\\nElon Musk: Yes. To, to that end of the show, it's like, we can go as humans to other countries and drive with some reasonable amount of assessment in those countries. And that's how you design the FSC software. Yes, exactly.\\nTravis Axelrod: Great. Thanks guys. The next question comes from George from Canaccord. George, please go ahead and unmute yourself.\\nGeorge Gianarikas: Hi, everyone. Thank you for taking my questions. Maybe just to expand on the regulatory question for a second. And I could be comparing apples and oranges, but GM canceled their pedal less, wheel less vehicle. And according to the company this morning, their decision was driven by uncertainty about the regulatory environment. And from what we understand, and again, maybe I'm wrong here, but the Robotaxi that has been shown at least in images of the public is also pedal less and wheel less. Is there a different regulatory concern just if you deploy a vehicle like that that doesn't have pedal -- pedals or a wheel, and that may not be different from just regular FSD on a traditional Tesla vehicle. Thank you.\\nElon Musk: Well, obviously the real reason that they cancel it is because GM can't make it work, not because the regulators, they're blaming regulators. That's misleading of them to do so, because Waymo is doing just fine in those markets. So it's just that their technology is not far.\\nGeorge Gianarikas: Right. And maybe just as a follow-up, I think you mentioned, that FSD take rates were up materially after you reduced the price. Is there any way you can help us quantify what that means Exactly? Thank you.\\nVaibhav Taneja: Yes, we shared the [indiscernible] that there we've seen a meaningful increase. I don't want to get into specific because we started from a low base and -- but we are seeing encouraging results. And the key thing here is, like Elon said, you need to experience it because words can't describe it till the time we actually use it. And that's why we are trying to make sure that every time a car is getting delivered, people are being showed how this thing is working because when you see it working, you realize how great it is. I mean, just to give you one example, so again, there's a bias example, but I have a more than 20 mile commute into the factory almost every day. I have zero interventions on the latest stack, and the card just literally drives me over. And especially with the latest version wherein, we are also tracking your eye movement, the steering wheel lag is almost not there as long as you're not wearing sunglasses.\\nElon Musk: Well, we are fixing the sunglasses thing. It's coming soon. So you will be able to drive -- you'll be able to have sunglasses on and have the car drive.\\nGeorge Gianarikas: Yes.\\nElon Musk: So -- but there's number of times I've talked with smart people who like live in New York or maybe downtown Boston and don't ever drive and then ask me about FSD, I'm like, you can just get a car and try it. And if you're not doing that, you have no idea what's going on.\\nTravis Axelrod: Thank you. The next question comes from Pierre from New Street. Pierre, please unmute yourself.\\nFerragu Pierre: Hey, guys. Thank you for taking my question. So it's on Robotaxi again, and I completely get it that with a universal solution, we will get like regulatory approval, we'll get there eventually clicking up miles and compute, et cetera. And my question is more, how you think about deployments, because I'm still like, I'm thinking once you have a car that can drive everywhere, that can replace me, it can replace a taxi, but then to do the right hailing service, you need a certain scale. And that means a lot of cars on the road and so you need an infrastructure to just maintain the cars, take care of them, et cetera. And so my question is, are you already working on that? Do you have already an idea of what, like your plan to deploy looks like? And is that like a test Tesla only plan or are you looking at partners, local partners, global partners to do that? And I'll have a quick follow-up.\\nElon Musk: Yes. This would just be the Tesla network. You just literally open the Tesla app and summon a car and resend a car to pick you up and take you somewhere. And you can -- our -- we'll have a fleet that's I don't know, on order of 7 million dedicated global autonomy soon. In the years come it'll be over 10 million, then over 20 million. This is immense scale. And the car is able to operate 24/7, unlike the human driver. So, the capability to -- like, if there's this basically instant scale with a software update. And now this is for a customer on fleet. So you can think of that as being a bit like Airbnb, like you can choose to allow your car to be used by the fleet, or cancel that and bring it back. It can be used by the fleet all the time. It can be used by the fleet some of the time, and then Tesla would take -- would share on the revenue with the customer. But you can think of the giant fleet of Tesla vehicles as like a giant sort of Airbnb equivalent fleet, Airbnb on wheels. The -- I mean, then in addition we would make some number of cars for Tesla that would just be owned by Tesla and be added to the fleet. I guess that would be a bit more like Uber. But this would all be a Tesla network. And there's an important clause we've put in, in every Tesla purchase, which is that the Tesla vehicles can only be used in the Tesla fleet. They cannot be used by a third-party for autonomy.\\nFerragu Pierre: Okay. And do you think that scale is like progressively so you can start in a city with just a handful of cars and you grow the number of cars over time? Or do you think there is like a critical mass you need to get to, to be able to offer like a service that is of competitive quality compared to what like the -- like Uber would be typically delivering already?\\nElon Musk: I guess I'm not -- maybe I'm not conveying this correctly. The entire Tesla fleet basically becomes active. This is obviously maybe there's some number of people who don't want their car to own money, but I think most people will. It's instant scale.\\nTravis Axelrod: Thank you. Our next question comes from Colin from Oppenheimer. Colin, please unmute yourself.\\nColin Rusch: Sorry about that guys. I've got two questions around energy storage. With the tight supply and the stationary storage, can you talk about your pricing strategy and how you're thinking about saturation and given geographies given that some of these larger systems are starting to shift wholesale power markets in a pretty meaningful way quickly?\\nVaibhav Taneja: So, I mean, we are working with a large set of players in the market and our pipeline is actually pretty long. And there's actually very -- there's actually long end in terms of where you enter into a contract where delivery started -- starts happening. And so far we have good pricing leverage. And now Mike, chime in on this too.\\nUnidentified Company Representative: Yes, I mean there's a lot of competition from Chinese OEMs just like there is in the vehicle space. So we're in close contact with our customers and making sure that we're remaining competitive in where they're needing to be competitive to, to secure contracts to sell power and energy in the markets. We had a really strong contracting quarter and continue to build our backlog for 2025 and 2026. So we feel pretty good about where we are in the market. We realize that competition is strong, but we have a pretty strong value proposition with offering a fully integrated product with our own power electronics and site level controls. So …\\nVaibhav Taneja: Yes, and again, the aspect which people miss do not fully understand is that there's also a whole software stack, which comes with from Megapack, right? And that is a unique proposition which we -- which is only available to us, and we are using it with other stuff too, but that gives us a much more of an edge as compared to the competition.\\nElon Musk: Yes, we find customers that they can sort of put together a hodgepodge solution. And so, and then sometimes they'll pick that solution, and then that doesn't work. And then they come back to us.\\nUnidentified Company Representative: Yes, and we're not really seeing saturation for like, on a global scale. There's little pockets of saturation in different markets, but we're more seeing that there's markets opening up given demand on the grid just continues to increase more than anyone expects. So that just opens up markets, really across the world in different pockets.\\nVaibhav Taneja: Yes, I mean just even on the AI computer side, right? These GPUs are really powerful already and the amount of new pipeline, which we're getting for people for data center backup and things like that is increasing at a pretty large scale.\\nColin Rusch: Yes. Thanks. And then the follow-up here is 4680 process technology and the role to role process. There's some news around your equipment suppliers. Can you talk about how far along you are in, in potentially qualifying an incremental supplier around some of that, those critical process technology steps?\\nLars Moravy: Yes, I can talk about that. As you're probably referring to the lawsuit that we have with one of our suppliers, look, I don't think this is going to affect our ability to roll out 4680. We have very strong IP position in the technology and the majority of the equipment that we use is in-house designed and some of it's in-house build. And so we can take our IP stack and have someone else build it if we need to. So it's, that's not really a concern right now.\\nElon Musk: Yes. I, I think people don't understand just how much demand there will be for grid storage. They really just like the [indiscernible] I think are underestimating this demand by probably orders magnitude. So that the actual energy, total energy output of, say the U.S grid is if the power plants can operate a steady state is at least two to three times, the amount of energy it currently produces, because there are a huge gap. There's a huge difference in the -- from peak to trough in terms of energy of power generation. So in order for a grid to not have blackouts, it must be able to support the load at the worst minute of the worst day of the year, the coldest or hottest day, which means that for the rest of the time, the rest of the year, it's got massive excess power generation capability, but it has no way to store that energy. Once you add battery packs, you can now run the power plants at steady state. Steady state means that basically any given grid anywhere in the world can produce in terms of cumulative energy in the course of the year, at least twice what it is currently producing in some cases, maybe three times.\\nTravis Axelrod: All right. Thank you, Elon. The next question comes from Colin Langan from Wells Fargo. Colin, please unmute yourself.\\nColin Langan: Oh, great. Thanks for taking my questions. Do you hear me?\\nTravis Axelrod: Yes.\\nColin Langan: Yes. Sorry. I guess when we are going to ask, if Trump wins, there's a higher chance that IRA could get cut. I think Elon, you had commented online that Tesla doesn't survive on EV subsidies. But when Tesla lose a lot of support if IRA goes away? I think model Y3 and Y get IRA help for customers, and I think your batteries get production tax credits. So, just one, can you clarify if the end, if IRA ends, would it be a negative for your profitability in the near-term? Why might it not be a negative? And then, any framing of the current support you get, IRA-related?\\nElon Musk: I guess that there would be like some impact, but I think it would be devastating for our competitors. But -- and it would hurt Tesla slightly. But long-term probably actually helps Tesla would be my guess. Yes -- but I've said this before on earnings calls, it -- the value of Tesla overwhelmingly is autonomy. These other things are in the noise relative to autonomy. So I recommend anyone who doesn't believe that Tesla will solve vehicle autonomy should not hold Tesla stock. They should sell their Tesla stock. You should believe Tesla will solve autonomy, you should buy Tesla stock. And all these other questions are in the noise.\\nVaibhav Taneja: Yes, I mean, I'll add this just to clarify a few things that -- at the end of the day, when we are looking at our business, we've always been looking at it whether or not IRA is there and we want our business to grow healthy without having any subsidies coming in, whichever way you look at it. And that's the way we have always modeled everything. And that is the way internally also even when we are looking at battery costs, yes, I --, there are manufacturing credits which we get, but we always drive ourselves to say, okay, what if there is no higher benefit and how do we operate in that kind of an environment? And like Elon said, we definitely have a big advantage as compared to a competition on that front. We've delivered it and you can see it in the numbers over the years. Like, so there is you cannot ignore the fundamental size of the business. And then on top of it, once you add autonomy to it, like even said, it becomes meaningless to you think about the short-term.\\nTravis Axelrod: Okay. I think that's unfortunately all the time we have for today. We appreciate all of your questions. We look forward to talking to you next quarter. Thank you very much and goodbye.\\nElon Musk: That's excellent.\",\n",
       "   \"Martin Viecha: Tesla's First Quarter 2024 Q&A Webcast. My name is Martin Viecha, VP of Investor Relations, and I'm joined today by Elon Musk, Vaibhav Taneja, and a number of other executives. Our Q1 results were announced at about 3.00 p.m. Central Time in the Update Deck we published at the same link as this webcast. During this call, we will discuss our business outlook and make forward-looking statements. These comments are based on our predictions and expectations as of today. Actual events and results could differ materially due to a number of risks and uncertainties, including those mentioned in our most recent filings with the SEC. During the question-and-answer portion of today's call, please limit yourself to one question and one follow-up. Please use the raise hand button to join the question queue. But before we jump into Q&A, Elon has some opening remarks. Elon?\\nElon Musk: Thanks, Martin. So to recap in Q1 we navigated several unforeseen challenges as well as the ramp of the updated Model 3 in Fremont. There was, as we all have seen, the EV adoption rate globally is under pressure and a lot of other auto manufacturers are pulling back on EVs and pursuing plug-in hybrids instead. We believe this is not the right strategy and electric vehicles will ultimately dominate the market. Despite these challenges, the Tesla team did a great job executing in a tough environment and energy storage deployments, the Megapack in particular, reached an all time high in Q1, leading to record profitability for the energy business, and that looks likely to continue to increase in the quarters and years ahead. It will increase. We actually know that it will, so significantly faster than the car business as we expected. We also continue to expand our AI training capacity in Q1, more than doubling our training compute sequentially. In terms of the new product roadmap, there has been a lot of talk about our upcoming vehicle line in the next – in the past several weeks. We've updated our future vehicle lineup to accelerate the launch of new models ahead, previously mentioned startup production in the second half of 2025, so we expect it to be more like the early 2025, if not late this year. These new vehicles, including more affordable models, will use aspects of the next generation platform as well as aspects of our current platforms, and will be able to produce on the same manufacturing lines as our current vehicle lineup. So it's not contingent on any new factory or massive new production line. It'll be made on our current production lines much more efficiently. And we think this should allow us to get to over 3 million vehicles of capacity when realized to the full extent. Regarding FSD Version 12, which is the pure AI-based self-driving, if you haven't experienced this, I strongly urge you to try it out. It's profound and the rate of improvement is rapid so – and we've now turned that on for all cars with the cameras and inference computer and everything from Hardware 3 on in North America. And so it's been pushed out to, I think, around 1.8 million vehicles and we're seeing about half of people use it so far and that percentage is increasing with each passing week. So we now have over 300 billion miles that have been driven with FSD V12. Since the launch of full self-driving, supervised full self-driving, it's become very clear that the vision-based approach with end to end neural networks is the right solution for scalable autonomy. It's really how humans drive. Our entire road network is designed for biological neural nets and eyes. So naturally cameras and digital neural nets are the solution to our current road system. To make it more accessible, we've reduced the subscription price to $99 a month, so it's easy to try out. And as we've announced, we'll be showcasing our purpose-built robotaxi, or Cybercab, in August. Yes. Regarding AI compute, over the past few months, we've been actively working on expanding Tesla's core AI infrastructure. For a while there, we were training constrained in our progress. We are, at this point, no longer training constrained and so we're making rapid progress. We've installed and commissioned, meaning they're actually working 35,000 H100 computers or GPUs, GPU is wrong word, they need a new word. I always feel like a wince when I say GPU because it's not – GPU stands – G stands for graphics, and it doesn't do graphics. But anyway roughly 35,000 H100s are active, and we expect that to be probably 85,000 or thereabouts by the end of this year and training, just for training. We are making sure that we're being as efficient as possible in our training. It's not just about the number of H100s, but how efficiently they're used. So in conclusion, we're super excited about our autonomy road map. I think it should be obvious to anyone who's driving Version 12 and it tells that that it is only a matter of time before we exceed the reliability of humans and not much time with that. And we're really headed for an electric vehicle, an autonomous future. And I'll go back to something I said several years ago that in the future, gasoline cars that are not autonomous will be like riding a horse and using a flip phone. And that will become very obvious in hindsight. We continue to make the necessary investments that will drive growth and profits for Tesla in the future, and I wanted to thank the Tesla team for incredible execution during this period and look forward to everything that we have planned ahead. Thanks.\\nMartin Viecha: Thank you very much, and Vaibhav has some comments as well.\\nVaibhav Taneja: Thanks. It's important to acknowledge what Elon said, from our auto business perspective. We did see a seasonal decline in revenues quarter-over-quarter and those were primarily because of seasonality, uncertain macroeconomic environment and the other reasons, which Elon had mentioned earlier. Auto margins declined from 18.9% to 18.5%. Excluding the impact of Cybertruck, the impact of pricing actions was largely offset by reductions in per unit costs and the recognition of revenue from Autopark feature for certain vehicles in the U.S. that previously did not have that functionality. Additionally, while we did experience higher cost due to the ramp of Model 3 in Fremont and disruptions in Berlin, these costs were largely offset by cost reduction initiatives. In fact, if we exclude Cybertruck and Fremont Model 3 ramp costs, the revenue from Autopark, auto margins improved slightly. Currently normalized Model Y cost per vehicle in Austin and Berlin are already very close to that of Fremont. Our ability to reduce costs without sacrificing on quality was due to the amazing efforts of the team, in executing Tesla's relentless pursuit of efficiency across the business. We've also witnessed that as other OEMs are pulling back on their investments in EV, there is increasing appetite for credits, and that means a steady stream of revenue for us. Obviously, seeing others pull back from EV is not the future we want. We would prefer it the whole industry went all in. On the demand front, we've undertaken a variety of initiatives, including lowering the price of both the purchase and subscription options for FSD launching extremely attractive leasing specials for the Model 3 in the U.S. for $299 a month and offering attractive financing options in certain markets. We believe that our awareness activities, paired with attractive financing, will go a long way in expanding our reach and driving demand for our products. Our Energy business continues to make meaningful progress with margins reaching a record of 24.6%. We expect the energy storage deployments for 2024 to grow at least 75% higher from 2023. And accordingly, this business will begin contributing significantly to our overall profitability. Note that there is a bit of lumpiness in our storage deployments due to a variety of factors that are outside of our control, so deployments may fluctuate quarter-over-quarter. On the operating expense front, we saw a sequential increase from our AI initiatives, continued investment in future projects, marketing and other activities. We had negative free cash flow of $2.5 billion in the first quarter. The primary driver of this was an increase in inventory from a mismatch between builds and deliveries as discussed before, and our elevated spend on CapEx across various initiatives, including AI compute. We expect the inventory build to reverse in the second quarter and free cash flow to return to positive again. As we prepare the company for the next phase of growth, we had to make the hard but necessary decision to reduce our head count by over 10%. The savings generated are expected to be well in excess of $1 billion on an annual run rate basis. We are also getting hyper focused on CapEx efficiency and utilizing our installed capacity in a more efficient manner. The savings from these initiatives, including our cost reductions will help improve our overall profitability and ultimately enable us to increase the scale of our investments in AI. In conclusion, the future is extremely bright and the journey to get there while challenging will be extremely rewarding. Once again, I would like to thank the whole Tesla team for delivering great results. And we can open it up to Q&A.\\nA - Martin Viecha: Okay. Let's start with investor Q&A. The first question is, what is the status of 4680. What is the current output? Lars?\\nLars Moravy : Sure. 4680 production increased about 18% to 20% from Q4 reaching greater than 1K a week for Cybertruck, which is about 7 gigawatt hours per year as we posted on X. We expect to stay ahead of the Cybertruck ramp with the cell production throughout Q2 as we ramp the third of four lines in Phase 1, while maintaining multiple weeks of cell inventory to make sure we're ahead of the ramp. Because we're ramping, COGS continues to drop rapidly week-over-week driven by yield improvements throughout the lines and production volume increases. So our goal, and we expect to do this is to beat supplier cost of nickel-based cells by the end of the year.\\nMartin Viecha: Thank you. The second question is on Optimus. So what is the current status of Optimus? Are they currently performing any factory tasks? When do you expect to start mass production?\\nElon Musk: We are able to do simple factory tasks or at least, I should say, factory tasks in the lab. In terms of – we do think we will have Optimus in limited production in the natural factory itself, doing useful tasks before the end of this year. And then I think we may be able to sell it externally by the end of next year. These are just guesses. As I've said before, I think Optimus will be more valuable than everything else combined. Because if you've got a sentient humanoid robots that is able to navigate reality and do tasks at request, there is no meaningful limit to the size of the economy. So that's what is going to happen. And I think Tesla is best positioned of any humanoid robot maker to be able to reach volume production with efficient inference on the robot itself. I mean this perhaps is a point that is worth emphasizing Tesla's AI inference efficiency is vastly better than any other company. There is no company even close to the inference efficiency of Tesla. We've had to do that because we were constrained by the inference hardware in the car, we didn't have a choice. But that will pay dividends in many ways.\\nMartin Viecha: Thank you. The third question is, what is the current assessment of the pathway towards regulatory approval for unsupervised FSD in the U.S. And how should we think about the appropriate safety threshold compared to human drivers?\\nElon Musk: Sure.\\nLars Moravy: I can start. There are a handful of states that already have adopted autonomous vehicle laws. These states are paving the way for operations, while the data for such operations guides a broader adoption of driver-less vehicles. I think Ashok can talk a little bit about our safety methodology, but we expect that these states and the work ongoing as well as the data that we're providing will pave a way for a broad-based regulatory approval in the U.S. at least and then in other countries as well?\\nAshok Elluswamy: Yes. It's actually been pretty helpful that other autonomous car companies have been cutting a path through the regulatory jungle, which is absurd. That's actually quite helpful. And they have obviously been operating in San Francisco for a while. I think they got approval for City of LA. So these approvals are happening rapidly. I think if you've got at scale, a statistically significant amount of data that shows conclusively that the autonomous car has, let's say, half the accident rate of a human-driven car, I think, that's difficult to ignore because at that point, stopping autonomy means killing people. So I actually do not think that there will be significant regulatory barriers provided there was conclusive data that the autonomous car is safer than a human-driven car. And in my view, this will be much like elevators. Elevators used to be operated by a guy with relay switch. But sometimes that guy would get tired or drunk or just make a mistake, and shatter somebody in half between floors. So we just get an elevator and press button, we don't think about it. In fact, it's kind of weird if somebody is standing there with a relay switch. And that will be how cars work. You just summon the car using your phone, you get in, it takes you to a destination, you get out.\\nVaibhav Taneja: You don't even think about it?\\nElon Musk: You don't even think about it. Just like an elevator, it takes you to your floor. That's it. Don't think about how the elevator is working or anything like that. And something I should clarify is that Tesla will be operating the fleet. So you can think of like how Tesla, think of it’s like some combination of Airbnb and Uber, meaning that there will be some number of cars that Tesla owns itself and operates in the fleet. There will be some number of cars and then there'll be a bunch of cars where they're owned by the end user. That end user can add or subtract their car to the fleet whenever they want, and they can decide if they want to only let the car be used by friends and family or only by 5-star users or by anyone at any time they could have the car come back to them and be exclusively theirs, like an Airbnb. You could rent out your guest room or not, any time you want. So as our fleet grows, we have 7 million cars going to – 9 million cars going to, eventually tens of millions of cars worldwide. With a constant feedback loop, every time something goes wrong, that gets added to the training data and you get this training flywheel happening in the same way that Google Search has the sort of flywheel, it's very difficult to compete with Google because people are constantly doing searches and clicking and Google is getting that feedback loop. It’s the same with Tesla. But at a scale that is maybe difficult to comprehend, but ultimately, it will be tens of millions. I think there's also some potential here for an AWS element down the road where if we've got very powerful inference because we've got a Hardware 3 in the cars, but now all cars are being made with Hardware 4. Hardware 5 is pretty much designed and should be in cars, hopefully towards the end of next year. And there's a potential to run – when the car is not moving to actually run distributed inference. So kind of like AWS, but distributed inference. Like it takes a lot of computers to train an AI model, but many orders of magnitude less compute to run it. So if you can imagine future, perhaps where there's a fleet of 100 million Teslas, and on average, they've got like maybe a kilowatt of inference compute. That's 100 gigawatts of inference compute distributed all around the world. It's pretty hard to put together 100 gigawatts of AI compute. And even in an autonomous future where the car is, perhaps, used instead of being used 10 hours a week, it is used 50 hours a week. That still leaves over 100 hours a week where the car inference computer could be doing something else. And it seems like it will be a waste not to use it.\\nMartin Viecha: Ashok, do you want to chime in on the air process and safety?\\nAshok Elluswamy: Yes, we have multiple tiers of validating the safety in any given week, we train hundreds of neural networks that can produce different trajectories for how to drive the car, we replay them through the millions of clips that we have already collected from our users and our own QA. Those are like critical events, like someone jumping out in front or like other critical events that we have gathered database over many, many years, and we replay through all of them to make sure that we are net improving safety. And on top of it, we have simulation systems that also try to recreate this and test this in closed loop fashion. And some of this is validated, we give it to our own QA drivers. We have hundreds of them in different cities, in San Francisco, Los Angeles, Austin, New York, a lot of different locations. They are also driving this and collecting real-world miles, and we have an estimate of what are the critical events, are they a net improvement compared to the previous week’s builds. And once we have confidence that the build is a net improvement, then we start shipping to early users, like 2,000 employees initially that they would like it to build, they will give feedback on like if it's an improvement there or they're noting some new issues that we did not capture in our own QA process. And only after all of this is validated, then we go to external customers. And even when we go external, we have like live dashboards of monitoring every critical event that's happening in the fleet sorted by the criticality of it. So we are having a constant pulse on the build quality and the safety improvement along the way. And then any failures like Elon alluded to, we get the data back, add it to the training and that improves the model in the next cycle. So we have this like constant feedback loop of issues, fixes, evaluations and then rinse and repeat. And especially with the new V12 architecture, all of this is automatically improving without requiring much engineering interventions in the sense that engineers don't have to be creative in like how they code the algorithms. It's mostly learning on its own based on data. So you see that, okay, every failure or like this is how a person shows, this is how you drive this intersection or something like that, they get the data back. We add it to the neural network, and it learns from that trained data automatically instead of some engineers saying that, oh, here, you must rotate the steering wheel by this much or something like that. There's no hard inference conditions, it's everything is neural network, it's very soft, it's probabilistic. So it will adapt its probability distribution based on the new data that it's getting.\\nElon Musk: Yes. We do have some insight into how good the things will be in like, let's say, three or four months because we have advanced models that are far more capable than what is in the car, but have some issues with them that we need to fix. So they are like there'll be a step change improvement in the capabilities of the car, but it will have some quirks that are – that need to be addressed in order to release it. As Ashok was saying, we have to be very careful in what we release the fleet or to customers in general. So like – if we look at say 12.4 and 12.5, which are really could arguably even be Version 13, Version 14 because it's pretty close to a total retrain of the neural nets in each case are substantially different. So we have good insight into where the model is, how well the car will perform, in, say, three or four months.\\nAshok Elluswamy: Yes. In terms of scaling laws, people in the AI community generally talk about model scaling laws where they increase the model size a lot and then their corresponding gains in performance, but we have also figured out scaling laws and other access in addition to the model side scaling, making also data scaling. You can increase the amount of data you use to train the neural network and that also gives similar gains and you can also scale up by training compute, you can train it for much longer or make more GPUs or more Dojo nodes and that also gives better performance, and you can also have architecture scaling where you count with better architectures that for the same amount of compute for produce better results. So a combination of model size scaling, data scaling, training compute scaling and the architecture scaling, we can basically extract like, okay, with the continue scaling based on this – at this ratio, we can sort of predict future performance. Obviously, it takes time to do the experiments because it takes a few weeks to train, it takes a few weeks to collect tens of millions of video clips and process all of them, but you can estimate what’s going to be the future progress based on the trends that we have seen in the past, and they’re generally held true based on past data.\\nMartin Viecha: Okay. Thank you very much. I’ll go to the next question, which is, can we get an official announcement of the time line for the $25,000 vehicle?\\nLars Moravy: I think we – Elon mentioned it in the opening remarks. But as you mentioned, we’re updating our future vehicle lineup to accelerate the launch of our low-cost vehicles in a more CapEx efficient way. That’s our mission to get the most affordable cars to customers as fast as possible. These new vehicles we built on our existing lines and open capacity, and that’s a major shift to utilize all our capacity with marginal CapEx before we go spend high CapEx to do anything.\\nElon Musk: Yes. We’ll talk about this more on August 8. But really, the way to think of Tesla is almost entirely in terms of solving autonomy and being able to turn on that autonomy for a gigantic fleet. And I think it might be the biggest asset value appreciation history when that day happens when you can do unsupervised full self-driving.\\nLars Moravy: 5 million cars?\\nElon Musk: Yes.\\nLars Moravy: A little less?\\nElon Musk: Yes. It will be 7 million cars in a year or so and then 10 million and then eventually, we’re talking about tens of millions of cars. Not eventually, it’s like, yes, for the end of the decade, its several tens of millions of cars I think.\\nMartin Viecha: Thank you. The next question is, what is the progress of Cybertruck ramp?\\nLars Moravy: I can take that one too. Cybertruck had 1K a week just a couple of weeks ago. This happened in the first four to five months since we SOP [ph] late last year. Of course, volume production is what matters. That’s what drives costs and so our costs are dropping, but the ramp still faces like a lot of challenges with so many new technologies, some supplier limitations, et cetera, and continue to ramp this year, just focusing on cost efficiency and quality.\\nMartin Viecha: Okay. Thank you. The next question, have any of the legacy automakers contacted Tesla about possibly licensing FSD in the future?\\nElon Musk: We’re in conversations with one major automaker regarding licensing FSD.\\nMartin Viecha: Thank you. The next question is about the robotaxi unveil. Elon already talked about that. So we’ll have to wait till August. The following question is about the next-generation vehicle. We already talked about that. So let’s go to the semi. What is the time line for scaling semi?\\nElon Musk: I think…\\nLars Moravy: So we’re finalizing the engineering of the semi to enable like a super cost-effective high-volume production with our learnings from our fleet and our pilot fleet and Pepsi’s fleet, which we are expanding this year marginally. In parallel, as we showed in the shareholders’ deck, we have started construction on the factory in Reno. Our first vehicles are planned for late 2025 with external customers starting in 2026.\\nMartin Viecha: Okay. A couple more questions. So our favorite, can we make FSD transfer permanent until FSD is fully delivered with Level 5 autonomy?\\nLars Moravy: Yes.\\nMartin Viecha: Okay. Next question, what is the getting the production ramp at Lathrop, where do you see the Megapack run rate at the end of the year. Mike?\\nUnidentified Company Representative: Yes. Yes, Lathrop is ramping as planned. We have our second GA line allowing us to increase our exit rate from 20 gigawatt hours per year to – at the start of this year to 40 gigawatt hours per year by the end of the year, that lines commissioned. There’s really nothing limiting the ramp. Its given the longer sales cycles for these large projects, we typically have order visibility 12 months to 24 months prior to ship dates. So we’re able to plan – the build plan several quarters in advance. So this allows us to ramp the factory to align with the business and order growth. Lastly, we’d like to thank our customers globally for their trust in Tesla as a partner for these incredible projects.\\nMartin Viecha: Okay. Thank you very much. Let’s go to analyst questions. The first question comes from Tony Sacconaghi from Bernstein. Tony, please go ahead and unmute.\\nTony Sacconaghi: Thank you for taking the question. I was just wondering if you can elaborate a little bit more on kind of the new vehicles that you talked about today. Are these like tweaks on existing models, given that they’re going to be running on the same lines? Are these like new models? And how should we think about them in the context of like the Model 3 Highland update, what will these models be like relative to that? And given the quick time frame, Model 3 Highland has required a lot of work and a lot of retooling. Maybe you can help put that all in context. Thank you, and I have a follow-up, please.\\nElon Musk: I think we've said, we were on that front. So what’s your follow-up?\\nTony Sacconaghi: It’s a more personal one for you, Elon, which is that you’re leading many important companies right now. Maybe you can just talk about where your heart is at in terms of your interests and do you expect to lessen your involvement with Tesla at any point over the next three years?\\nElon Musk: Tesla constitutes a majority of my work time and I work pretty much every day of the week. It’s rare for me to take a Sunday afternoon. So I’m going to make sure Tesla is quite prosperous. And it is – like it is prosperous and it will be very much so in the future.\\nMartin Viecha: Okay. Thank you. Let’s go to Adam Jonas from Morgan Stanley. Adam, please go ahead and unmute.\\nAdam Jonas: Okay. Great. Hey, Elon. So you and your team on volume expect a 2024 growth rate, notably lower than that achieved in 2023. But what's your team's degree of confidence on growth above 0%? Or in other words, does that statement leave room for potentially lower sales year-on-year?\\nElon Musk: No, I think we'll have higher sales this year than last year.\\nAdam Jonas: Okay. My follow-up, Elon, on future product. If you had nailed execution, assuming that you nail execution on your next-gen cheaper vehicles, more aggressive giga castings, I don't want to say one piece, but getting closer to one piece, structural pack, unboxed, 300-mile range, $25,000 price point, putting aside robotaxi, those features unique to you. How long would it take your best Chinese competitors to copy a cheaper and better vehicle that you could offer a couple of years from now? How long would it take your best Chinese competitors to copy that? Thanks.\\nElon Musk: I mean, I don't know what our competitors could do, except we've done relatively better than they have. If you look at the drop in our competitors in China sales versus our drop in sales, our drop was less than theirs. So we're doing well. But I think Cathy Wood said it best, like really, we should be thought of as an AI or robotics company. If you value Tesla as just like an auto company, you just have to – fundamentally, it's just the wrong framework and it will come to be. If you ask the wrong question, then the right answer is impossible. So I mean, if somebody doesn't believe Tesla is going to solve autonomy, I think they should not be an investor in the company. Like, that is – but we will and we are. And then you have a car that goes from 10 hours of use a week, like 1.5 hours a day to probably 50%, but it costs the same.\\nVaibhav Taneja: I think that's the key thing to remember, right, especially if you look at FSD Supervised, if you didn't believe in autonomy, this should give you a review that this is coming. It's actually getting better day by day.\\nElon Musk: Yes. If you've not tried the FSD 12.3, and like I said, 12.4 is going to be significantly better and 12.5 even better than that. And we have visibility into those things. Then you really don't understand what's going on. It's not possible.\\nVaibhav Taneja: Yes. And that's why we can't just look at just as a car company because a car company would just have a car. But here, we have more than a car company because the cars can be autonomous. And like I said, it's happening.\\nAshok Elluswamy: Yes. This is all in addition to Tesla – the overall AI community is just like increasing – like, improving rapidly.\\nElon Musk: Yes. I mean we're putting the actual auto in automobile. So sort of – we go like, well, sort of like tell us about future horse carriages you're making. I'm like, well, actually, it doesn't need a horse that's the whole point. That's really the whole point.\\nMartin Viecha: Okay, thank you. The next question comes from Alex Potter from Piper Sandler. Alex, please go ahead and unmute.\\nAlex Potter: Great, thanks. Yes, so I couldn't agree more. The thesis hinges completely on AI, the future of AI, full self-driving neural net training, all of these things. In that context, Elon, you've spoken about your desire to obtain 25% voting control of the company. And I understand completely why that would be. So I'm not necessarily asking about that. I'm asking if you've come up with any mechanism by which you can ensure that you'll obtain that level of voting control. Because if not, then the core part of the thesis could potentially be at risk. So any additional commentary you might have on that topic.\\nElon Musk: Well, I think no matter what Tesla, even if I got kidnapped by aliens tomorrow, Tesla will solve autonomy, maybe a little slower, but it would solve autonomy for vehicles at least. I don't know if it would winon with respect to Optimus or with respect to future products, but it would that there's enough momentum for Tesla to solve autonomy even if I disappeared for vehicles. Yes, there's a whole range of things we can do in the future beyond that. I'll be more reticent with respect to Optimus, if we have a super-sentient humanoid robot that can follow you indoors and that you can escape, we're talking terminator-level risk. And yes, I'd be uncomfortable with. If there's not some meaningful level of influence over how that is deployed. And if there's shareholders have an opportunity to ratify or reratify the sort of competition because I can't say that. That is a fact. They have an opportunity. And yes, we'll see. If the company generates a lot of positive cash flow, we could obviously buy back shares.\\nAlex Potter: All right. That's actually all very helpful context. Thank you. Maybe one final question and I'll pass it on. OpEx reductions, thank you for quantifying the impact there. I'd be interested also in potentially more qualitative discussion of what the implications are for these headcount reductions. What are the types of activities that you're presumably sacrificing as a result of parting ways with these folks? Thanks very much.\\nVaibhav Taneja: So like we said, we've done these headcount reductions across the board. And as companies grow over time, there are certain redundancies. There's some duplication of efforts, which happens in certain areas. So you need to go back and look at where all these pockets are, get rid of it. So we're basically going through that exercise wherein we're like, hey, how do we set this company right for the next phase of growth. And the way to think about it is any tree which grows, it needs pruning. This is the pruning exercise which we went through. And at the end of it, we'll be much stronger and much more resilient to deal with the future because the future is really bright. Like I said in my opening remarks, we just have to get through this period and get there.\\nElon Musk: Yes, we're not giving up anything that is significant that I'm aware of. So we've had a long period of prosperity from 2019 to now. And so if a company sort of organizationally is 5% wrong per year, that accumulates to 25%, 30% of inefficiency. We've made some corrections along the way. But it is time to reorganize the company for the next phase of growth and you really need to reorganize it, just like a human when we start off with one cell and kind of zygote, blastocyst and you start growing arms and legs and briefly, you have a tail. And so…\\nAlex Potter: But you shed the tail.\\nElon Musk: You shed the tail, hopefully. And then you're baby, you basically, you have to be the organism – a company is kind of like creature growing. And if you don't reorganize it for different phases of growth, it will fail. You can't have the same organizational structure if you're 10 cells versus 100 cells versus 1 million cells versus 1 billion cells versus 1 trillion cells. Humans are around 35 trillion cells, doesn't feel like it feels like, like one person. But you're basically a walking cell colony of roughly 35 trillion depending on your body mass and about three times that number in bacteria. So anyway, you've got to reorganize the company for a new phase of growth or will fail to achieve that growth.\\nMartin Viecha: Thank you. Let's go to Mark Delaney from Goldman Sachs. Mark please go ahead and unmute.\\nMark Delaney: Yes. Good afternoon. Thanks very much for taking the question. The company previously characterized potential FSD licensing discussions in the early phase and some OEMs had not really been believing in it. Can you elaborate on how much the licensing business opportunity you mentioned today has progressed? And is there anything Tesla needs to achieve with the technology in terms of product milestones in order to be successful at reaching a licensing agreement in your view?\\nElon Musk: Well, I think we just need to – it just needs to be obvious that our approach is the right approach. And I think it is. I think we've now with 12.3, if you just have the car drive you around; it is obvious that our solution with a relatively low-cost inference computer and standard cameras can achieve self-driving. No LiDARs, no radars, no ultrasonic nothing.\\nVaibhav Taneja: No heavy integration work for vehicle manufacturers.\\nElon Musk: Yes. So it really just be a case of having them use the same cameras and inference computer and licensing our software. But once it becomes obvious that if you don't have this in a car, nobody wants your car. It's a smart car. I still remember in, back when Nokia was king of the hill, Yes, crushing. And they certainly come out with a smartphone that was basically a break with limited functionality. And then the iPhone and Android, people still do not understand that all the phones are going to be that way. There's not going to be any flip [ph] phones. If there will be a niche product.\\nLars Moravy: Or home phones.\\nElon Musk: Yes, no even exactly. When is the last time you saw a home phone.\\nLars Moravy: No idea in a hotel, sometimes in hotels.\\nElon Musk: Yes, the hotels have them. Yes. So the people don't understand all cars will need to be smart cars, or you will not sell or the car will not – nobody would buy it. Once that becomes obvious, I think licensing becomes not optional.\\nMark Delaney: It becomes a method of survival?\\nElon Musk: Yes, absolutely, it is. License it or nobody will buy your car.\\nVaibhav Taneja: I mean one other thing which I'll add is in the conversations, which we've had with some of these OEMs, I just want to also point out that they take a lot of time in their product life cycle.\\nElon Musk: Yes.\\nVaibhav Taneja: They're talking about years before they will put it in their product. We might have a licensing deal earlier than that, but it takes a while. So this is where the big difference between us and them is, right?\\nElon Musk: Yes, I mean, really a deal signed now would result in it being in a car probably three years.\\nVaibhav Taneja: That would be early.\\nElon Musk: Yes. That's like lightening basically.\\nLars Moravy: That's in eager [ph] OEM.\\nElon Musk: Yes. So I wouldn't be surprise if we do sign a deal. I think we have a good chance we do sign a deal this year, maybe more than one. But yes, it would be probably three years before it's integrated with a car. Even though all you need is cameras and our inference computer. So just talking about a massive design change.\\nVaibhav Taneja: Yes. And again, just to clarify, it's not the work which we have to do. It's the work which they have to do, which will take the time.\\nElon Musk: Yes.\\nVaibhav Taneja: Mark, is it helpful?\\nMark Delaney: Yes, very helpful. Thank you. My follow-up was to better understand Tesla's approach to pricing going forward. Previously, the company had said that the price reductions were driving incremental demand with how affordable the cars have become, especially for vehicles that have access to IRA credits and some of the leasing offers that Tesla has in place. Do you still see meaningful incremental price reductions as making sense from here for the existing products? And can the company meaningfully lower prices from here and also stay free cash flow positive on an annual basis with the current product set? Thanks.\\nElon Musk: Yes. I think we can be free cash flow positive meaningfully.\\nLars Moravy: I think Vaibhav said it in his opening remarks, like our cost down efforts, we basically were offsetting the price cut like we’re trying to give it back to the customers.\\nElon Musk: Yes. I mean the end of the day, like for any given company, if you sell a great product at a great price – if you have a great product at a great price, the sales will be excellent. That’s true of any area. So over time, we do need to keep making sure that we’re – that it’s a great product at a great price. And moreover, that price is accessible to people. So it’s not – you have to solve both the value for money and the fundamental affordability question. The fundamental affordability question is sometimes overlooked. If somebody is earning several hundred thousand dollars a year, they don’t think of a car from a fundamental affordability standpoint. But from vast majority of people are living paycheck to paycheck. So it actually makes a difference if the cost per month for lease refinancing is $10 one way or the other. So it is important to keep improving the affordability and to keep making the price.\\nLars Moravy: More accessible.\\nElon Musk: Yes, exactly. Make the price more accessible, the value for money better, and to keep improving that over time.\\nLars Moravy: But also make kick as cost that people want to buy.\\nElon Musk: Yes, it’s going to be a great product and at a great price. And the standards for what constitutes great product at a great price keep increasing. So there’s like – you can’t just be static. You have to keep making the car better, improving the price, but improving the cost of production, and that’s what we’re doing.\\nVaibhav Taneja: Yes. And in fact, like I said in my opening remarks also, like the revised – the updated Model 3 is a fantastic car. I don’t think people fully even understand that lot of engineering effort which has gone and Lars and team have actually put out videos explaining how much the car is different. I mean it looks and feels different. Not only it looks and feels different. We’ve added so much value to it, but you can lease it for like as low as $299 a month.\\nLars Moravy: Without gas.\\nVaibhav Taneja: Yes.\\nMartin Viecha: All right. The next question comes from George from Canaccord. George, please go ahead and unmute.\\nUnidentified Analyst: Hi, thank you for taking my question. First, could you please help us understand some of the timing of launching FSD in additional geographies, including maybe clarifying your recent comment about China? Thank you.\\nElon Musk: I mean like new markets, yes, we are – there are a bunch of markets where we don’t currently sell cars that we should be selling cars in. We’ll see some acceleration of that.\\nUnidentified Analyst: And FSD new markets?\\nElon Musk: Yes. So think about the end-to-end neural net-based autonomy is that just like a human, it actually works pretty well without modification in almost any market. So we plan on – with the approval of the regulators, releasing it as a supervised autonomy system in any market that – where we can get regulatory approval for that, which we think includes China. So yes, it’s – just like a human, you can go rent a car in a foreign country and you can drive pretty well. Obviously, if you live in that country, you’ll drive better. And so we’ll make the car drive better in these other countries with country-specific training. But it can drive quite well almost everywhere.\\nVaibhav Taneja: The basics of driving are basically same everywhere like car is a car, the traffic lights, road is the road. Yes.\\nElon Musk: It understands that it shouldn’t hit things, no matter what the road rules are.\\nVaibhav Taneja: Exactly. There are some road rules that you need to follow. And in China, you shouldn’t cross over a solid line to do a lane change. In U.S. it’s a recommendation I think. In China, you get fined heavily if you do that. We have to do some more actions, but it’s mostly smaller reduction. It’s not like the entire change or type or something.\\nElon Musk: Yes.\\nMartin Viecha: Hey, George, do you have a follow-up?\\nUnidentified Analyst: Yes. So my follow-up has to do with the first quarter deliveries and I’m curious as to whether or not you feel that supply constraints that you mentioned throughout the release impacted the results and maybe can you help us quantify that? And is that why you have some confidence in unit growth in 2024?\\nVaibhav Taneja: Yes. I think we did cover this a little bit in the opening remarks to you. Q1 had a lot of different things which are happening. Seasonality was a big one, continued pressure from the macroeconomic environment. We had attacks at our factory. We had Red Sea attacks, we are ramping Model 3, we’re ramping Cybertruck. All these things are happening. I mean, it almost feels like a culmination of all those activities in a constrained period. And that gives us that confidence that, hey, we don’t expect these things to recur.\\nElon Musk: Yes. We think Q2 will be a lot better.\\nVaibhav Taneja: Yes.\\nLars Moravy: It’s just one thing after another. Our Cybertrucks are crazy. Thank you.\\nElon Musk: Yes, exactly. It’s just – if you’ve got cars that are sitting on ships, they obviously cannot delivered to people. And if you’ve got the excess demand for Model 3 and Model Y in one market, but you don’t have it there. It’s quite a – it’s extremely complex logistics situation. So I’d say also the – we did overcomplicate the sales process, which we’ve just in the past week or so have greatly simplified. So it became far too complex to buy a Tesla, whereas it should just be you can buy the car in under a minute. So we’re getting back to that you can buy a Tesla in under an minute interface from what was quite complex.\\nMartin Viecha: Okay, thank you. Let’s go to Colin Rusch from Oppenheimer. Colin, go ahead and unmute, please.\\nColin Rusch: Thanks so much, guys. Given the pursuit of Tesla really as a leader in AI for the physical world, in your comments around distributed inference, can you talk about what that approach is unlocking beyond what’s happening in the vehicle right now?\\nElon Musk: Do you want to say something?\\nAshok Elluswamy: Yes. Like Elon mentioned like the car even when it's a full robotaxi it's probably going to be used 150 hours a week.\\nElon Musk: That's my guess like a third of the hours of the week.\\nAshok Elluswamy: Yes. It could be more or less, but then there's certainly going to be some hours left for charging and cleaning and maintenance in that world, you can do a lot of other workloads, even right now we are seeing, for example, these LLM companies have these like batch workloads where they send a bunch of documents and those run through pretty large neural networks and take a lot of compute to chunk through those workloads. And now that we have already paid for this compute in these cars, it might be wise to use them and not let them be idle, be like buying a lot of expensive machinery and leaving to them idle. Like we don't want that, we want to use the computer as much as possible and close to like basically 100% of the time to make it a use of it.\\nElon Musk: That’s right. I think it's analogous to Amazon Web Services, where people didn't expect that AWS would be the most valuable part of Amazon when it started out as a bookstore. So that was on nobody's radar. But they found that they had excess compute because the compute needs would spike to extreme levels for brief periods of the year and then they had idle compute for the rest of the year. So then what should they do to pull that excess compute for the rest of the year? That's kind of...\\nAshok Elluswamy: Monetize it\\nElon Musk: Yes, monetize it. So, it seems like kind of a no-brainer to say, okay, if we've got millions and then tens of millions of vehicles out there where the computers are idle most of the time that we might well have them do something useful.\\nAshok Elluswamy: Exactly.\\nElon Musk: And then, I mean, if you get like to the 100 million vehicle level, which I think we will, at some point, get to, then – and you've got a kilowatt of useable compute and maybe your own hardware 6 or 7 by that time. Then you really – I think you could have on the order of 100 gigawatts of useful compute, which might be more than anyone more than any company, probably more than a company.\\nAshok Elluswamy: Yes, probably because it takes a lot of intelligence to drive the car anyway. And when it's not driving the car, you just put this intelligence to other uses, solving scientific problems or answer in terms of someone else.\\nElon Musk: It's like a human, ideally. We've already learned about deploying workloads to these nodes\\nAshok Elluswamy: Yes. And unlike laptops and our cell phones, it is totally under Tesla's control. So it's easier to distribute the workload across different nodes as opposed to asking users for permission on their own cell phones to be very tedious.\\nElon Musk: Well, you're just draining the battery on the phone.\\nAshok Elluswamy: Yes, exactly. The battery is also...\\nElon Musk: So like technically, I suppose like Apple would have the most amount of distributed compute, but you can't use it because you can't get the – you can't just run the phone at full power and drain the battery.\\nAshok Elluswamy: Yes.\\nElon Musk: So, whereas for the car, even if you're a kilowatt level inference computer, which is crazy power compared to a phone. If you've got 50 or 60 kilowatt hour pack, it's still not a big deal to run if you are plugged it – whether you plugged it or not – you could be plugged in or not like you could run for 10 hours and use 10-kilowatt hours of your kilowatt of compute power.\\nLars Moravy: Yes. We got built in like liquid cold thermal management.\\nElon Musk: Yes, exactly.\\nLars Moravy: Exactly for data centers, it's already there in the car.\\nElon Musk: Exactly. Yes. Its distributed power generation – distributed access to power and distributed cooling, that was already paid for.\\nAshok Elluswamy: Yes. I mean that distributed power and cooling, people underestimate that costs a lot of money.\\nVaibhav Taneja: Yes. And the CapEx is shared by the entire world sort of everyone wants a small chunk, and they get a small profit out of it, maybe.\\nElon Musk: Yes.\\nColin Rusch: Thanks so much guys. And just my follow-up is a little bit more mundane. Looking at the 4680 ramp, can you talk about how close you were to target yields and when you might start to accelerate incremental capacity expansions on that technology?\\nElon Musk: We're making good progress on that. But I don't think it's super important for at least in the near term. As Lars said, we think it will be exceed the competitiveness of suppliers by the end of this year and then we'll continue to improve.\\nLars Moravy: Yes. I mean, I think it's important to note also that like the ramp right now is relevant to the Cybertruck ramp.\\nElon Musk: Yes.\\nLars Moravy: And so like we're not going to just randomly build 4680s unless we have a place to put them and so we're going to make sure we're prudent about that. But we also have a lot of investments with all our cell suppliers and vendors. They're great partners, and they've done great development work with us and a lot of the advancements in technologies and chemistry we found 4680, they're also putting into their cells.\\nElon Musk: Yes. I mean a big part of the 4680, Tesla doing internal cells was a hedge against what would happen with our suppliers because for a while they are it was very difficult because every big carmaker put in massive battery orders, and so the price per kilowatt hour of lithium-ion batteries went to crazy numbers, crazy levels.\\nVaibhav Taneja: Bonkers.\\nElon Musk: Yes, just bonkers. So like, okay, we've got to have some hedge here to deal with cost per kilowatt hours of numbers that were double what we anticipated. If we have an internal cell production, then we have that hedge against demand shocks, we have too much demand. That's really the way to think about it. It's not like we want to take on a whole bunch of problems just for the hell of it. We did the cell program in order to address the crazy increase in cost per kilowatt hour from our suppliers due to gigantic orders placed by every carmaker on earth.\\nMartin Viecha: Okay. Thank you. And the last question comes from Ben Kallo from Baird. Ben, go ahead and unmute. Ben, you're still muted.\\nElon Musk: Well, I want to say again, we'd just like to strongly recommend that anyone who is, I guess, thinking about the Tesla stock should really drive FSD 12.3. It really – you can't – it's impossible to understand the company if you do not do this.\\nMartin Viecha: All right. So since Ben is not unmuting. Let's try Shreyas Patil from Wolfe Research. Final question.\\nShreyas Patil: Thanks so much. Just Elon, during the Investor Day last year, you mentioned that auto COGS per unit for the next-gen vehicle would decline by 50% versus the current three and Y. I think that was implying something around $20,000 of COGS. About one-third of that was coming from the on-box manufacturing process. But I'm curious if you see an opportunity that the – some of the other drivers around powertrain cost reduction or material cost savings, would those be largely transferable to some of the new products that you're now talking about introducing?\\nLars Moravy: Yes, sure. I mean, in short, yes, I mean, like the on-box manufacturing method is certainly great and revolutionary, but with it comes some risks because new production lines and not, but all the subsystems we developed, whether it was powertrains, drive units, battery improvements in manufacturing and automation, thermal systems, seating, integration of interior components and reduction of LV controllers, all that's transferable, and that's what we're doing, trying to get it in their products as fast as possible. And so yes, that engineering work, we're not trying to just throw it away and put a cars and we're going to take it and utilize it and utilize it to the best advantage of the cars we make and the future cars make.\\nShreyas Patil: Okay. Great. And then just on that topic of 4680 cells, I know you mentioned it, you really thought of it more as like a hedge against rising battery costs from other OEMs. But it seems even today, it seems like you would have a cost advantage against some of those other automakers. And I'm wondering, given the rationalizing of your vehicle manufacturing plans that you're talking about now, if there's an opportunity to maybe convert the 4680 cells and maybe sell those to other automakers and really generate an additional revenue stream. I'm just curious if you have any thoughts about that.\\nElon Musk: Great. What seems to be happening is that the I'm missing something, the orders for batteries from other automakers have declined dramatically. So we're seeing much more competitive prices for sales from our suppliers, dramatically more competitive than in the past. It is clear that a lot of our suppliers have excess capacity.\\nVaibhav Taneja: Yes. In addition to what Elon, this is kind of in addition to what Elon said, about 4680, what 4680 did for us from a supply chain perspective was help us understand the supply chain that's upstream of our cell suppliers. So a lot of the deals that we had struck for 4680, we can also supply those materials to our partners, help reducing the overall cost back to Tesla. So we're basically inserting ourselves in the upstream supply chain by doing that. So that's also been beneficial in reducing the overall pricing in addition to the excess capacity that these suppliers have.\\nElon Musk: Yes. No, I mean this is going to wax and wane, obviously. So there's going to be a boom and bust in battery cell production where production exceeds supply and then supply exceeds production and back and forth kind of like, I don't know, DRAM or something. But Yes. So it's like what is true today will not be true in the future, there's going to be somewhat of a boom and bust cycle here. And then there are additional complications with government incentives like the Inflation Reduction Act, the IRA, Joe [ph] has found like a funny name.\\nVaibhav Taneja: Comical name.\\nElon Musk: Yes, it is like Irish Republican Army, The Internet Research Agency from Russia.\\nVaibhav Taneja: Independent retirement account.\\nElon Musk: Yes, exactly. Roth IRA. It's like Spider-Man situation, which IRA wins. So but it is complicate the incentive structure. So that is there's the stronger demand for cells that are produced in the U.S. than outside the U.S. But then how long is that the IRA last, I don't know.\\nVaibhav Taneja: Which is why it's important that we have both internet [ph] cells and vendor cells that hedge against all of this.\\nMartin Viecha: Okay. Thank you very much. That's all the time we have today. But at the same time, I would like to make a short announcement. And I wanted to let the investment community know that about a month ago, I met up with Elon and Vaibhav and announced that I'll be moving on from the world of Investor Relations. I'll be hanging around for another couple of months or so. So feel free to reach out at any time. But after the seven year sprint, I'm going to be taking a break and spending some good quality time with my family. And I wanted to say that these seven years have been the greatest privilege of my professional life. I'll never forget the memories from I started literally at the beginning of production hell and just watching the company from the inside to see what it's become today. And especially super thankful to the people in this room and dozens of people outside of this room that I've worked for over the years. I think the team's strength and teamwork at Tesla is unlike anything else I've seen in my career. Elon, thank you very much for this opportunity that I got back in 2017. Thank you for seeking investor feedback and regularly and debating it with me.\\nElon Musk: Yes. Well, I mean the reason I reached out to you was because I thought your analysis of Tesla was the best that I had seen.\\nMartin Viecha: Thank you.\\nElon Musk: So, thank you for helping Tesla to get to where it is today over seven years. It's been a pleasure working with you.\\nMartin Viecha: Thank you so much. And yes, thank you for all the thousands of shareholders that we've met over the years and walked around factories and loved all the interactions, even the tough ones. And yes, looking forward to the call in the next three months, but I'll be on the other side, listening in. Thank you very much.\\nVaibhav Taneja: Thanks.\"]],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = ' '.join(results['documents'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Travis Axelrod: Good afternoon, everyone and welcome to Tesla's Second Quarter 2024 Q&A Webcast. My name is Travis Axelrod, Head of Investor Relations and I’m joined today by Elon Musk, Vaibhav Taneja, and a number of other executives. Our Q2 results were announced at about 3.00 p.m. Central Time and the Update Deck we published at the same link as this webcast. During this call, we will discuss our business outlook and make forward-looking statements. These comments are based on our predictions and expectations as of today. Actual events or results could differ materially due to a number of risks and uncertainties, including those mentioned in our most recent filings with the SEC. During the question-and-answer portion of today's call, please limit yourself to one question and one follow-up. Please use the raise hand button to join the question queue. Before we jump into Q&A, Elon has some opening remarks. Elon?\\nElon Musk: Thank you. So to recap, we saw large adoption exploration in EVs, and then a bit of a hangover as others struggle to make compelling EVs. So there are quite a few competing electric vehicles that have entered the market. And mostly they’ve not done well, but they’ve discounted their EVs very substantially, which has made it a bit more difficult for Tesla. We don’t see this as long-term issue, but really -- fairly short-term. And we still obviously firmly believe that EVs are best for customers and that the world is headed for a fully electrified transport, not just the cars, but also aircrafts and boats. Despite many challenges the Tesla team did a great job executing and we did achieve record quarterly revenues. Energy storage deployments reached an all-time high in Q2, leading to record profits for the energy business. And we are investing in many future projects, including AI training and inference and great deal of infrastructure to support future products. We won't get too much into the product roadmap here, because that is reserved for product announcement events. But we are on track to deliver a more affordable model in the first half of next year. The big -- really by far the biggest differentiator for Tesla is autonomy. In addition to that, we've scale economies and we're the most efficient electric vehicle producer in the world. So, this, anyway -- while others are pursuing different parts of the AI robotic stack, we are pursuing all of them. This allows for better cost control, more scale, quicker time to market, and a superior product, applying not to -- not just to autonomous vehicles, but to autonomous humanoid robots like Optimus. Regarding Full Self-Driving and Robotaxi, we've made a lot of progress with Full Self-Driving in Q2 and with version 12.5 beginning rollout, we think customers will experience a step change improvement in how well supervised full self-driving works. Version 12.5 has 5x the parameters of 12.4 and will finally merge the highway and city stacks. So the highway stack is still at this point is pretty old. So often the issues people encounter are on highway, but with 12.5, we are finally merged the two stacks. I still find that most people actually don't know how good the system is, and I would encourage anyone to understand the system better, to simply try it out and let the car drive you around. One of the things we're going to be doing just to make sure people actually understand the capabilities of the car is when delivering a new car and when picking up a car for service to just show people how to use it and just drive them around the block. Once people use it at all they tend to continue using it. So it's very compelling. And then this I think will be a massive demand driver, even unsupervised full self-driving will be a massive demand driver. And as we increase the miles between intervention, it will transition from supervised full self-driving to unsupervised full self-driving, and we can unlock massive potential in [V3] (ph). We postponed the sort of Robotaxi the sort of product unveil by a couple of months where it were -- it shifted to 10/10 to the 10th October -end because I wanted to make some important changes that I think would improve the vehicle -- sort of Robotaxi, the thing that we are -- the main thing that we are going to show and we are also going to show off a couple of other things. So moving it back a few months allowed us to improve the Robotaxi as well as add in a couple other things for the product unveil. We're also nearing completion of the South expansion of Giga Texas, which will house our largest training cluster to date. So it will be an incremental for 50,000 H100s plus 20,000 of our hardware 4 AI5 Tesla AI computer. With Optimus, Optimus is already performing tasks in our factory. And we expect to have Optimus production Version 1 in limited production starting early next year. This will be for Tesla consumption. It's just better for us to iron out the issues ourselves. But we expect to have several thousand Optimus robots produced and doing useful things by the end of next year in the Tesla factories. And then in 2026, ramping up production quite a bit, and at that point we'll be providing Optimus robots to outside customers. That will be Production Version 2 of Optimus. For the energy business, this is growing faster than anything else. This is -- we are really demand constrained rather than production constrained. So we are ramping up production in our U.S. factory as well as building the Megapack factory in China that should roughly double our output, maybe more than double -- maybe triple potentially. So in conclusion, we are super excited about the progress across the board. We are changing the energy system, how people move around, how people approach the economy. The undertaking is massive, but I think the future is incredibly bright. I really just can't emphasize just the importance of autonomy for the vehicle side and for Optimus. Although the numbers sound crazy, I think Tesla producing at volume with unsupervised FSD essentially enabling the fleet to operate like a giant autonomous fleet. And it takes the valuation, I think, to some pretty crazy number. ARK Invest thinks, on the order of $5 trillion, I think they are probably not wrong. And long-term Optimus, I think, it achieves a valuation several times that number. I want to thank the Tesla team for a strong execution and looking forward to exciting years ahead.\\nTravis Axelrod: Great. Thank you very much, Elon, and Vaibhav has opening remarks as well.\\nVaibhav Taneja: Thanks. As Elon mentioned, the Tesla team rose to the occasion yet again and delivered on all fronts with some notable records. In addition to those records, we saw our automotive deliveries go sequentially. I would like to thank the entire Tesla team for their efforts in delivering a great quarter. On the auto business front, affordability remains a top of mind for customers, and in response in Q2, we offered attractive financing options to offset sustained high interest rates. These programs had an impact on revenue per unit in the quarter. These impacts will persist into Q3 as we have already launched similar programs. We are now offering extremely competitive financing rates in most parts of the world. This is the best time to buy a Tesla, I mean, if you are waiting on the sidelines, come out and get your car. We had a record quarter on regulatory credits, revenues, and as well. On net, our auto margins remained flat sequentially. It is important to note that the demand for regulatory credits is dependent on other OEMs plans for the kind of vehicles they are manufacturing and selling as well as changes in regulations. We pride ourselves to be the company with the most American-made cars and are continuing our journey to further localize our supply chain, not just in the U.S., but in Europe and China as well for the respective factories. As always, our focus is on providing the most compelling products at a reasonable price. We have stepped up our efforts to provide more trims that have estimated range of more than 300 miles on a single charge. We believe this, along with the expansion of our supercharging network, is the right strategy to combat range anxiety. Since the revision of FSD pricing in North America, we've seen production rates increase meaningfully and expect this to be a driver of vehicle sales as the feature set improves further. Cost per vehicle declined sequentially when we removed the impact of Cybertruck. While we are experiencing material costs trending down, note that there is latency on the cost side and such reductions would show up in the P&L when the vehicles built with these materials get delivered. Additionally, as we get into the second half of the year, it is important to note that we are still ramping Cybertruck and Model 3 and are also getting impacted by varying amounts of tariffs on both raw materials and finished goods. While our teams are working feverishly to offset these, unfortunately it may have an impact on the cost in the near-term. We previously talked about the potential of the energy business and now feel excited that the foundation that was laid over time is bearing the expected results. Energy storage deployments more than doubled with contribution not just from Megapack, but also Powerwall, resulting in record revenues and profit for the energy business. Energy storage backlog is strong. As discussed before, deployments will fluctuate from period to period with some quarters seeing large increases and others seeing a decline. Recognition of storage gigawatt hours is dependent on a variety of factors, including logistics timing as we send units from a single factory to markets across the world, customer readiness and in case of EPC projects on construction activities. Moving on to the other parts of the business, service and other gross profits also improved sequentially from the improvement in service utilization and growth in our collision repair business. The impact of our recent reorg is reflected in restructuring other - on the income statement. Just to level set, this was about $622 million of charge, which got recorded in the period. And I want people to remember that we've called it out separately on the financials. Sequentially, our operating expenses excluding surcharges reduced despite an increase in spend for AI-related activities and higher legal and other costs. On the CapEx front, while we saw a sequential decline in Q2, we still expect the year to be over $10 billion in CapEx as we increase our spend to bring a 50k GPU cluster online. This new cluster will immensely increase our capabilities to scale FSD and other AI initiatives. We reverted to positive free cash flow of $1.3 billion in Q2. This was despite restructuring payments being made in the quarter and we ended the quarter with over $30 billion of cash and investments. Once again, we've begun the journey towards the next phase for the company with the building blocks being placed. It will take some time, but will be a rewarding experience for everyone involved. Once again, I would like to thank the entire Tesla team for their efforts.\\nA - Travis Axelrod: Great. Thank you very much, Vaibhav. Now let's go to investor questions. The first question is, what is the status on the Roadster?\\nElon Musk: With respect to Roadster, we've completed most of the engineering. And I think there's still some upgrades we want to make to it, but we expect to be in production with Roadster next year. It will be something special, like the whole thing [Indiscernible].\\nTravis Axelrod: Fantastic. The next question is about timing of Robotaxi event, which we've already covered. So we'll go to the next question, when do you expect the first Robotaxi ride?\\nElon Musk: I guess that, that's really just a question of when can we expect the first -- or when can we do unsupervised full self-driving. It's difficult, obviously, my predictions on this have been overly optimistic in the past. So I mean, based on the current trend, it seems as though we should get miles between interventions to be high enough that -- to be far enough in excess of humans that you could do unsupervised possibly by the end of this year. I would be shocked if we cannot do it next year. So next year seems highly probable to me based on [quite simply] (ph) plus the points of the curve of miles between intervention. That trend exceeds humans for sure next year, so yes.\\nTravis Axelrod: Thank you very much. Our third question is, the Cybertruck is an iconic product that wows everyone who sees it. Do you have plans to expand the cyber vehicle lineup to a cyber SUV or cyber van?\\nElon Musk: I think we want to limit product announcements to when we have a special -- specific product announcement event, rather than earnings calls.\\nTravis Axelrod: Great, thank you. Our next question is, what is the current status of 4680 battery cell production and how is the ramp up progressing?\\nLars Moravy: Yes, 4680 production ramped strongly in Q2, delivering 51% more cells than Q1 while reducing COGS significantly. We currently produce more than 1,400 Cybertrucks of 4680 cells per week, and we'll continue to ramp output as we drive cost down further towards the cost parity target we set for the end of the year. We've built our first validation Cybertruck with dry cathode process made on our mass production equipment, which is a huge technical milestone and we're super proud of that. We're on track for production launch with dry cathode in Q4, and this will enable cell cost to be significantly below available alternatives, which was the original goal of the 4680 program.\\nTravis Axelrod: Great. Thank you very much. The next question is any update on Dojo?\\nElon Musk: Yes, so Dojo, I should preface this by saying I'm incredibly impressed by NVIDIA's execution and the capability of their hardware. And what we are seeing is that the demand for NVIDIA hardware is so high that it's often difficult to get the GPUs. And there just seems this, I guess I'm quite concerned about actually being able to get state-of-the-art NVIDIA GPUs when we want them. And I think this therefore requires that we put a lot more effort on Dojo in order to have -- in order to ensure that we've got the training capability that we need. So we are going to double down on Dojo, and we do see a path to being competitive with NVIDIA with Dojo. And I think we kind of have no choice because the demand for NVIDIA is so high and the -- it's obviously their obligation essentially to raise the price of GPUs to whatever the market will bear, which is very high. So, I think we've really got to make Dojo work and we will.\\nTravis Axelrod: Right. The next question is what type of accessories will be offered with Optimus?\\nElon Musk: There's -- Optimus is intended to be a generalized humanoid robot with a lot of intelligence. So it's like saying what kind of accessories will be offered with a human. It's just really intended to be able to be backward compatible with human tasks. So it would use any accessories that a human would use. Yes.\\nTravis Axelrod: Thank you. The next question is, do you feel you're cheating people out of the joys of owning a Tesla by not advertising?\\nElon Musk: We are doing some advertising, so, want to say something?\\nVaibhav Taneja: Yes, I would say something. Our fundamental belief is that we need to be providing the best products at a reasonable price to the consumers. Just to give you a fact, in U.S. alone in Q2, over two-thirds of our sales were to -- deliveries were to people who had never owned a Tesla before and which is encouraging. We've spent money on advertising and other awareness programs and we have adjusted our strategy. We're not saying no to advertising, but this is a dynamic play and we know that we have not exhausted all our options and therefore plan to keep adjusting, but in the latter half of this year as well.\\nTravis Axelrod: Great. Thank you very much. The next question is on energy growth, which we already covered in opening remarks, so we'll move on to the next one. What is the updated timeline for Giga Mexico and what will be the primary vehicles produced initially?\\nElon Musk: Well, we currently are paused on Giga Mexico. I think we need to see just where things stand after the election. Trump has said that he will put heavy tariffs on vehicles produced in Mexico. So it doesn't make sense to invest a lot in Mexico if that is going to be the case. So we kind of need to see where the things play out politically. However, we are increasing capacity at our existing factories quite significantly. And I should say that the Cybertaxi or Robotaxi will be produced here at our headquarters at Giga Texas.\\nTravis Axelrod: All right. Thank you.\\nElon Musk: And as well Optimus towards the end of next year for Optimus production Version 2, the high volume version of Optimus will also be produced here in Texas.\\nTravis Axelrod: Great. Thank you. Just a couple more. Is Tesla still in talks with an OEM to license FSD?\\nElon Musk: There are a few major OEMs that have expressed interest in licensing Tesla full self-driving. And I suspect there will be more over time. But we can't comment on the details of those discussions.\\nTravis Axelrod: All right. Thank you. And the last one, any updates on investing in xAI and integrating Grok into Tesla software?\\nElon Musk: I should say Tesla is learning quite a bit from xAI. It's been actually helpful in advancing full self-driving and in building up the new Tesla data center. With -- regarding investing in xAI, I think, we need to have a shareholder approval of any such investment. But I'm certainly supportive of that if shareholders are, the group -- probably, I think we need a vote on that. And I think there are opportunities to integrate Grok into Tesla's software, yes.\\nTravis Axelrod: All right. Thanks very much. And now we will move on to analyst questions. The first question comes from Will Stein from Truist. Will, please go ahead and unmute yourself.\\nWill Stein: Great. Thanks so much for taking my question. And this relates a little bit to the last one that was asked. Elon, I share your strong enthusiasm about AI and I recognize Tesla's opportunity to do some great things with the technology. But there are some concerns I have about Tesla's commercialization and that's what I'd like to ask about specifically. There were some news stories through the quarter that indicated that you redirected some AI compute systems that were destined for Tesla instead to xAI or perhaps it was to X, I'm not sure. And similarly, a few quarters ago, if you recall, I asked about your ability to hire engineers in this area, and you noted that there was a great desire for some of these engineers to work on projects that you were involved with, but some of them weren't at Tesla, they were instead at xAI or perhaps even X again. So the question is, when it comes to your capital investments, your AI R&D, your AI engineers, how do you make allocation decisions among these various ventures and how do you make Tesla owners comfortable that you're doing it in a way that really benefits them? Thank you.\\nElon Musk: Yes, I mean, I think you're referring to a very -- like an old article, regarding GPUs. I think that's like 6 or 7 months old. At Tesla, we had no place to try them on, so it would've been a waste of Tesla capital because we would just have to order H100 and have no place to try them on. So it was just -- there was -- this wasn't a, let's pick xAI of Tesla. There's -- there was no -- the Tesla data centers were full. There was no place to actually put them. The -- we've been working 24/7 to complete the South extension on the Tesla Giga factory in Texas. That South extension is what will house 50,000 H100s and we're beginning to move the H100 server racks into place there. But we really needed -- we needed that to complete physically. You can't just order compute -- order GPUs and turn them on, you need a data center, it's not possible. So I want to be clear, that was in Tesla's interest, not contrary to Tesla's interest. Does Tesla no good to have GPUs that it can't turn on. That South extension is able to take GPUs, which is really just this week. We are moving the GPUs in there and we'll bring them online. With regard to xAI, there are a few that only want to work on AGI. So what I was finding was that when trying to recruit people to Tesla, they were only interested in working on AGI and not on Tesla's specific problems and they want to start -- do a start-up. So it was a case of either they go to a start-up or -- and I am involved or they do a start-up and I am not involved. Those are the two choices. This wasn't they would come to Tesla. They were not going to come to Tesla under any circumstances. So, yes.\\nVaibhav Taneja: Yes, I mean, I would even add that AI is a broad spectrum and there are a lot of things which we are focused on full time driving as Tesla and also Optimus, but there's the other spectrum of AI which we're not working on, and that's the kind of work which other companies are trying to do in this case, xAI. So you have to keep that in mind that it's a broad spectrum. It's not just one specific thing.\\nElon Musk: Yes. And once again, I want to just repeat myself here. I tried to recruit them to Tesla, including to say like, you can work on AGI, I if you want and they refused. Only then was xAI created.\\nWill Stein: I really appreciate that clarification. If I can ask one follow-up, it relates to the new vehicles that you're planning to introduce next year. I understand this is not the venue for product announcements, but when we think about the focus, I've heard on the one hand that the focus is on cost reduction. On the other hand, you also said that the Roadster would come out. Should we expect other maybe more limited variants like, similar to the cars that you make today, but with some changes or improvements or different, some other variability in the form factors. It should -- we expect that to be a significant part of the strategy in the next year or two?\\nElon Musk: I don't want to get into details of product announcements. And we have to be careful of the Osborne effect here. So, if you start announcing some great thing, it affects our near-term sales. We're going to make great products in future just like we have in the past, end of story.\\nTravis Axelrod: Right. The next question comes from Ben Kallo from Baird. Ben, please go ahead and unmute yourself.\\nBen Kallo: Hi. Thanks for taking my question. When we think about revenue contribution and with energy growing so quickly and Optimus on the come, how do we think about the overall segments longer term? And then do you think that auto revenue will fall below 50% of your overall revenue? And then my follow-up is just on the last call you talked about, distributed compute on your new hardware. Could you just update us and talk a little bit more about that, the timeline for it and how you would reward customers for letting you use their compute power and their cars? Thanks.\\nElon Musk: Yes, I mean, as I've said a few times, I think the long-term value of Optimus will exceed that of everything else that Tesla combined. So, it's simply -- just simply consider the usefulness utility of a humanoid robot that can do pretty much anything you ask of it. I think everyone on earth is going to want one. There's 8 billion people on earth, so it's 8 billion right there. Then you've got, all of the industrial uses, which is probably at least as much, if not way more. So I suspect that the long-term demand for general purpose humanoid robots is in excess of 20 billion units. And Tesla is -- that has the most advanced humanoid robot in the world, and is also very good at manufacturing, which these other companies are not. And we've got a lot of experience -- with the most experienced with the world leaders in real world AI. So we have all of the ingredients. I think we are unique in having all of the ingredients necessary for large scale, high utility, generalized humanoid robots. That's why my rough estimate long-term is in accordance with the ARK [ph] Invest analysis of market cap on the order of $5 trillion for -- maybe more for autonomous transport, and it's several times that number for general purpose humanoid robots. I mean, at that point, I'm not sure what money even means, but in the benign AI scenario, we are headed for an age of abundance where there is no shortage of goods and services. Anyone can have pretty much anything they want. It's a wild -- very wild future we're heading for.\\nBen Kallo: On the distributed compute?\\nElon Musk: Yes, distributed compute, that seems like a pretty obvious thing to do. I think the -- where this distributed compute becomes interesting is with our next generation Tesla AI truck, which is hardware viable or what we're calling AI5, which is -- from the standpoint of inference capability comparable toB200 -- and a bit of B200. And we are aiming to have that in production at the end of next year and scale production in '26. So it just seemed like if you've got -- even if you've got autonomous vehicles that are operating for 50 or 60 hours a week, there's a 168 hours in a week. So you have somewhere above I think a 100 [indiscernible] net computing. I think we need a better word than GPU because GPU means graph express in unit. So there's a 100 hours plus per week of AI compute, AI advanced compute from the fleet, from the vehicles and probably some percentage from the humanoid robots that it would make sense to do distributed inference. And if you're -- if there's a fleet of at some point a 100 million vehicles with AI5 and beyond, because you have AI 6 and 7 and whatnot, and there may be billions of humanoid robots that is just a staggering amount of inference compute or that could be used for general purposes at computing. It doesn't have to be used for, the humanoid robot or for the car. So I think, that's just -- that -- that's a pretty obvious thing to say, like, well, it's more useful than having to do nothing.\\nTravis Axelrod: All right. Thank you. The next question comes from Alex Potter from Piper Alex. Alex, please go ahead and unmute yourself.\\nAlex Potter: Perfect. Thanks. I wanted to ask a question on FSD licensing. You mentioned that in passing previously, was just wondering if you can elaborate maybe on the mechanics of how that would work. I guess presumably this would not be some sort of simple plug and play proposition that presumably an OEM would need, I don't know, several years to develop its own vehicle platform that's based on FSD. I imagine they would need to adopt Tesla's electrical architecture, compute, sensor stack. So I, correct me if I'm sort of misunderstanding this, but if you had a cooperative agreement of some kind with another OEM, then presumably it would take you several years before you'd be able to recognize licensing revenue from that agreement. Is that the right way to think about that?\\nElon Musk: Yes. The OEMs not real fast. There's not really a sensor suite, it's just cameras. But they would have to integrate our AI computer and have cameras with a 360 degree view. And at least the gateway, like the what talks to the internet, and communicates with the Tesla system, what that you need kind of a gateway computer too. So it's really gateway computer with the cellular and Wi-Fi connectivity, the Tesla AI computer, and seven cameras, or not cameras, again, a 360 degree view. But this will -- given the speed at which, the auto industry moves, it would be several years before you would see this in volume.\\nAlex Potter: Okay, good. That's more or less what I expected. So then the follow-up here is, if you did sign an FSD licensing agreement with another automaker, when do you think you would disclose that? Would you do it right when you signed the agreement or only after that multiple years has passed and the vehicle is ready to be rolled out? think it depends on the OEM. I guess we'd be happy either way. Yes, it depends on, what kind of arrangement we enter into. A lot of those things are, we are not resolved yet, so we'll make that determination as and when we get to that point.\\nElon Musk: And the kind of deals that are obviously relevant are only if, some OEM is willing to do this in a million cars a year or something significant. It's not -- if it's like 10,000 or a 100,000 cars a year. We can just make that ourselves.\\nTravis Axelrod: All right, thank you. The next question comes from Dan Levy from Barclays. Dan, please go ahead and unmute yourself.\\nDan Levy: Hi, good evening. Thanks for taking the questions. First, wanted to start with a question on Shanghai. You've leveraged Shanghai as an export center really due its low cost, and that makes sense. But maybe you can just give us a sense of, of how the strategy changes, if at all, given, the implementation of tariffs in Europe. Also to what extent, your import of batteries from China into the U.S., how that might change given the tariffs. Thank you.\\nElon Musk: Yes. I think I covered some part of it in my opening remarks, but just to give you a little bit more, just on the tariff side, the European authorities did sample certain other OEMs in the first round to establish the tariffs for cars being imported from China into Europe. While we were not picked up in our individual examination in the first round, they did pick us up in the second round. They visited our factory. They -- we worked with them, provided them all the information. As a result, we were adjusting our import strategy out of China into Europe. But -- and one other thing to note is in Q2 itself, we started building right hand from model wise out of Berlin and we also delivered it in U.K. And we're adjusting as needed, but we will keep adjust. We're still importing Model 3s into Europe, out of Shanghai. And we are still evaluating what is the best alternate manage all this just on the examination by the European authorities. Like I said, we cooperated with them. Well, we are confident that they, we should get a better rate than what they have imposed for now. But this is literally evolving and we are adjusting as fast as we can with this. It is -- I would also add that, because of this, you've seen the impact that Berlin is doing more imports into places like Taiwan as well as, U.K I just mentioned. So it will keep changing and we will keep adapting as we go about it.\\nDan Levy: Great. Thanks. Yes, thank you. As a follow-up, wanted to ask about the Robotaxi strategy and specifically the shareholder deck here notes that the release is going to be -- one of the gating factors is regulatory approval. So maybe you could help us understand which regulations specifically are the ones that we should be looking for? Is it FMVSS, that's standard? And then to what extent does the strategy shift? You've done with FSD more of a nationwide, no boundary approach. Is the Robotaxi approach one that's more geofenced, so to speak, and is more driven by a state by state approach?\\nElon Musk: I mean, our solution is a generalized solution like what everybody else has. They, if you see like Waymo has one of it, they have a very localized solution that requires high density mapping. It's not -- it's quite fragile. So, their ability to expand rapidly is limited. Our solution is a general solution that works anywhere. It would even work on a different earth. So if you're rendered a new Earth, it would work on a new earth. So it's -- there's this capability I think in our experience, once we demonstrate that something is safe enough or significantly safer than human. We are fine that regulators are supportive of deploying deployment of that capability. It's difficult to argue with if you -- if you've got a large number of -- yes, if you've got billions of miles that show that in the future unsupervised FSD is safer than human. What regulator could really stand in the way of that? They would -- they're morally obligated to approve. So I don't think regulatory approval will be a limiting factor. I should also say that the self-driving capabilities of this are deployed outside of North America are far behind that in, in North America. So with the -- with Version 12.5, and maybe a 12.6, but pretty soon we will ask for regular regulatory approval of the Tesla supervised FSD in Europe, China, and other countries. And I, I think we're likely to receive that before the end of the year, which will be a helpful demand driver in those regions obviously.\\nTravis Axelrod: Thank you. Just to …\\nElon Musk: Go ahead, Travis.\\nTravis Axelrod: In terms of like, as Elon said, in terms of regulatory approval, the vehicles are governed by FMVSS in U.S., which is the same across all 50 states. The road rules are the same across all 50 states. So creating a generalized solution gives us the best opportunity to deploy in all 50 states, reasonably. Of course there are state and even local and municipal level regulations that may apply to, being a transportation company or deploying taxes. But as far as getting the vehicle on the road, that's all federal and that's very much in line with what you was just suggesting about the data and the vehicle itself.\\nVaibhav Taneja: And to add to the technology point, the end-to-end network basically makes no assumption about the location. Like you could add data from different countries and it just like perform equally well there, just like almost like close to zero US specific, um, code in there. It's all just the data that comes from the U.S\\nElon Musk: Yes. To, to that end of the show, it's like, we can go as humans to other countries and drive with some reasonable amount of assessment in those countries. And that's how you design the FSC software. Yes, exactly.\\nTravis Axelrod: Great. Thanks guys. The next question comes from George from Canaccord. George, please go ahead and unmute yourself.\\nGeorge Gianarikas: Hi, everyone. Thank you for taking my questions. Maybe just to expand on the regulatory question for a second. And I could be comparing apples and oranges, but GM canceled their pedal less, wheel less vehicle. And according to the company this morning, their decision was driven by uncertainty about the regulatory environment. And from what we understand, and again, maybe I'm wrong here, but the Robotaxi that has been shown at least in images of the public is also pedal less and wheel less. Is there a different regulatory concern just if you deploy a vehicle like that that doesn't have pedal -- pedals or a wheel, and that may not be different from just regular FSD on a traditional Tesla vehicle. Thank you.\\nElon Musk: Well, obviously the real reason that they cancel it is because GM can't make it work, not because the regulators, they're blaming regulators. That's misleading of them to do so, because Waymo is doing just fine in those markets. So it's just that their technology is not far.\\nGeorge Gianarikas: Right. And maybe just as a follow-up, I think you mentioned, that FSD take rates were up materially after you reduced the price. Is there any way you can help us quantify what that means Exactly? Thank you.\\nVaibhav Taneja: Yes, we shared the [indiscernible] that there we've seen a meaningful increase. I don't want to get into specific because we started from a low base and -- but we are seeing encouraging results. And the key thing here is, like Elon said, you need to experience it because words can't describe it till the time we actually use it. And that's why we are trying to make sure that every time a car is getting delivered, people are being showed how this thing is working because when you see it working, you realize how great it is. I mean, just to give you one example, so again, there's a bias example, but I have a more than 20 mile commute into the factory almost every day. I have zero interventions on the latest stack, and the card just literally drives me over. And especially with the latest version wherein, we are also tracking your eye movement, the steering wheel lag is almost not there as long as you're not wearing sunglasses.\\nElon Musk: Well, we are fixing the sunglasses thing. It's coming soon. So you will be able to drive -- you'll be able to have sunglasses on and have the car drive.\\nGeorge Gianarikas: Yes.\\nElon Musk: So -- but there's number of times I've talked with smart people who like live in New York or maybe downtown Boston and don't ever drive and then ask me about FSD, I'm like, you can just get a car and try it. And if you're not doing that, you have no idea what's going on.\\nTravis Axelrod: Thank you. The next question comes from Pierre from New Street. Pierre, please unmute yourself.\\nFerragu Pierre: Hey, guys. Thank you for taking my question. So it's on Robotaxi again, and I completely get it that with a universal solution, we will get like regulatory approval, we'll get there eventually clicking up miles and compute, et cetera. And my question is more, how you think about deployments, because I'm still like, I'm thinking once you have a car that can drive everywhere, that can replace me, it can replace a taxi, but then to do the right hailing service, you need a certain scale. And that means a lot of cars on the road and so you need an infrastructure to just maintain the cars, take care of them, et cetera. And so my question is, are you already working on that? Do you have already an idea of what, like your plan to deploy looks like? And is that like a test Tesla only plan or are you looking at partners, local partners, global partners to do that? And I'll have a quick follow-up.\\nElon Musk: Yes. This would just be the Tesla network. You just literally open the Tesla app and summon a car and resend a car to pick you up and take you somewhere. And you can -- our -- we'll have a fleet that's I don't know, on order of 7 million dedicated global autonomy soon. In the years come it'll be over 10 million, then over 20 million. This is immense scale. And the car is able to operate 24/7, unlike the human driver. So, the capability to -- like, if there's this basically instant scale with a software update. And now this is for a customer on fleet. So you can think of that as being a bit like Airbnb, like you can choose to allow your car to be used by the fleet, or cancel that and bring it back. It can be used by the fleet all the time. It can be used by the fleet some of the time, and then Tesla would take -- would share on the revenue with the customer. But you can think of the giant fleet of Tesla vehicles as like a giant sort of Airbnb equivalent fleet, Airbnb on wheels. The -- I mean, then in addition we would make some number of cars for Tesla that would just be owned by Tesla and be added to the fleet. I guess that would be a bit more like Uber. But this would all be a Tesla network. And there's an important clause we've put in, in every Tesla purchase, which is that the Tesla vehicles can only be used in the Tesla fleet. They cannot be used by a third-party for autonomy.\\nFerragu Pierre: Okay. And do you think that scale is like progressively so you can start in a city with just a handful of cars and you grow the number of cars over time? Or do you think there is like a critical mass you need to get to, to be able to offer like a service that is of competitive quality compared to what like the -- like Uber would be typically delivering already?\\nElon Musk: I guess I'm not -- maybe I'm not conveying this correctly. The entire Tesla fleet basically becomes active. This is obviously maybe there's some number of people who don't want their car to own money, but I think most people will. It's instant scale.\\nTravis Axelrod: Thank you. Our next question comes from Colin from Oppenheimer. Colin, please unmute yourself.\\nColin Rusch: Sorry about that guys. I've got two questions around energy storage. With the tight supply and the stationary storage, can you talk about your pricing strategy and how you're thinking about saturation and given geographies given that some of these larger systems are starting to shift wholesale power markets in a pretty meaningful way quickly?\\nVaibhav Taneja: So, I mean, we are working with a large set of players in the market and our pipeline is actually pretty long. And there's actually very -- there's actually long end in terms of where you enter into a contract where delivery started -- starts happening. And so far we have good pricing leverage. And now Mike, chime in on this too.\\nUnidentified Company Representative: Yes, I mean there's a lot of competition from Chinese OEMs just like there is in the vehicle space. So we're in close contact with our customers and making sure that we're remaining competitive in where they're needing to be competitive to, to secure contracts to sell power and energy in the markets. We had a really strong contracting quarter and continue to build our backlog for 2025 and 2026. So we feel pretty good about where we are in the market. We realize that competition is strong, but we have a pretty strong value proposition with offering a fully integrated product with our own power electronics and site level controls. So …\\nVaibhav Taneja: Yes, and again, the aspect which people miss do not fully understand is that there's also a whole software stack, which comes with from Megapack, right? And that is a unique proposition which we -- which is only available to us, and we are using it with other stuff too, but that gives us a much more of an edge as compared to the competition.\\nElon Musk: Yes, we find customers that they can sort of put together a hodgepodge solution. And so, and then sometimes they'll pick that solution, and then that doesn't work. And then they come back to us.\\nUnidentified Company Representative: Yes, and we're not really seeing saturation for like, on a global scale. There's little pockets of saturation in different markets, but we're more seeing that there's markets opening up given demand on the grid just continues to increase more than anyone expects. So that just opens up markets, really across the world in different pockets.\\nVaibhav Taneja: Yes, I mean just even on the AI computer side, right? These GPUs are really powerful already and the amount of new pipeline, which we're getting for people for data center backup and things like that is increasing at a pretty large scale.\\nColin Rusch: Yes. Thanks. And then the follow-up here is 4680 process technology and the role to role process. There's some news around your equipment suppliers. Can you talk about how far along you are in, in potentially qualifying an incremental supplier around some of that, those critical process technology steps?\\nLars Moravy: Yes, I can talk about that. As you're probably referring to the lawsuit that we have with one of our suppliers, look, I don't think this is going to affect our ability to roll out 4680. We have very strong IP position in the technology and the majority of the equipment that we use is in-house designed and some of it's in-house build. And so we can take our IP stack and have someone else build it if we need to. So it's, that's not really a concern right now.\\nElon Musk: Yes. I, I think people don't understand just how much demand there will be for grid storage. They really just like the [indiscernible] I think are underestimating this demand by probably orders magnitude. So that the actual energy, total energy output of, say the U.S grid is if the power plants can operate a steady state is at least two to three times, the amount of energy it currently produces, because there are a huge gap. There's a huge difference in the -- from peak to trough in terms of energy of power generation. So in order for a grid to not have blackouts, it must be able to support the load at the worst minute of the worst day of the year, the coldest or hottest day, which means that for the rest of the time, the rest of the year, it's got massive excess power generation capability, but it has no way to store that energy. Once you add battery packs, you can now run the power plants at steady state. Steady state means that basically any given grid anywhere in the world can produce in terms of cumulative energy in the course of the year, at least twice what it is currently producing in some cases, maybe three times.\\nTravis Axelrod: All right. Thank you, Elon. The next question comes from Colin Langan from Wells Fargo. Colin, please unmute yourself.\\nColin Langan: Oh, great. Thanks for taking my questions. Do you hear me?\\nTravis Axelrod: Yes.\\nColin Langan: Yes. Sorry. I guess when we are going to ask, if Trump wins, there's a higher chance that IRA could get cut. I think Elon, you had commented online that Tesla doesn't survive on EV subsidies. But when Tesla lose a lot of support if IRA goes away? I think model Y3 and Y get IRA help for customers, and I think your batteries get production tax credits. So, just one, can you clarify if the end, if IRA ends, would it be a negative for your profitability in the near-term? Why might it not be a negative? And then, any framing of the current support you get, IRA-related?\\nElon Musk: I guess that there would be like some impact, but I think it would be devastating for our competitors. But -- and it would hurt Tesla slightly. But long-term probably actually helps Tesla would be my guess. Yes -- but I've said this before on earnings calls, it -- the value of Tesla overwhelmingly is autonomy. These other things are in the noise relative to autonomy. So I recommend anyone who doesn't believe that Tesla will solve vehicle autonomy should not hold Tesla stock. They should sell their Tesla stock. You should believe Tesla will solve autonomy, you should buy Tesla stock. And all these other questions are in the noise.\\nVaibhav Taneja: Yes, I mean, I'll add this just to clarify a few things that -- at the end of the day, when we are looking at our business, we've always been looking at it whether or not IRA is there and we want our business to grow healthy without having any subsidies coming in, whichever way you look at it. And that's the way we have always modeled everything. And that is the way internally also even when we are looking at battery costs, yes, I --, there are manufacturing credits which we get, but we always drive ourselves to say, okay, what if there is no higher benefit and how do we operate in that kind of an environment? And like Elon said, we definitely have a big advantage as compared to a competition on that front. We've delivered it and you can see it in the numbers over the years. Like, so there is you cannot ignore the fundamental size of the business. And then on top of it, once you add autonomy to it, like even said, it becomes meaningless to you think about the short-term.\\nTravis Axelrod: Okay. I think that's unfortunately all the time we have for today. We appreciate all of your questions. We look forward to talking to you next quarter. Thank you very much and goodbye.\\nElon Musk: That's excellent. Martin Viecha: Tesla's First Quarter 2024 Q&A Webcast. My name is Martin Viecha, VP of Investor Relations, and I'm joined today by Elon Musk, Vaibhav Taneja, and a number of other executives. Our Q1 results were announced at about 3.00 p.m. Central Time in the Update Deck we published at the same link as this webcast. During this call, we will discuss our business outlook and make forward-looking statements. These comments are based on our predictions and expectations as of today. Actual events and results could differ materially due to a number of risks and uncertainties, including those mentioned in our most recent filings with the SEC. During the question-and-answer portion of today's call, please limit yourself to one question and one follow-up. Please use the raise hand button to join the question queue. But before we jump into Q&A, Elon has some opening remarks. Elon?\\nElon Musk: Thanks, Martin. So to recap in Q1 we navigated several unforeseen challenges as well as the ramp of the updated Model 3 in Fremont. There was, as we all have seen, the EV adoption rate globally is under pressure and a lot of other auto manufacturers are pulling back on EVs and pursuing plug-in hybrids instead. We believe this is not the right strategy and electric vehicles will ultimately dominate the market. Despite these challenges, the Tesla team did a great job executing in a tough environment and energy storage deployments, the Megapack in particular, reached an all time high in Q1, leading to record profitability for the energy business, and that looks likely to continue to increase in the quarters and years ahead. It will increase. We actually know that it will, so significantly faster than the car business as we expected. We also continue to expand our AI training capacity in Q1, more than doubling our training compute sequentially. In terms of the new product roadmap, there has been a lot of talk about our upcoming vehicle line in the next – in the past several weeks. We've updated our future vehicle lineup to accelerate the launch of new models ahead, previously mentioned startup production in the second half of 2025, so we expect it to be more like the early 2025, if not late this year. These new vehicles, including more affordable models, will use aspects of the next generation platform as well as aspects of our current platforms, and will be able to produce on the same manufacturing lines as our current vehicle lineup. So it's not contingent on any new factory or massive new production line. It'll be made on our current production lines much more efficiently. And we think this should allow us to get to over 3 million vehicles of capacity when realized to the full extent. Regarding FSD Version 12, which is the pure AI-based self-driving, if you haven't experienced this, I strongly urge you to try it out. It's profound and the rate of improvement is rapid so – and we've now turned that on for all cars with the cameras and inference computer and everything from Hardware 3 on in North America. And so it's been pushed out to, I think, around 1.8 million vehicles and we're seeing about half of people use it so far and that percentage is increasing with each passing week. So we now have over 300 billion miles that have been driven with FSD V12. Since the launch of full self-driving, supervised full self-driving, it's become very clear that the vision-based approach with end to end neural networks is the right solution for scalable autonomy. It's really how humans drive. Our entire road network is designed for biological neural nets and eyes. So naturally cameras and digital neural nets are the solution to our current road system. To make it more accessible, we've reduced the subscription price to $99 a month, so it's easy to try out. And as we've announced, we'll be showcasing our purpose-built robotaxi, or Cybercab, in August. Yes. Regarding AI compute, over the past few months, we've been actively working on expanding Tesla's core AI infrastructure. For a while there, we were training constrained in our progress. We are, at this point, no longer training constrained and so we're making rapid progress. We've installed and commissioned, meaning they're actually working 35,000 H100 computers or GPUs, GPU is wrong word, they need a new word. I always feel like a wince when I say GPU because it's not – GPU stands – G stands for graphics, and it doesn't do graphics. But anyway roughly 35,000 H100s are active, and we expect that to be probably 85,000 or thereabouts by the end of this year and training, just for training. We are making sure that we're being as efficient as possible in our training. It's not just about the number of H100s, but how efficiently they're used. So in conclusion, we're super excited about our autonomy road map. I think it should be obvious to anyone who's driving Version 12 and it tells that that it is only a matter of time before we exceed the reliability of humans and not much time with that. And we're really headed for an electric vehicle, an autonomous future. And I'll go back to something I said several years ago that in the future, gasoline cars that are not autonomous will be like riding a horse and using a flip phone. And that will become very obvious in hindsight. We continue to make the necessary investments that will drive growth and profits for Tesla in the future, and I wanted to thank the Tesla team for incredible execution during this period and look forward to everything that we have planned ahead. Thanks.\\nMartin Viecha: Thank you very much, and Vaibhav has some comments as well.\\nVaibhav Taneja: Thanks. It's important to acknowledge what Elon said, from our auto business perspective. We did see a seasonal decline in revenues quarter-over-quarter and those were primarily because of seasonality, uncertain macroeconomic environment and the other reasons, which Elon had mentioned earlier. Auto margins declined from 18.9% to 18.5%. Excluding the impact of Cybertruck, the impact of pricing actions was largely offset by reductions in per unit costs and the recognition of revenue from Autopark feature for certain vehicles in the U.S. that previously did not have that functionality. Additionally, while we did experience higher cost due to the ramp of Model 3 in Fremont and disruptions in Berlin, these costs were largely offset by cost reduction initiatives. In fact, if we exclude Cybertruck and Fremont Model 3 ramp costs, the revenue from Autopark, auto margins improved slightly. Currently normalized Model Y cost per vehicle in Austin and Berlin are already very close to that of Fremont. Our ability to reduce costs without sacrificing on quality was due to the amazing efforts of the team, in executing Tesla's relentless pursuit of efficiency across the business. We've also witnessed that as other OEMs are pulling back on their investments in EV, there is increasing appetite for credits, and that means a steady stream of revenue for us. Obviously, seeing others pull back from EV is not the future we want. We would prefer it the whole industry went all in. On the demand front, we've undertaken a variety of initiatives, including lowering the price of both the purchase and subscription options for FSD launching extremely attractive leasing specials for the Model 3 in the U.S. for $299 a month and offering attractive financing options in certain markets. We believe that our awareness activities, paired with attractive financing, will go a long way in expanding our reach and driving demand for our products. Our Energy business continues to make meaningful progress with margins reaching a record of 24.6%. We expect the energy storage deployments for 2024 to grow at least 75% higher from 2023. And accordingly, this business will begin contributing significantly to our overall profitability. Note that there is a bit of lumpiness in our storage deployments due to a variety of factors that are outside of our control, so deployments may fluctuate quarter-over-quarter. On the operating expense front, we saw a sequential increase from our AI initiatives, continued investment in future projects, marketing and other activities. We had negative free cash flow of $2.5 billion in the first quarter. The primary driver of this was an increase in inventory from a mismatch between builds and deliveries as discussed before, and our elevated spend on CapEx across various initiatives, including AI compute. We expect the inventory build to reverse in the second quarter and free cash flow to return to positive again. As we prepare the company for the next phase of growth, we had to make the hard but necessary decision to reduce our head count by over 10%. The savings generated are expected to be well in excess of $1 billion on an annual run rate basis. We are also getting hyper focused on CapEx efficiency and utilizing our installed capacity in a more efficient manner. The savings from these initiatives, including our cost reductions will help improve our overall profitability and ultimately enable us to increase the scale of our investments in AI. In conclusion, the future is extremely bright and the journey to get there while challenging will be extremely rewarding. Once again, I would like to thank the whole Tesla team for delivering great results. And we can open it up to Q&A.\\nA - Martin Viecha: Okay. Let's start with investor Q&A. The first question is, what is the status of 4680. What is the current output? Lars?\\nLars Moravy : Sure. 4680 production increased about 18% to 20% from Q4 reaching greater than 1K a week for Cybertruck, which is about 7 gigawatt hours per year as we posted on X. We expect to stay ahead of the Cybertruck ramp with the cell production throughout Q2 as we ramp the third of four lines in Phase 1, while maintaining multiple weeks of cell inventory to make sure we're ahead of the ramp. Because we're ramping, COGS continues to drop rapidly week-over-week driven by yield improvements throughout the lines and production volume increases. So our goal, and we expect to do this is to beat supplier cost of nickel-based cells by the end of the year.\\nMartin Viecha: Thank you. The second question is on Optimus. So what is the current status of Optimus? Are they currently performing any factory tasks? When do you expect to start mass production?\\nElon Musk: We are able to do simple factory tasks or at least, I should say, factory tasks in the lab. In terms of – we do think we will have Optimus in limited production in the natural factory itself, doing useful tasks before the end of this year. And then I think we may be able to sell it externally by the end of next year. These are just guesses. As I've said before, I think Optimus will be more valuable than everything else combined. Because if you've got a sentient humanoid robots that is able to navigate reality and do tasks at request, there is no meaningful limit to the size of the economy. So that's what is going to happen. And I think Tesla is best positioned of any humanoid robot maker to be able to reach volume production with efficient inference on the robot itself. I mean this perhaps is a point that is worth emphasizing Tesla's AI inference efficiency is vastly better than any other company. There is no company even close to the inference efficiency of Tesla. We've had to do that because we were constrained by the inference hardware in the car, we didn't have a choice. But that will pay dividends in many ways.\\nMartin Viecha: Thank you. The third question is, what is the current assessment of the pathway towards regulatory approval for unsupervised FSD in the U.S. And how should we think about the appropriate safety threshold compared to human drivers?\\nElon Musk: Sure.\\nLars Moravy: I can start. There are a handful of states that already have adopted autonomous vehicle laws. These states are paving the way for operations, while the data for such operations guides a broader adoption of driver-less vehicles. I think Ashok can talk a little bit about our safety methodology, but we expect that these states and the work ongoing as well as the data that we're providing will pave a way for a broad-based regulatory approval in the U.S. at least and then in other countries as well?\\nAshok Elluswamy: Yes. It's actually been pretty helpful that other autonomous car companies have been cutting a path through the regulatory jungle, which is absurd. That's actually quite helpful. And they have obviously been operating in San Francisco for a while. I think they got approval for City of LA. So these approvals are happening rapidly. I think if you've got at scale, a statistically significant amount of data that shows conclusively that the autonomous car has, let's say, half the accident rate of a human-driven car, I think, that's difficult to ignore because at that point, stopping autonomy means killing people. So I actually do not think that there will be significant regulatory barriers provided there was conclusive data that the autonomous car is safer than a human-driven car. And in my view, this will be much like elevators. Elevators used to be operated by a guy with relay switch. But sometimes that guy would get tired or drunk or just make a mistake, and shatter somebody in half between floors. So we just get an elevator and press button, we don't think about it. In fact, it's kind of weird if somebody is standing there with a relay switch. And that will be how cars work. You just summon the car using your phone, you get in, it takes you to a destination, you get out.\\nVaibhav Taneja: You don't even think about it?\\nElon Musk: You don't even think about it. Just like an elevator, it takes you to your floor. That's it. Don't think about how the elevator is working or anything like that. And something I should clarify is that Tesla will be operating the fleet. So you can think of like how Tesla, think of it’s like some combination of Airbnb and Uber, meaning that there will be some number of cars that Tesla owns itself and operates in the fleet. There will be some number of cars and then there'll be a bunch of cars where they're owned by the end user. That end user can add or subtract their car to the fleet whenever they want, and they can decide if they want to only let the car be used by friends and family or only by 5-star users or by anyone at any time they could have the car come back to them and be exclusively theirs, like an Airbnb. You could rent out your guest room or not, any time you want. So as our fleet grows, we have 7 million cars going to – 9 million cars going to, eventually tens of millions of cars worldwide. With a constant feedback loop, every time something goes wrong, that gets added to the training data and you get this training flywheel happening in the same way that Google Search has the sort of flywheel, it's very difficult to compete with Google because people are constantly doing searches and clicking and Google is getting that feedback loop. It’s the same with Tesla. But at a scale that is maybe difficult to comprehend, but ultimately, it will be tens of millions. I think there's also some potential here for an AWS element down the road where if we've got very powerful inference because we've got a Hardware 3 in the cars, but now all cars are being made with Hardware 4. Hardware 5 is pretty much designed and should be in cars, hopefully towards the end of next year. And there's a potential to run – when the car is not moving to actually run distributed inference. So kind of like AWS, but distributed inference. Like it takes a lot of computers to train an AI model, but many orders of magnitude less compute to run it. So if you can imagine future, perhaps where there's a fleet of 100 million Teslas, and on average, they've got like maybe a kilowatt of inference compute. That's 100 gigawatts of inference compute distributed all around the world. It's pretty hard to put together 100 gigawatts of AI compute. And even in an autonomous future where the car is, perhaps, used instead of being used 10 hours a week, it is used 50 hours a week. That still leaves over 100 hours a week where the car inference computer could be doing something else. And it seems like it will be a waste not to use it.\\nMartin Viecha: Ashok, do you want to chime in on the air process and safety?\\nAshok Elluswamy: Yes, we have multiple tiers of validating the safety in any given week, we train hundreds of neural networks that can produce different trajectories for how to drive the car, we replay them through the millions of clips that we have already collected from our users and our own QA. Those are like critical events, like someone jumping out in front or like other critical events that we have gathered database over many, many years, and we replay through all of them to make sure that we are net improving safety. And on top of it, we have simulation systems that also try to recreate this and test this in closed loop fashion. And some of this is validated, we give it to our own QA drivers. We have hundreds of them in different cities, in San Francisco, Los Angeles, Austin, New York, a lot of different locations. They are also driving this and collecting real-world miles, and we have an estimate of what are the critical events, are they a net improvement compared to the previous week’s builds. And once we have confidence that the build is a net improvement, then we start shipping to early users, like 2,000 employees initially that they would like it to build, they will give feedback on like if it's an improvement there or they're noting some new issues that we did not capture in our own QA process. And only after all of this is validated, then we go to external customers. And even when we go external, we have like live dashboards of monitoring every critical event that's happening in the fleet sorted by the criticality of it. So we are having a constant pulse on the build quality and the safety improvement along the way. And then any failures like Elon alluded to, we get the data back, add it to the training and that improves the model in the next cycle. So we have this like constant feedback loop of issues, fixes, evaluations and then rinse and repeat. And especially with the new V12 architecture, all of this is automatically improving without requiring much engineering interventions in the sense that engineers don't have to be creative in like how they code the algorithms. It's mostly learning on its own based on data. So you see that, okay, every failure or like this is how a person shows, this is how you drive this intersection or something like that, they get the data back. We add it to the neural network, and it learns from that trained data automatically instead of some engineers saying that, oh, here, you must rotate the steering wheel by this much or something like that. There's no hard inference conditions, it's everything is neural network, it's very soft, it's probabilistic. So it will adapt its probability distribution based on the new data that it's getting.\\nElon Musk: Yes. We do have some insight into how good the things will be in like, let's say, three or four months because we have advanced models that are far more capable than what is in the car, but have some issues with them that we need to fix. So they are like there'll be a step change improvement in the capabilities of the car, but it will have some quirks that are – that need to be addressed in order to release it. As Ashok was saying, we have to be very careful in what we release the fleet or to customers in general. So like – if we look at say 12.4 and 12.5, which are really could arguably even be Version 13, Version 14 because it's pretty close to a total retrain of the neural nets in each case are substantially different. So we have good insight into where the model is, how well the car will perform, in, say, three or four months.\\nAshok Elluswamy: Yes. In terms of scaling laws, people in the AI community generally talk about model scaling laws where they increase the model size a lot and then their corresponding gains in performance, but we have also figured out scaling laws and other access in addition to the model side scaling, making also data scaling. You can increase the amount of data you use to train the neural network and that also gives similar gains and you can also scale up by training compute, you can train it for much longer or make more GPUs or more Dojo nodes and that also gives better performance, and you can also have architecture scaling where you count with better architectures that for the same amount of compute for produce better results. So a combination of model size scaling, data scaling, training compute scaling and the architecture scaling, we can basically extract like, okay, with the continue scaling based on this – at this ratio, we can sort of predict future performance. Obviously, it takes time to do the experiments because it takes a few weeks to train, it takes a few weeks to collect tens of millions of video clips and process all of them, but you can estimate what’s going to be the future progress based on the trends that we have seen in the past, and they’re generally held true based on past data.\\nMartin Viecha: Okay. Thank you very much. I’ll go to the next question, which is, can we get an official announcement of the time line for the $25,000 vehicle?\\nLars Moravy: I think we – Elon mentioned it in the opening remarks. But as you mentioned, we’re updating our future vehicle lineup to accelerate the launch of our low-cost vehicles in a more CapEx efficient way. That’s our mission to get the most affordable cars to customers as fast as possible. These new vehicles we built on our existing lines and open capacity, and that’s a major shift to utilize all our capacity with marginal CapEx before we go spend high CapEx to do anything.\\nElon Musk: Yes. We’ll talk about this more on August 8. But really, the way to think of Tesla is almost entirely in terms of solving autonomy and being able to turn on that autonomy for a gigantic fleet. And I think it might be the biggest asset value appreciation history when that day happens when you can do unsupervised full self-driving.\\nLars Moravy: 5 million cars?\\nElon Musk: Yes.\\nLars Moravy: A little less?\\nElon Musk: Yes. It will be 7 million cars in a year or so and then 10 million and then eventually, we’re talking about tens of millions of cars. Not eventually, it’s like, yes, for the end of the decade, its several tens of millions of cars I think.\\nMartin Viecha: Thank you. The next question is, what is the progress of Cybertruck ramp?\\nLars Moravy: I can take that one too. Cybertruck had 1K a week just a couple of weeks ago. This happened in the first four to five months since we SOP [ph] late last year. Of course, volume production is what matters. That’s what drives costs and so our costs are dropping, but the ramp still faces like a lot of challenges with so many new technologies, some supplier limitations, et cetera, and continue to ramp this year, just focusing on cost efficiency and quality.\\nMartin Viecha: Okay. Thank you. The next question, have any of the legacy automakers contacted Tesla about possibly licensing FSD in the future?\\nElon Musk: We’re in conversations with one major automaker regarding licensing FSD.\\nMartin Viecha: Thank you. The next question is about the robotaxi unveil. Elon already talked about that. So we’ll have to wait till August. The following question is about the next-generation vehicle. We already talked about that. So let’s go to the semi. What is the time line for scaling semi?\\nElon Musk: I think…\\nLars Moravy: So we’re finalizing the engineering of the semi to enable like a super cost-effective high-volume production with our learnings from our fleet and our pilot fleet and Pepsi’s fleet, which we are expanding this year marginally. In parallel, as we showed in the shareholders’ deck, we have started construction on the factory in Reno. Our first vehicles are planned for late 2025 with external customers starting in 2026.\\nMartin Viecha: Okay. A couple more questions. So our favorite, can we make FSD transfer permanent until FSD is fully delivered with Level 5 autonomy?\\nLars Moravy: Yes.\\nMartin Viecha: Okay. Next question, what is the getting the production ramp at Lathrop, where do you see the Megapack run rate at the end of the year. Mike?\\nUnidentified Company Representative: Yes. Yes, Lathrop is ramping as planned. We have our second GA line allowing us to increase our exit rate from 20 gigawatt hours per year to – at the start of this year to 40 gigawatt hours per year by the end of the year, that lines commissioned. There’s really nothing limiting the ramp. Its given the longer sales cycles for these large projects, we typically have order visibility 12 months to 24 months prior to ship dates. So we’re able to plan – the build plan several quarters in advance. So this allows us to ramp the factory to align with the business and order growth. Lastly, we’d like to thank our customers globally for their trust in Tesla as a partner for these incredible projects.\\nMartin Viecha: Okay. Thank you very much. Let’s go to analyst questions. The first question comes from Tony Sacconaghi from Bernstein. Tony, please go ahead and unmute.\\nTony Sacconaghi: Thank you for taking the question. I was just wondering if you can elaborate a little bit more on kind of the new vehicles that you talked about today. Are these like tweaks on existing models, given that they’re going to be running on the same lines? Are these like new models? And how should we think about them in the context of like the Model 3 Highland update, what will these models be like relative to that? And given the quick time frame, Model 3 Highland has required a lot of work and a lot of retooling. Maybe you can help put that all in context. Thank you, and I have a follow-up, please.\\nElon Musk: I think we've said, we were on that front. So what’s your follow-up?\\nTony Sacconaghi: It’s a more personal one for you, Elon, which is that you’re leading many important companies right now. Maybe you can just talk about where your heart is at in terms of your interests and do you expect to lessen your involvement with Tesla at any point over the next three years?\\nElon Musk: Tesla constitutes a majority of my work time and I work pretty much every day of the week. It’s rare for me to take a Sunday afternoon. So I’m going to make sure Tesla is quite prosperous. And it is – like it is prosperous and it will be very much so in the future.\\nMartin Viecha: Okay. Thank you. Let’s go to Adam Jonas from Morgan Stanley. Adam, please go ahead and unmute.\\nAdam Jonas: Okay. Great. Hey, Elon. So you and your team on volume expect a 2024 growth rate, notably lower than that achieved in 2023. But what's your team's degree of confidence on growth above 0%? Or in other words, does that statement leave room for potentially lower sales year-on-year?\\nElon Musk: No, I think we'll have higher sales this year than last year.\\nAdam Jonas: Okay. My follow-up, Elon, on future product. If you had nailed execution, assuming that you nail execution on your next-gen cheaper vehicles, more aggressive giga castings, I don't want to say one piece, but getting closer to one piece, structural pack, unboxed, 300-mile range, $25,000 price point, putting aside robotaxi, those features unique to you. How long would it take your best Chinese competitors to copy a cheaper and better vehicle that you could offer a couple of years from now? How long would it take your best Chinese competitors to copy that? Thanks.\\nElon Musk: I mean, I don't know what our competitors could do, except we've done relatively better than they have. If you look at the drop in our competitors in China sales versus our drop in sales, our drop was less than theirs. So we're doing well. But I think Cathy Wood said it best, like really, we should be thought of as an AI or robotics company. If you value Tesla as just like an auto company, you just have to – fundamentally, it's just the wrong framework and it will come to be. If you ask the wrong question, then the right answer is impossible. So I mean, if somebody doesn't believe Tesla is going to solve autonomy, I think they should not be an investor in the company. Like, that is – but we will and we are. And then you have a car that goes from 10 hours of use a week, like 1.5 hours a day to probably 50%, but it costs the same.\\nVaibhav Taneja: I think that's the key thing to remember, right, especially if you look at FSD Supervised, if you didn't believe in autonomy, this should give you a review that this is coming. It's actually getting better day by day.\\nElon Musk: Yes. If you've not tried the FSD 12.3, and like I said, 12.4 is going to be significantly better and 12.5 even better than that. And we have visibility into those things. Then you really don't understand what's going on. It's not possible.\\nVaibhav Taneja: Yes. And that's why we can't just look at just as a car company because a car company would just have a car. But here, we have more than a car company because the cars can be autonomous. And like I said, it's happening.\\nAshok Elluswamy: Yes. This is all in addition to Tesla – the overall AI community is just like increasing – like, improving rapidly.\\nElon Musk: Yes. I mean we're putting the actual auto in automobile. So sort of – we go like, well, sort of like tell us about future horse carriages you're making. I'm like, well, actually, it doesn't need a horse that's the whole point. That's really the whole point.\\nMartin Viecha: Okay, thank you. The next question comes from Alex Potter from Piper Sandler. Alex, please go ahead and unmute.\\nAlex Potter: Great, thanks. Yes, so I couldn't agree more. The thesis hinges completely on AI, the future of AI, full self-driving neural net training, all of these things. In that context, Elon, you've spoken about your desire to obtain 25% voting control of the company. And I understand completely why that would be. So I'm not necessarily asking about that. I'm asking if you've come up with any mechanism by which you can ensure that you'll obtain that level of voting control. Because if not, then the core part of the thesis could potentially be at risk. So any additional commentary you might have on that topic.\\nElon Musk: Well, I think no matter what Tesla, even if I got kidnapped by aliens tomorrow, Tesla will solve autonomy, maybe a little slower, but it would solve autonomy for vehicles at least. I don't know if it would winon with respect to Optimus or with respect to future products, but it would that there's enough momentum for Tesla to solve autonomy even if I disappeared for vehicles. Yes, there's a whole range of things we can do in the future beyond that. I'll be more reticent with respect to Optimus, if we have a super-sentient humanoid robot that can follow you indoors and that you can escape, we're talking terminator-level risk. And yes, I'd be uncomfortable with. If there's not some meaningful level of influence over how that is deployed. And if there's shareholders have an opportunity to ratify or reratify the sort of competition because I can't say that. That is a fact. They have an opportunity. And yes, we'll see. If the company generates a lot of positive cash flow, we could obviously buy back shares.\\nAlex Potter: All right. That's actually all very helpful context. Thank you. Maybe one final question and I'll pass it on. OpEx reductions, thank you for quantifying the impact there. I'd be interested also in potentially more qualitative discussion of what the implications are for these headcount reductions. What are the types of activities that you're presumably sacrificing as a result of parting ways with these folks? Thanks very much.\\nVaibhav Taneja: So like we said, we've done these headcount reductions across the board. And as companies grow over time, there are certain redundancies. There's some duplication of efforts, which happens in certain areas. So you need to go back and look at where all these pockets are, get rid of it. So we're basically going through that exercise wherein we're like, hey, how do we set this company right for the next phase of growth. And the way to think about it is any tree which grows, it needs pruning. This is the pruning exercise which we went through. And at the end of it, we'll be much stronger and much more resilient to deal with the future because the future is really bright. Like I said in my opening remarks, we just have to get through this period and get there.\\nElon Musk: Yes, we're not giving up anything that is significant that I'm aware of. So we've had a long period of prosperity from 2019 to now. And so if a company sort of organizationally is 5% wrong per year, that accumulates to 25%, 30% of inefficiency. We've made some corrections along the way. But it is time to reorganize the company for the next phase of growth and you really need to reorganize it, just like a human when we start off with one cell and kind of zygote, blastocyst and you start growing arms and legs and briefly, you have a tail. And so…\\nAlex Potter: But you shed the tail.\\nElon Musk: You shed the tail, hopefully. And then you're baby, you basically, you have to be the organism – a company is kind of like creature growing. And if you don't reorganize it for different phases of growth, it will fail. You can't have the same organizational structure if you're 10 cells versus 100 cells versus 1 million cells versus 1 billion cells versus 1 trillion cells. Humans are around 35 trillion cells, doesn't feel like it feels like, like one person. But you're basically a walking cell colony of roughly 35 trillion depending on your body mass and about three times that number in bacteria. So anyway, you've got to reorganize the company for a new phase of growth or will fail to achieve that growth.\\nMartin Viecha: Thank you. Let's go to Mark Delaney from Goldman Sachs. Mark please go ahead and unmute.\\nMark Delaney: Yes. Good afternoon. Thanks very much for taking the question. The company previously characterized potential FSD licensing discussions in the early phase and some OEMs had not really been believing in it. Can you elaborate on how much the licensing business opportunity you mentioned today has progressed? And is there anything Tesla needs to achieve with the technology in terms of product milestones in order to be successful at reaching a licensing agreement in your view?\\nElon Musk: Well, I think we just need to – it just needs to be obvious that our approach is the right approach. And I think it is. I think we've now with 12.3, if you just have the car drive you around; it is obvious that our solution with a relatively low-cost inference computer and standard cameras can achieve self-driving. No LiDARs, no radars, no ultrasonic nothing.\\nVaibhav Taneja: No heavy integration work for vehicle manufacturers.\\nElon Musk: Yes. So it really just be a case of having them use the same cameras and inference computer and licensing our software. But once it becomes obvious that if you don't have this in a car, nobody wants your car. It's a smart car. I still remember in, back when Nokia was king of the hill, Yes, crushing. And they certainly come out with a smartphone that was basically a break with limited functionality. And then the iPhone and Android, people still do not understand that all the phones are going to be that way. There's not going to be any flip [ph] phones. If there will be a niche product.\\nLars Moravy: Or home phones.\\nElon Musk: Yes, no even exactly. When is the last time you saw a home phone.\\nLars Moravy: No idea in a hotel, sometimes in hotels.\\nElon Musk: Yes, the hotels have them. Yes. So the people don't understand all cars will need to be smart cars, or you will not sell or the car will not – nobody would buy it. Once that becomes obvious, I think licensing becomes not optional.\\nMark Delaney: It becomes a method of survival?\\nElon Musk: Yes, absolutely, it is. License it or nobody will buy your car.\\nVaibhav Taneja: I mean one other thing which I'll add is in the conversations, which we've had with some of these OEMs, I just want to also point out that they take a lot of time in their product life cycle.\\nElon Musk: Yes.\\nVaibhav Taneja: They're talking about years before they will put it in their product. We might have a licensing deal earlier than that, but it takes a while. So this is where the big difference between us and them is, right?\\nElon Musk: Yes, I mean, really a deal signed now would result in it being in a car probably three years.\\nVaibhav Taneja: That would be early.\\nElon Musk: Yes. That's like lightening basically.\\nLars Moravy: That's in eager [ph] OEM.\\nElon Musk: Yes. So I wouldn't be surprise if we do sign a deal. I think we have a good chance we do sign a deal this year, maybe more than one. But yes, it would be probably three years before it's integrated with a car. Even though all you need is cameras and our inference computer. So just talking about a massive design change.\\nVaibhav Taneja: Yes. And again, just to clarify, it's not the work which we have to do. It's the work which they have to do, which will take the time.\\nElon Musk: Yes.\\nVaibhav Taneja: Mark, is it helpful?\\nMark Delaney: Yes, very helpful. Thank you. My follow-up was to better understand Tesla's approach to pricing going forward. Previously, the company had said that the price reductions were driving incremental demand with how affordable the cars have become, especially for vehicles that have access to IRA credits and some of the leasing offers that Tesla has in place. Do you still see meaningful incremental price reductions as making sense from here for the existing products? And can the company meaningfully lower prices from here and also stay free cash flow positive on an annual basis with the current product set? Thanks.\\nElon Musk: Yes. I think we can be free cash flow positive meaningfully.\\nLars Moravy: I think Vaibhav said it in his opening remarks, like our cost down efforts, we basically were offsetting the price cut like we’re trying to give it back to the customers.\\nElon Musk: Yes. I mean the end of the day, like for any given company, if you sell a great product at a great price – if you have a great product at a great price, the sales will be excellent. That’s true of any area. So over time, we do need to keep making sure that we’re – that it’s a great product at a great price. And moreover, that price is accessible to people. So it’s not – you have to solve both the value for money and the fundamental affordability question. The fundamental affordability question is sometimes overlooked. If somebody is earning several hundred thousand dollars a year, they don’t think of a car from a fundamental affordability standpoint. But from vast majority of people are living paycheck to paycheck. So it actually makes a difference if the cost per month for lease refinancing is $10 one way or the other. So it is important to keep improving the affordability and to keep making the price.\\nLars Moravy: More accessible.\\nElon Musk: Yes, exactly. Make the price more accessible, the value for money better, and to keep improving that over time.\\nLars Moravy: But also make kick as cost that people want to buy.\\nElon Musk: Yes, it’s going to be a great product and at a great price. And the standards for what constitutes great product at a great price keep increasing. So there’s like – you can’t just be static. You have to keep making the car better, improving the price, but improving the cost of production, and that’s what we’re doing.\\nVaibhav Taneja: Yes. And in fact, like I said in my opening remarks also, like the revised – the updated Model 3 is a fantastic car. I don’t think people fully even understand that lot of engineering effort which has gone and Lars and team have actually put out videos explaining how much the car is different. I mean it looks and feels different. Not only it looks and feels different. We’ve added so much value to it, but you can lease it for like as low as $299 a month.\\nLars Moravy: Without gas.\\nVaibhav Taneja: Yes.\\nMartin Viecha: All right. The next question comes from George from Canaccord. George, please go ahead and unmute.\\nUnidentified Analyst: Hi, thank you for taking my question. First, could you please help us understand some of the timing of launching FSD in additional geographies, including maybe clarifying your recent comment about China? Thank you.\\nElon Musk: I mean like new markets, yes, we are – there are a bunch of markets where we don’t currently sell cars that we should be selling cars in. We’ll see some acceleration of that.\\nUnidentified Analyst: And FSD new markets?\\nElon Musk: Yes. So think about the end-to-end neural net-based autonomy is that just like a human, it actually works pretty well without modification in almost any market. So we plan on – with the approval of the regulators, releasing it as a supervised autonomy system in any market that – where we can get regulatory approval for that, which we think includes China. So yes, it’s – just like a human, you can go rent a car in a foreign country and you can drive pretty well. Obviously, if you live in that country, you’ll drive better. And so we’ll make the car drive better in these other countries with country-specific training. But it can drive quite well almost everywhere.\\nVaibhav Taneja: The basics of driving are basically same everywhere like car is a car, the traffic lights, road is the road. Yes.\\nElon Musk: It understands that it shouldn’t hit things, no matter what the road rules are.\\nVaibhav Taneja: Exactly. There are some road rules that you need to follow. And in China, you shouldn’t cross over a solid line to do a lane change. In U.S. it’s a recommendation I think. In China, you get fined heavily if you do that. We have to do some more actions, but it’s mostly smaller reduction. It’s not like the entire change or type or something.\\nElon Musk: Yes.\\nMartin Viecha: Hey, George, do you have a follow-up?\\nUnidentified Analyst: Yes. So my follow-up has to do with the first quarter deliveries and I’m curious as to whether or not you feel that supply constraints that you mentioned throughout the release impacted the results and maybe can you help us quantify that? And is that why you have some confidence in unit growth in 2024?\\nVaibhav Taneja: Yes. I think we did cover this a little bit in the opening remarks to you. Q1 had a lot of different things which are happening. Seasonality was a big one, continued pressure from the macroeconomic environment. We had attacks at our factory. We had Red Sea attacks, we are ramping Model 3, we’re ramping Cybertruck. All these things are happening. I mean, it almost feels like a culmination of all those activities in a constrained period. And that gives us that confidence that, hey, we don’t expect these things to recur.\\nElon Musk: Yes. We think Q2 will be a lot better.\\nVaibhav Taneja: Yes.\\nLars Moravy: It’s just one thing after another. Our Cybertrucks are crazy. Thank you.\\nElon Musk: Yes, exactly. It’s just – if you’ve got cars that are sitting on ships, they obviously cannot delivered to people. And if you’ve got the excess demand for Model 3 and Model Y in one market, but you don’t have it there. It’s quite a – it’s extremely complex logistics situation. So I’d say also the – we did overcomplicate the sales process, which we’ve just in the past week or so have greatly simplified. So it became far too complex to buy a Tesla, whereas it should just be you can buy the car in under a minute. So we’re getting back to that you can buy a Tesla in under an minute interface from what was quite complex.\\nMartin Viecha: Okay, thank you. Let’s go to Colin Rusch from Oppenheimer. Colin, go ahead and unmute, please.\\nColin Rusch: Thanks so much, guys. Given the pursuit of Tesla really as a leader in AI for the physical world, in your comments around distributed inference, can you talk about what that approach is unlocking beyond what’s happening in the vehicle right now?\\nElon Musk: Do you want to say something?\\nAshok Elluswamy: Yes. Like Elon mentioned like the car even when it's a full robotaxi it's probably going to be used 150 hours a week.\\nElon Musk: That's my guess like a third of the hours of the week.\\nAshok Elluswamy: Yes. It could be more or less, but then there's certainly going to be some hours left for charging and cleaning and maintenance in that world, you can do a lot of other workloads, even right now we are seeing, for example, these LLM companies have these like batch workloads where they send a bunch of documents and those run through pretty large neural networks and take a lot of compute to chunk through those workloads. And now that we have already paid for this compute in these cars, it might be wise to use them and not let them be idle, be like buying a lot of expensive machinery and leaving to them idle. Like we don't want that, we want to use the computer as much as possible and close to like basically 100% of the time to make it a use of it.\\nElon Musk: That’s right. I think it's analogous to Amazon Web Services, where people didn't expect that AWS would be the most valuable part of Amazon when it started out as a bookstore. So that was on nobody's radar. But they found that they had excess compute because the compute needs would spike to extreme levels for brief periods of the year and then they had idle compute for the rest of the year. So then what should they do to pull that excess compute for the rest of the year? That's kind of...\\nAshok Elluswamy: Monetize it\\nElon Musk: Yes, monetize it. So, it seems like kind of a no-brainer to say, okay, if we've got millions and then tens of millions of vehicles out there where the computers are idle most of the time that we might well have them do something useful.\\nAshok Elluswamy: Exactly.\\nElon Musk: And then, I mean, if you get like to the 100 million vehicle level, which I think we will, at some point, get to, then – and you've got a kilowatt of useable compute and maybe your own hardware 6 or 7 by that time. Then you really – I think you could have on the order of 100 gigawatts of useful compute, which might be more than anyone more than any company, probably more than a company.\\nAshok Elluswamy: Yes, probably because it takes a lot of intelligence to drive the car anyway. And when it's not driving the car, you just put this intelligence to other uses, solving scientific problems or answer in terms of someone else.\\nElon Musk: It's like a human, ideally. We've already learned about deploying workloads to these nodes\\nAshok Elluswamy: Yes. And unlike laptops and our cell phones, it is totally under Tesla's control. So it's easier to distribute the workload across different nodes as opposed to asking users for permission on their own cell phones to be very tedious.\\nElon Musk: Well, you're just draining the battery on the phone.\\nAshok Elluswamy: Yes, exactly. The battery is also...\\nElon Musk: So like technically, I suppose like Apple would have the most amount of distributed compute, but you can't use it because you can't get the – you can't just run the phone at full power and drain the battery.\\nAshok Elluswamy: Yes.\\nElon Musk: So, whereas for the car, even if you're a kilowatt level inference computer, which is crazy power compared to a phone. If you've got 50 or 60 kilowatt hour pack, it's still not a big deal to run if you are plugged it – whether you plugged it or not – you could be plugged in or not like you could run for 10 hours and use 10-kilowatt hours of your kilowatt of compute power.\\nLars Moravy: Yes. We got built in like liquid cold thermal management.\\nElon Musk: Yes, exactly.\\nLars Moravy: Exactly for data centers, it's already there in the car.\\nElon Musk: Exactly. Yes. Its distributed power generation – distributed access to power and distributed cooling, that was already paid for.\\nAshok Elluswamy: Yes. I mean that distributed power and cooling, people underestimate that costs a lot of money.\\nVaibhav Taneja: Yes. And the CapEx is shared by the entire world sort of everyone wants a small chunk, and they get a small profit out of it, maybe.\\nElon Musk: Yes.\\nColin Rusch: Thanks so much guys. And just my follow-up is a little bit more mundane. Looking at the 4680 ramp, can you talk about how close you were to target yields and when you might start to accelerate incremental capacity expansions on that technology?\\nElon Musk: We're making good progress on that. But I don't think it's super important for at least in the near term. As Lars said, we think it will be exceed the competitiveness of suppliers by the end of this year and then we'll continue to improve.\\nLars Moravy: Yes. I mean, I think it's important to note also that like the ramp right now is relevant to the Cybertruck ramp.\\nElon Musk: Yes.\\nLars Moravy: And so like we're not going to just randomly build 4680s unless we have a place to put them and so we're going to make sure we're prudent about that. But we also have a lot of investments with all our cell suppliers and vendors. They're great partners, and they've done great development work with us and a lot of the advancements in technologies and chemistry we found 4680, they're also putting into their cells.\\nElon Musk: Yes. I mean a big part of the 4680, Tesla doing internal cells was a hedge against what would happen with our suppliers because for a while they are it was very difficult because every big carmaker put in massive battery orders, and so the price per kilowatt hour of lithium-ion batteries went to crazy numbers, crazy levels.\\nVaibhav Taneja: Bonkers.\\nElon Musk: Yes, just bonkers. So like, okay, we've got to have some hedge here to deal with cost per kilowatt hours of numbers that were double what we anticipated. If we have an internal cell production, then we have that hedge against demand shocks, we have too much demand. That's really the way to think about it. It's not like we want to take on a whole bunch of problems just for the hell of it. We did the cell program in order to address the crazy increase in cost per kilowatt hour from our suppliers due to gigantic orders placed by every carmaker on earth.\\nMartin Viecha: Okay. Thank you. And the last question comes from Ben Kallo from Baird. Ben, go ahead and unmute. Ben, you're still muted.\\nElon Musk: Well, I want to say again, we'd just like to strongly recommend that anyone who is, I guess, thinking about the Tesla stock should really drive FSD 12.3. It really – you can't – it's impossible to understand the company if you do not do this.\\nMartin Viecha: All right. So since Ben is not unmuting. Let's try Shreyas Patil from Wolfe Research. Final question.\\nShreyas Patil: Thanks so much. Just Elon, during the Investor Day last year, you mentioned that auto COGS per unit for the next-gen vehicle would decline by 50% versus the current three and Y. I think that was implying something around $20,000 of COGS. About one-third of that was coming from the on-box manufacturing process. But I'm curious if you see an opportunity that the – some of the other drivers around powertrain cost reduction or material cost savings, would those be largely transferable to some of the new products that you're now talking about introducing?\\nLars Moravy: Yes, sure. I mean, in short, yes, I mean, like the on-box manufacturing method is certainly great and revolutionary, but with it comes some risks because new production lines and not, but all the subsystems we developed, whether it was powertrains, drive units, battery improvements in manufacturing and automation, thermal systems, seating, integration of interior components and reduction of LV controllers, all that's transferable, and that's what we're doing, trying to get it in their products as fast as possible. And so yes, that engineering work, we're not trying to just throw it away and put a cars and we're going to take it and utilize it and utilize it to the best advantage of the cars we make and the future cars make.\\nShreyas Patil: Okay. Great. And then just on that topic of 4680 cells, I know you mentioned it, you really thought of it more as like a hedge against rising battery costs from other OEMs. But it seems even today, it seems like you would have a cost advantage against some of those other automakers. And I'm wondering, given the rationalizing of your vehicle manufacturing plans that you're talking about now, if there's an opportunity to maybe convert the 4680 cells and maybe sell those to other automakers and really generate an additional revenue stream. I'm just curious if you have any thoughts about that.\\nElon Musk: Great. What seems to be happening is that the I'm missing something, the orders for batteries from other automakers have declined dramatically. So we're seeing much more competitive prices for sales from our suppliers, dramatically more competitive than in the past. It is clear that a lot of our suppliers have excess capacity.\\nVaibhav Taneja: Yes. In addition to what Elon, this is kind of in addition to what Elon said, about 4680, what 4680 did for us from a supply chain perspective was help us understand the supply chain that's upstream of our cell suppliers. So a lot of the deals that we had struck for 4680, we can also supply those materials to our partners, help reducing the overall cost back to Tesla. So we're basically inserting ourselves in the upstream supply chain by doing that. So that's also been beneficial in reducing the overall pricing in addition to the excess capacity that these suppliers have.\\nElon Musk: Yes. No, I mean this is going to wax and wane, obviously. So there's going to be a boom and bust in battery cell production where production exceeds supply and then supply exceeds production and back and forth kind of like, I don't know, DRAM or something. But Yes. So it's like what is true today will not be true in the future, there's going to be somewhat of a boom and bust cycle here. And then there are additional complications with government incentives like the Inflation Reduction Act, the IRA, Joe [ph] has found like a funny name.\\nVaibhav Taneja: Comical name.\\nElon Musk: Yes, it is like Irish Republican Army, The Internet Research Agency from Russia.\\nVaibhav Taneja: Independent retirement account.\\nElon Musk: Yes, exactly. Roth IRA. It's like Spider-Man situation, which IRA wins. So but it is complicate the incentive structure. So that is there's the stronger demand for cells that are produced in the U.S. than outside the U.S. But then how long is that the IRA last, I don't know.\\nVaibhav Taneja: Which is why it's important that we have both internet [ph] cells and vendor cells that hedge against all of this.\\nMartin Viecha: Okay. Thank you very much. That's all the time we have today. But at the same time, I would like to make a short announcement. And I wanted to let the investment community know that about a month ago, I met up with Elon and Vaibhav and announced that I'll be moving on from the world of Investor Relations. I'll be hanging around for another couple of months or so. So feel free to reach out at any time. But after the seven year sprint, I'm going to be taking a break and spending some good quality time with my family. And I wanted to say that these seven years have been the greatest privilege of my professional life. I'll never forget the memories from I started literally at the beginning of production hell and just watching the company from the inside to see what it's become today. And especially super thankful to the people in this room and dozens of people outside of this room that I've worked for over the years. I think the team's strength and teamwork at Tesla is unlike anything else I've seen in my career. Elon, thank you very much for this opportunity that I got back in 2017. Thank you for seeking investor feedback and regularly and debating it with me.\\nElon Musk: Yes. Well, I mean the reason I reached out to you was because I thought your analysis of Tesla was the best that I had seen.\\nMartin Viecha: Thank you.\\nElon Musk: So, thank you for helping Tesla to get to where it is today over seven years. It's been a pleasure working with you.\\nMartin Viecha: Thank you so much. And yes, thank you for all the thousands of shareholders that we've met over the years and walked around factories and loved all the interactions, even the tough ones. And yes, looking forward to the call in the next three months, but I'll be on the other side, listening in. Thank you very much.\\nVaibhav Taneja: Thanks.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=100)\n",
    "\n",
    "texts = text_splitter.create_documents([documents])\n",
    "split_texts = text_splitter.split_documents(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_ls = [x.page_content for x in split_texts]\n",
    "len(split_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/138 [00:00<?, ?it/s]/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "  1%|          | 1/138 [00:25<57:21, 25.12s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sub \u001b[38;5;129;01min\u001b[39;00m tqdm(split_ls):\n\u001b[1;32m     13\u001b[0m     summarizer \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummarization\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/bart-large-cnn\u001b[39m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28msum\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43msummarizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     summ_ls\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28msum\u001b[39m)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m#print(sum)\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py:269\u001b[0m, in \u001b[0;36mSummarizationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    246\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m    Summarize the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;124;03m          ids of the summary.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py:167\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    139\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(el, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(res) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result)\n\u001b[1;32m    172\u001b[0m     ):\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/transformers/pipelines/base.py:1257\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1250\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1251\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m         )\n\u001b[1;32m   1255\u001b[0m     )\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/transformers/pipelines/base.py:1264\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1263\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1264\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/transformers/pipelines/base.py:1164\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1163\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1164\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py:191\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m     in_b, input_length \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mshape(model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_inputs(\n\u001b[1;32m    187\u001b[0m     input_length,\n\u001b[1;32m    188\u001b[0m     generate_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmin_length),\n\u001b[1;32m    189\u001b[0m     generate_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_length),\n\u001b[1;32m    190\u001b[0m )\n\u001b[0;32m--> 191\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m out_b \u001b[38;5;241m=\u001b[39m output_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/transformers/generation/utils.py:1745\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_attention_mask_for_generation(\n\u001b[1;32m   1740\u001b[0m         inputs_tensor, generation_config\u001b[38;5;241m.\u001b[39m_pad_token_tensor, generation_config\u001b[38;5;241m.\u001b[39m_eos_token_tensor\n\u001b[1;32m   1741\u001b[0m     )\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[1;32m   1744\u001b[0m     \u001b[38;5;66;03m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_encoder_decoder_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_input_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[38;5;66;03m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/transformers/generation/utils.py:549\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[0m\n\u001b[1;32m    547\u001b[0m encoder_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    548\u001b[0m encoder_kwargs[model_input_name] \u001b[38;5;241m=\u001b[39m inputs_tensor\n\u001b[0;32m--> 549\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]: ModelOutput \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoder_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:1114\u001b[0m, in \u001b[0;36mBartEncoder.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1106\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1107\u001b[0m             encoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1108\u001b[0m             hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1111\u001b[0m             output_attentions,\n\u001b[1;32m   1112\u001b[0m         )\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1114\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:567\u001b[0m, in \u001b[0;36mBartEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;124;03m    hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;124;03m        returned tensors for more detail.\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 567\u001b[0m hidden_states, attn_weights, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    573\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    574\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:496\u001b[0m, in \u001b[0;36mBartSdpaAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    492\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_causal \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;66;03m# NOTE: SDPA with memory-efficient backend is currently (torch==2.1.2) bugged when using non-contiguous inputs and a custom attn_mask,\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;66;03m# but we are fine here as `_shape` do call `.contiguous()`. Reference: https://github.com/pytorch/pytorch/issues/112577\u001b[39;00m\n\u001b[0;32m--> 496\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m (bsz, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim):\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`attn_output` should be of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(bsz,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\u001b[38;5;250m \u001b[39mtgt_len,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_output\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    509\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "\n",
    "summ_ls = []\n",
    "\n",
    "for sub in tqdm(split_ls):\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device='mps')\n",
    "    sum = summarizer(sub, max_length=150, min_length=30, do_sample=False)\n",
    "    summ_ls.append(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'summary_text': \"This conference call is being recorded and will be available for replay from the Investor Relations section of Airbnb's website following this call. I will now hand the call over to Elli Mertz, VP of Finance. Please go ahead.\"}],\n",
       " [{'summary_text': \"Ellie Mertz: Thank you. Good afternoon and welcome to Airbnb's third quarter of 2023 earnings call. On the call today, we have Airbnb's Co-Founder and CEO, Brian Chesky and our Chief Financial Officer, Dave Stephenson. During the call, we'll make brief opening remarks and then spend the remainder of time on Q&A.\"}],\n",
       " [{'summary_text': 'During this call, we will discuss some non-GAAP financial measures. These measures are not intended to be a substitute for our GAAP results. You should be aware that these statements should be considered estimates only.'}],\n",
       " [{'summary_text': 'Brian Chesky: Q3 was another strong quarter for Airbnb. We had over 113 million Nights and Experiences Booked. Revenue of $3.4 billion grew 18% year-over-year. Free cash flow for the quarter was $1.3 billion.'}],\n",
       " [{'summary_text': 'Q3 was a record travel season on Airbnb. Nights and Experiences Booked grew 14% in Q3 compared to a year ago. We saw an acceleration of nights growth across all geographies. In Asia Pacific, our business has fully recovered to pre-pandemic level.'}],\n",
       " [{'summary_text': 'During Q3 alone, Airbnb host earned more than $19 billion. We ended the quarter with the highest number of active listing and we saw strong active listings growth across all regions of the market types.'}],\n",
       " [{'summary_text': 'In the past year alone, this has included things such as improved customer service, total price display and new tools to help host set more competitive prices. We have even more improvements coming as part of our November 8 winter release.'}],\n",
       " [{'summary_text': \"Korea has now become one of our fastest growing countries compared to 2019 with gross nights booked 54% higher than they were in Q3 2019 on origin basis. As international travel continues to recover, we're building greater momentum for Airbnb in underpenetrated markets.\"}],\n",
       " [{'summary_text': \"We'll take our first question from Mark Mahaney at Evercore ISI. Mark, could you talk about some of these improvements you've seen in markets like Germany, Brazil and Korea. And then, in terms of future services that you could offer to sellers, any update on when we could see those particularly things like sponsored listings for sellers for host.\"}],\n",
       " [{'summary_text': \"Airbnb is in 220 countries in the region. Chesky: We think there's a huge amount of growth if we could just get Airbnb to even a fraction of the percentage of penetration that we have in the United States.\"}],\n",
       " [{'summary_text': \"We've just scratched the surface of what we can do in Germany, Brazil and Korea. I think those markets are on a good trajectory. They could be significantly larger. We're now looking at Japan and India, China, around Asia Pacific. We have some optimizations in Southeast Asia, continual growth in Mexico.\"}],\n",
       " [{'summary_text': \"Over the next couple of years, I think you're going to see a number of new services roll out for host. have these new tools and services, including sponsor listings. Co-hosting is a service where we match host that don't have homes but they have extra time with homeowners to have space.\"}],\n",
       " [{'summary_text': \"Eric Sheridan: I just have one. Brian, in a number of interviews in the quarter, you talked about potential for product road map over the longer term. How do you think about product evolution that's being offered to the consumers on the platform?\"}],\n",
       " [{'summary_text': \"Brian Chesky: We've really, really benefited by being focused. It was always our attention to do much more than just short-term housing for travelers. So we're working on making Airbnb more of an extensible platform.\"}],\n",
       " [{'summary_text': 'The company is thinking about using AI to reimagine much of its product catalog. \"Imagine Airbnb being almost like the ultimate travel agent as an app,\" he says.'}],\n",
       " [{'summary_text': \"Brian Chesky: And Brian, sorry, are you referring to things we've already shipped or things that we're working on that we haven't shipped? Operator: We'll move to our next question from Brian Nowak at Morgan Stanley.\"}],\n",
       " [{'summary_text': 'Brian Nowak: Yes, if you have one that already shipped that would be great. If you have other ones you want to tell us about next week, that would also be good too.'}],\n",
       " [{'summary_text': \"Brian Chesky: So yes, so let me -- why don't I answer the innovation and Dave, you can talk about the guide for going forward? So maybe let me talk about some things that we've already done. I can give a little bit of sense of how we're thinking about next week and beyond.\"}],\n",
       " [{'summary_text': 'Since we rolled out new pricing tools, about half of new listings are now offering a monthly discount. We also have this new tool called similar listings, where you can see where other people are charging around you.'}],\n",
       " [{'summary_text': 'On a U.S. basis, our price is actually down 3% in North America, while hotels are up towards double digits, I think. For every person who stays in Airbnb, approximately 9 people every night stay in a hotel or about 9 bookings.'}],\n",
       " [{'summary_text': '\"I think I\\'m pretty optimistic about what you\\'ll see next week,\" he says. \"We\\'re already working on stuff for next May and next October releases as well\"'}],\n",
       " [{'summary_text': \"David Stephenson: For the fourth quarter, we have our revenue guidance between $2.13 billion and $ 2.17 billion. In terms of the nights guide, we're just seeing some variability in our nights demand. And so we're not being specific on it but anticipate nights to be a few points below.\"}],\n",
       " [{'summary_text': \"Lee Horowitz: Can you maybe help us think about how you guys are tracking towards expectations on occupancy or utilization moving forward? And then, maybe just one high level one. Sticking beyond the current cycle, we've seen a lot of other remote travel models, sort of hit this low teens to high single-digit growth rate and decelerate from there. Could you maybe take a step back and help us better understand how you think that maybe Airbnb may be a little bit different than prior ratios?\"}],\n",
       " [{'summary_text': \"Brian Chesky: Yes. Yes, you start with occupancy and I'll take the second question. Brian Chesky. Yes. You start with Occupancy and I'd like to know how many bedrooms you have.\"}],\n",
       " [{'summary_text': \"In terms of occupancy, we've actually seen it be pretty stable in terms of kind of on a global basis. The vast majority of our hosts on Airbnb are individual houses. They're not looking to drive 100% occupancy of all their listings.\"}],\n",
       " [{'summary_text': '\"We monitor it on local by local because what really matters is that we have great available listings in a specific market on a specific date\"'}],\n",
       " [{'summary_text': \"Brian Chesky: I think we're only scratching the surface to how this company becomes. I absolutely think that we can get to really solid double-digit revenue growth for many years to come.\"}],\n",
       " [{'summary_text': \"Airbnb is almost always better is when you're traveling with 3 or more people. It's the highest performing digital campaign we've ever done. And this is going to be the basis for a new -- major new marketing campaign next year.\"}],\n",
       " [{'summary_text': \"Airbnb is in 220 countries in the region, there's only a couple of countries where we even have penetration at rivals of United States. After that, U.K. a little bit, it really starts to tip down. And so we have like massive, massive opportunity and just by bringing Airbnb's playbook to these other countries.\"}],\n",
       " [{'summary_text': '\"I think the biggest strength I have as a CEO is not driving profitability even though we\\'ve done a really good job,\" Airbnb CEO says. \"I think it is literally inventing new products and services. That\\'s why we\\'ve hired so many great technologists, designers\"'}],\n",
       " [{'summary_text': \"We'll go to our next question from Doug Anmuth at JPMorgan. Doug: Just curious if you have a sense of kind of visibility and any kind of bookings into 2024. And perhaps maybe how that visibility compares now versus a year ago?\"}],\n",
       " [{'summary_text': \"David Stephenson: It's hard to completely pin down the root cause of any kind of softness or volatility. I think I'm feeling great about our overall playbook and plans, as kind of Brian has mentioned. And overall, I'm seeing solid demand for Airbnbs.\"}],\n",
       " [{'summary_text': \"Jed Kelly: Can you just give us further update on the regulations you talked about in the shareholder letter. And then Google announced a new update to their vacation rentals where they're essentially letting property managers show their price.\"}],\n",
       " [{'summary_text': 'Brian Chesky: 80% of our top 200 markets already have regulations on the books. These regulations, though they vary, generally have found workable solutions for home sharing.'}],\n",
       " [{'summary_text': \"Hotel price in New York are now up 8% year-over-year. A lot of people can't even afford to go there anymore. We are seeing work bookings in Jersey City and the perimeters around New York City.\"}],\n",
       " [{'summary_text': \"New York. paid $9 billion in hotel tax. So generally, it's gone fairly well. It is going to be notable that if you just read the news, you're always going to seem to be reading about these cities, something happen in New York.\"}],\n",
       " [{'summary_text': \"David Stephenson: We're not going to respond directly to any kind of specific thing that Google is doing. I think if you do step back though and remember that the vast majority of host on Airbnb or individual host, approximately 90% of them, are unique to Airbnb.\"}],\n",
       " [{'summary_text': \"Brian Chesky: We're just seeing a lot of strength in mobile bookings. 53% of our gross nights booked in the last quarter were on native mobile apps, essentially iOS and Android.\"}],\n",
       " [{'summary_text': 'Brian Wojcicki: How do you feel about the average prices on Airbnb today? Is there still room to kind of -- if you get those lower? And I guess as you talk about some of the marketing and advertising campaigns, do you think kind of travelers or consumers view Airbnb as a premium offering, a discount offering?'}],\n",
       " [{'summary_text': 'Brian Chesky: When we started Airbnb, our original tagline was a cheap affordable alternative to a hotel. Now once they used it, we used to say money as the hook but the experience is the reason you keep coming back.'}],\n",
       " [{'summary_text': \"Many of our hosts run at low enough occupancy and they always have that if they lower the price just a bit, they can sell more nights. And so we think there's a win-win where if we continue to encourage host to offer more competitive pricing, it's awin for guests but it's also a win for many of the host.\"}],\n",
       " [{'summary_text': \"Airbnb's offering really is one of the most unique and resilient models. We think that as we continue to be more affordable, we'll continue to stimulate more demand. We actually think there's a very high correlation of relationship between ADR and night growth.\"}],\n",
       " [{'summary_text': \"We're one of the most popular brands for people under 30 in travel. We're also very much a family travel brand. As we continue to get more affordable, I think that's going to continue to drive a lot more growth for us.\"}],\n",
       " [{'summary_text': \"Next, we'll go to Ron Josey at Citi. Ron, I wanted to ask a little bit about your comments on first-time bookers. I'm just trying to understand a little more on the drivers that are attracting these new bookers, Brian. And second question, just on probably with Experiences, there's any update there?\"}],\n",
       " [{'summary_text': 'Brian Chesky: The number one way people come to Airbnb is because a friend or a family member told them about Airbnb. After that, then we have a lot of earned media. We have some 500,000 to 600,000 press articles a year.'}],\n",
       " [{'summary_text': \"90% of our traffic is direct or unpaid. We do performance marketing but we think unlike other travel companies, it's not necessarily a way to buy customers. So a lot of it remains direct.\"}],\n",
       " [{'summary_text': '\"We should have some updates coming in the coming -- obviously, coming next year and beyond on this product. And you\\'ll see we\\'re continually investing in this product,\" he said.'}],\n",
       " [{'summary_text': 'Kevin Kopelman: Could you touch on your vision for building more of a travel community on Airbnb and maybe the time line you expect for rolling out some of the new community features.'}],\n",
       " [{'summary_text': \"Brian Chesky: I think one of the biggest visions that we have as a company isn't just to be a marketplace to become but to build literally quite literally a global travel community.\"}],\n",
       " [{'summary_text': 'Airbnb and OTAs are probably going to benefit more quickly from AI than, say, a hotel will. And so the transformation will happen at the digital surface sooner. Right now, customer service in Airbnb is a little bit of a different community.'}],\n",
       " [{'summary_text': \"Airbnb is really, really hard, especially compared to hotels. The problem is, imagine you have a Japanese host booking with -- hosting a German guest and there's a problem. There's a myriad of issues, there's no front desk, we can't go on-premise.\"}],\n",
       " [{'summary_text': 'With AI, there can be entirely different booking models. I think this is like a Cambrian moment for like the Internet or mobile for travel. They could ask you questions and they could offer you a significantly greater personalized service.'}],\n",
       " [{'summary_text': 'Justin Post: Supply is up 19%. How do you think about that as a leading indicator for room night growth? And how do you maybe accelerate night growth to capture that? And then the second question is on ADRs. Is that supply coming in higher or lower, similar ADRs?'}],\n",
       " [{'summary_text': \"David Stephenson: On the ADR side, it varies a little bit by market. In North America, we're seeing more of the prices come down. And in Europe, the ADRs have been a bit more elevated.\"}],\n",
       " [{'summary_text': \"The overall growth of Airbnb since 2019, nights growth has been relatively in line with the total growth of supply. strength of 19% listings growth is a great leading indicator of what we're capable of growing over time. So, I'm really bullish on our overall growth.\"}],\n",
       " [{'summary_text': \"Brian Chesky: I think that supply is even more important than it seems on the surface. Ultimately, when you're tiny and no one ever hears about you, one of the big levers is awareness. Once you're a brand like Airbnb that's known as really [indiscernible] used all over the world, so supply growth becomes a very important like long-term leading indicator.\"}],\n",
       " [{'summary_text': 'James Lee: Two questions here, Dave. I remember at the beginning of the year when you were guiding ADR down about mid-single digits. You were talking about leverage and like variable expenses like payment and cloud. I was just wondering where you are in that process, how much up to unlock going forward? And secondarily, on sales and marketing, it looks like supply is creating demand right now. Is it fair to assume we\\'re shifting more demand-side advertising going forward?\"'}],\n",
       " [{'summary_text': \"David Stephenson: We're not actually shifting more to demand-side marketing. The vast majority of our traffic is direct or unpaid. We use our search engine marketing as kind of a laser to focus on areas where maybe we have less demand.\"}],\n",
       " [{'summary_text': \"Our fixed cost growth discipline has been excellent, probably grow our fixed -- headcount this year, approximately 4%. So we're growing our head count and fixed expenses less than revenue. We continue to make great strides of improvement in our operations and support.\"}],\n",
       " [{'summary_text': 'Airbnb has been talking a lot about innovating on the search experience. Do you see a path where some of these features over the longer term like community in search drive enough differentiation that you could bring on more traditional supply?'}],\n",
       " [{'summary_text': 'Brian Chesky: We are very much supportive having hotel inventory on Airbnb. We acquired HotelTonight before the pandemic because we believe so much in it. New York still has a lot of traffic of people searching for New York.'}],\n",
       " [{'summary_text': \"I think our bread and butter for combinations are always going to be home. I think that's where our heart and soul is. We've just been prioritizing homes because we wanted to be really focused.\"}],\n",
       " [{'summary_text': 'Room nights up 19% with double-digit growth in all territories. Every week, we read about new STR regulations. Where is that supply growth really happening, especially in the kind of Western markets?'}],\n",
       " [{'summary_text': \"There's not a specific region where we're seeing it. I think maybe the biggest thing we've seen is that it's more broad-based on a global basis right now. We saw maybe some of it just late September and it's kind of been early October.\"}],\n",
       " [{'summary_text': 'I think those are outliers. about our overall regulatory landscape on a global basis. We have really good partnerships with many cities around the world and things like our City Portal and other things.'}],\n",
       " [{'summary_text': \"Brian Chesky: We're in like 100,000 cities around the world and for every headline you read, there's cities that actually have very workable solutions. We're actually seeing growth in supply across all types of markets.\"}],\n",
       " [{'summary_text': \"2/3 of the hosts that are using the pricing tool today, as you add new supply, you mentioned that ADRs of new supply is at a higher rate. Are those people more likely to use the discounting tools that you've kind of mentioned after they've listed before? And then maybe on the implications for take rate when you move into international markets, you're tracking towards -- over 50% of your rooms are going to be there. Is take rate can eventually just kind of bleed lower as that expands?\"}],\n",
       " [{'summary_text': 'Brian Chesky: Generally, new host adopt new tools at a higher rate than existing host. The ADR-related new host might also be related to the mix shift.'}],\n",
       " [{'summary_text': '\"I can hand over to you. Dave, I can handover to you,\" he said. \"I\\'ve got a few things I want to ask you to do.\"'}],\n",
       " [{'summary_text': \"David Stephenson: I think over time, the way we think about our take rate is that it's been very stable. We've actually made no underlying kind of recent changes to our absolute take rate. As we add more services and capabilities, that would be the way to further kind of monetize Airbnb.\"}],\n",
       " [{'summary_text': 'Brian Chesky: And you could -- theoretically, you could argue the inverse which is to say that as the expanded new markets, they might be more interested in new services that we can offer because hosting is newer to them. So as we expand in new markets and as we expansion to new host services, we want to make sure that new host and new markets are percent of those opportunities.'}],\n",
       " [{'summary_text': 'Brian Chesky: Just to recap, revenue was $3.4 billion, 18% higher than a year ago. Net income and adjusted EBITDA were both Q3 records. trailing 12-month free cash flow was $4.2 billion.'}]]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_text = \" \".join([x[0]['summary_text'] for x in summ_ls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    "\n",
    "texts = text_splitter.create_documents([summarized_text])\n",
    "split_texts = text_splitter.split_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.prompts import PromptTemplate\n",
    "local_path = (\n",
    "    \"../llm_models/gpt4all-falcon-q4_0.gguf\"  # replace with your desired local file path\n",
    ")\n",
    "\n",
    "# Callbacks support token-wise streaming\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "\n",
    "# Verbose is required to pass to the callback manager\n",
    "llm = GPT4All(model=local_path, callbacks=callbacks, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 93553.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load\n",
    "\n",
    "# Define your model to import\n",
    "model_name = \"mlx-community/Meta-Llama-3-8B-Instruct-4bit\"\n",
    "\n",
    "# Loading model\n",
    "model, tokenizer = load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:   0%|          | 0/4 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3-8B\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/transformers/modeling_utils.py:3715\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3712\u001b[0m \u001b[38;5;66;03m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[1;32m   3713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[1;32m   3714\u001b[0m     \u001b[38;5;66;03m# resolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[0;32m-> 3715\u001b[0m     resolved_archive_file, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_checkpoint_shard_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3716\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3717\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3718\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3719\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3720\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3722\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3723\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3724\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3725\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3726\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3727\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3728\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3731\u001b[0m     is_safetensors_available()\n\u001b[1;32m   3732\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resolved_archive_file, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m   3733\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m resolved_archive_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3734\u001b[0m ):\n\u001b[1;32m   3735\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(resolved_archive_file, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/transformers/utils/hub.py:1079\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shard_filename \u001b[38;5;129;01min\u001b[39;00m tqdm(shard_filenames, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading shards\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1078\u001b[0m         \u001b[38;5;66;03m# Load from URL\u001b[39;00m\n\u001b[0;32m-> 1079\u001b[0m         cached_filename \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshard_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;66;03m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;66;03m# we don't have to catch them here.\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/transformers/utils/hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    417\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m         message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m custom_message\n\u001b[1;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/huggingface_hub/file_download.py:1240\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m   1221\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m   1222\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1237\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1238\u001b[0m     )\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/huggingface_hub/file_download.py:1389\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1387\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1392\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1398\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1399\u001b[0m     _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/huggingface_hub/file_download.py:1915\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1912\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m   1913\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m-> 1915\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1916\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1922\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1924\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1925\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/huggingface_hub/file_download.py:549\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    547\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 549\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[1;32m    550\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    551\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/urllib3/response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp):\n\u001b[0;32m--> 628\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m    631\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/urllib3/response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    564\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 567\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m         flush_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/urllib3/response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/http/client.py:465\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    464\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 465\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"mps\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"refine\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "\n",
    "# Map\n",
    "map_template = \"\"\"The following is a set of summarized transcripts\n",
    "{docs}\n",
    "Based on this list of docs, generate a summary of the main points discussed.\n",
    "Don't return conversation between people, just distill the main points.\n",
    "Helpful Answer:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce\n",
    "reduce_template = \"\"\"The following is set of summaries of transcripts:\n",
    "{docs}\n",
    "Take these and distill it into a final, consolidated summary of the main challenges and successes. \n",
    "Helpful Answer:\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "\n",
    "\n",
    "# Run chain\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"docs\"\n",
    ")\n",
    "\n",
    "# Combines and iteratively reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored on calling ctypes callback function: <function LLModel._prompt_callback at 0x2c0bfab00>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/gpt4all/pyllmodel.py\", line 479, in _prompt_callback\n",
      "    @staticmethod\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Tesla's Q2 2024 results were announced at around 3:00 p.m. Central Time.\n",
      "2. The Update Deck was published at the same link as this webcast.\n",
      "3. During this call, the company will discuss its business outlook and make forward-looking statements.\n",
      "4. Actual events or results could differ materially due to a number of risks and uncertainties, including those mentioned in the company's most recent filings with the SEC.\n",
      "5. During the question-and-answer portion of today's call, please limit yourself to one question and one follow-up.\n",
      "6. Please use the raise hand button to join the question queue.\n",
      "7. Before we jump into Q&A, Elon Musk will make some opening remarks.\n",
      "8. The company's Q2 2024 results were announced at around 3:00 p.m. Central Time.\n",
      "9. The Update Deck was published at the same link as this webcast.\n",
      "10. During this call, the company will discuss its business outlook and make forward-looking statements.\n",
      "11. Actual events or results could differ materially due to a number of risks and uncertainties, including those mentioned in the company's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored on calling ctypes callback function: <function LLModel._callback_decoder.<locals>._raw_callback at 0x2c0bfb760>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/gpt4all/pyllmodel.py\", line 438, in _raw_callback\n",
      "    def _raw_callback(token_id: int, response: bytes) -> bool:\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The main points discussed in the list of documents are:\n",
      "\n",
      "* Tesla achieved record quarterly revenues and profits for the energy business in Q2 2023.\n",
      "* The company is investing in many future projects, including AI training and inference, and infrastructure to support future products.\n",
      "* Tesla's CEO, Elon Musk, believes that EVs are best for customers and that the world is headed for a fully electrified transport, not just the cars, but also aircrafts and boats.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%time\n",
    "result = map_reduce_chain.run(split_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAirbnb's main challenges include increasing supply and improving search experience, while its main successes include focusing on home rentals, innovating on the platform, and expanding into new markets. The company is also exploring opportunities in other areas like vacation rentals and experiences.\""
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
