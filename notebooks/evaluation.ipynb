{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finance.llm.summarization import TranscriptLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TranscriptLoader.get_texts() missing 1 required positional argument: 'preprocess'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m      4\u001b[0m where_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$and\u001b[39m\u001b[38;5;124m'\u001b[39m:[\n\u001b[1;32m      5\u001b[0m   {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymbol\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m      6\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$in\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTSLA\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m   }]\n\u001b[1;32m     11\u001b[0m  }\n\u001b[1;32m     13\u001b[0m result \u001b[38;5;241m=\u001b[39m dataloader\u001b[38;5;241m.\u001b[39mquery_client(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTESLA transcripts\u001b[39m\u001b[38;5;124m'\u001b[39m, n_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mwhere_dict)\n\u001b[0;32m---> 14\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: TranscriptLoader.get_texts() missing 1 required positional argument: 'preprocess'"
     ]
    }
   ],
   "source": [
    "dataloader = TranscriptLoader(collection_name = \"transcripts_mililm_l6_v3\", embedding_model = \"all-MiniLM-L6-v2\")\n",
    "dataloader.instantiate_client()\n",
    "\n",
    "where_dict = {'$and':[\n",
    "  {'symbol': {\n",
    "   \"$in\": ['TSLA']}\n",
    "  }, \n",
    "  {'year': {\n",
    "\"$gt\": 2023}\n",
    "  }]\n",
    " }\n",
    "\n",
    "result = dataloader.query_client('TESLA transcripts', n_results=1, **where_dict)\n",
    "texts = dataloader.get_texts(result, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout_final_only import (FinalStreamingStdOutCallbackHandler)\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "\n",
    "from langchain.chains import (\n",
    "    StuffDocumentsChain,\n",
    "    LLMChain,\n",
    "    ReduceDocumentsChain,\n",
    "    MapReduceDocumentsChain,\n",
    ")\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.llms import OpenAI\n",
    "\n",
    "# This controls how each document will be formatted. Specifically,\n",
    "# it will be passed to `format_document` - see that function for more\n",
    "# details.\n",
    "document_prompt = PromptTemplate(\n",
    "    input_variables=[\"page_content\"],\n",
    "     template=\"{page_content}\"\n",
    ")\n",
    "document_variable_name = \"context\"\n",
    "\n",
    "\n",
    "callbacks = [FinalStreamingStdOutCallbackHandler()] \n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "llm = GPT4All(model='../llm_models/gpt4all-falcon-q4_0.gguf', callbacks=callbacks, verbose=True)# The prompt here should take as an input variable the\n",
    "# `document_variable_name`\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"The following is a part of a transcript for a company containing it's financial performance.\n",
    "        {context}\n",
    "        Summarize the text focussing on challenges and successes and elaborate on the most important details.\n",
    "        Helpful Answer:\"\"\"\n",
    ")\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# We now define how to combine these summaries\n",
    "reduce_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You will receive a summarized text with challenges and successes for a company.\n",
    "        {context}\n",
    "        Firstly extract the year and quarter for which the transcript is for and list it at the top.\n",
    "        Secondly based on this set of docs, please summarize and list the following:\n",
    "        - main challenges\n",
    "        - main successes\n",
    "        Helpful Answer:\"\"\"\n",
    ")\n",
    "reduce_llm_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_llm_chain,\n",
    "    document_prompt=document_prompt,\n",
    "    document_variable_name=document_variable_name\n",
    ")\n",
    "\n",
    "\n",
    "# collapse_documents_chain which is specifically aimed at collapsing documents BEFORE\n",
    "# the final call.\n",
    "prompt = PromptTemplate.from_template(\n",
    "            \"Collapse this content while keeping the most important information: {context}\"\n",
    "        )\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "collapse_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_prompt=document_prompt,\n",
    "    document_variable_name=document_variable_name\n",
    ")\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    collapse_documents_chain=collapse_documents_chain,\n",
    "    token_max=2000\n",
    ")\n",
    "chain = MapReduceDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finance.llm.transcript import TranscriptLoader\n",
    "\n",
    "dataloader = TranscriptLoader(collection_name = \"transcripts_mililm_l6_v3\", embedding_model = \"all-MiniLM-L6-v2\")\n",
    "dataloader.instantiate_client()\n",
    "    \n",
    "where_dict = {'$and':[\n",
    "              {'symbol': {\n",
    "                       \"$in\": ['TSLA']}\n",
    "              }, \n",
    "              {'year': {\n",
    "                        \"$gt\": 2023}\n",
    "              }]\n",
    "         }\n",
    "    \n",
    "result = dataloader.query_client('TESLA transcripts', n_results=1, **where_dict)\n",
    "texts = dataloader.get_texts(result, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to have a fully autonomous factory by the end of next year. We're also making progress on our Giga Berlin factory, which will be the largest factory in the world when it's completed. We're also making progress on our Giga Shanghai factory, which will be the largest factory in the world when it's completed. And we're also making progress on our Giga Austin factory, which will be the largest factory in the world when it's completed. So we're making progress on all of these things. We're also making progress on our Giga Shanghai factory, which will be the largest factory in the world when it's completed. And we're also making progress on our Giga Austin factory, which will be the largest factory in the world when it's completed. So we're making progress on all of these things. We're also making progress on our Giga Shanghai factory, which will be the largest factory in the world when it's completed. And we're also making progress on our Giga Austin factory, which will be the largest factory in the world when it's completed. So we're making progress on all of these things. We're also making progress on our to have a lot more of that going forward. So we're really excited about the future.\n",
      "Collapse this content: \n",
      "We are excited to announce that we will be hosting a product unveil event on May 25th at 10am CST. This event will showcase our latest advancements in autonomous driving and robotics, including our new Robotaxi and the expansion of Giga Texas. We hope you can join us for this exciting event! quarter. The energy business is now a significant contributor to our overall revenue and profits. We are also making progress on the Gigafactory 4 expansion, which will be the largest of its kind in the world. We expect to start construction in Q3 and have a target to reach full capacity by the end of 2022. We are also making progress on the Shanghai factory, which is now fully operational and producing Model Ys. We are also making progress on the Berlin factory, which is now fully operational and producing Model Ys as well. We are also making progress on the Austin factory, which is now fully operational and producing Model Ys. We are also making progress on the Buffalo factory, which is now fully operational and producing Model Ys. We are also making progress on the Austin factory, which is now fully operational and producing Model Ys. We are also making progress on the Shanghai factory, which is now fully operational and producing Model Ys. We are also making progress on the Berlin factory, which is now fully operational and producing Model Ys. We are also making progress on the Austin factory, which is now fully operational and producing Model Ys. We are also making progress on the Buffalo factory, which is now fully operational and producing Model Y quarter.\n",
      "Collapse this content: \n",
      "and are also getting impacted by varying amounts of tariffs on both raw materials and finished goods. While our teams are working feverishly to offset these, unfortunately it may have an impact on the cost in the near-term. We previously talked about the potential of the energy business and now feel excited that the foundation that was laid over time is bearing the expected results. Energy storage deployments more than doubled with contribution not just from Megapack, but also Powerwall, resulting in record revenues and profit for the quarter. program.\n",
      "Travis Axelrod: Thank you very much. Our final question is, what is the current status of the Shanghai Gigafactory?\n",
      "Elon Musk: The Shanghai Gigafactory is on track for a Q4 2022 opening, and we're excited to bring our latest innovations to China and the world. We've made significant progress in hiring and training local talent, and we're working closely with the government to ensure a smooth transition.\n",
      "Travis Axelrod: Thank you very much. And that concludes our Q2 2023 earnings call. Thank you for joining us today. target.\n",
      "We are making progress towards our goal of producing a Cybertruck with dry cathode process made on mass production equipment. This is a significant technical milestone for us and we are proud of it. We are on track for production launch in Q4 and this will enable cell cost to be significantly below available alternatives, which was the original goal of the 46,80 target. supportive of it. And as for Grok, we're still working on integrating Grok into our software stack. It's a complex process and it's going to take some time.\n",
      "Travis Axelrod: All right. Thank you very much. supportive of it.\n",
      "Rod: Okay. And then, just to follow up on that, do you have any updates on the timeline for when you expect to be able to deploy xAI in the Tesla fleet?\n",
      "Elon Musk: I think we're making good progress on that. We're still working on some of the regulatory approvals and things like that. But I think we're making good progress.\n",
      "Rod: Okay. And then, just to follow up on that, do you have any updates on the timeline for when you expect to be able to deploy xAI in the Tesla fleet?\n",
      "Elon Musk: I think we're making good progress on that. We're still working on some of the regulatory approvals and things like that. But I think we're making good progress.\n",
      "Rod: Okay. And then, just to follow up on that, do you have any updates on the timeline for when you expect to be able to deploy xAI in the Tesla fleet?\n",
      "Elon Musk: I think we're making good progress on that. We're still working on some of the regulatory approvals and things like that. But I think we're making good progress.\n",
      "Rod: Okay. And then, just about the future of Tesla and the potential for new vehicles, what are some of the key considerations that you're thinking about in terms of the technology that will be required to make those vehicles successful?\n",
      "Elon Musk: Well, I think it's a combination of things. One is the battery technology, which is really important. We're working on a lot of different battery technologies, including solid-state batteries, which are much more efficient than lithium-ion batteries. And we're also working on other battery technologies like sodium-sulfur batteries, which are even more efficient than solid-state batteries. So that's one thing.\n",
      "Another thing is the autonomous driving technology, which is really important for our full self-driving capability. We're working on a lot of different things there, including the ability to handle all sorts of different scenarios and conditions, like rain, snow, fog, and so on. And we're also working on the ability to handle things like traffic lights and stop signs and other things that are not currently possible with our current technology.\n",
      "So I think it's a combination of those things, as well as the ability to handle different types of terrain and different types of driving scenarios, that will be important about the future of transportation, what do you see as the most important trends or developments that will drive the industry forward?\n",
      "\n",
      "Elon Musk: Well, I think there are a lot of different things that are going to be important. One thing that's going to be really important is the transition to electric vehicles. We're already seeing a lot of progress on that front, and I think that's going to continue to accelerate.\n",
      "\n",
      "Another thing that's going to be really important is the development of autonomous driving technology. That's going to be a huge game-changer for transportation, and I think we're going to see a lot of progress on that front in the next few years.\n",
      "\n",
      "And then there are also things like space travel, which I think is going to become more and more important as we look to the future. So those are just a few examples of the different trends and developments that I think will be driving the industry forward. So I think distributed compute is a very obvious thing to do. And I think it's going to be a very big part of our future.\n",
      "Travis Axelrod: Thank you, Ben. Next question comes from Chris Schillinger from Goldman Sachs. Chris, please go ahead and unmute yourself.\n",
      "Chris Schillinger: Hi. Thanks for taking my question. When we think about the potential for Tesla to become a major player in the autonomous trucking space, what are some of the key factors that you think will drive adoption? And how do you see the industry evolving over the next few years?\n",
      "Elon Musk: Yeah, so I think there's a lot of factors that will drive adoption. One is cost. So, as we get to the point where the cost of our trucks is comparable to the cost of a human driver, and then it's cheaper to have a truck than a human driver, then you're going to see a lot of adoption. And I think that's going to happen pretty quickly. I mean, we're already seeing a lot of adoption in Europe, where there are no human drivers. And I think that's going to continue to grow.\n",
      "Another factor is safety. So, as\n",
      "Collapse this content: 100 hours plus per week of AI compute, AI advanced compute from the fleet, from the vehicles and probably some percentage from the humanoid robots that it would make sense to do distributed inference. And if you're -- if there's a fleet of at some point a 100 million vehicles with AI5 and beyond, because you have AI 6 and 7 and whatnot, and there may be billions of humanoid robots that is just a staggering amount of inference compute or that could be used for general purposes at computing., we're not sure what the tariffs will be. We're not sure what the quotas will be. We're not sure what the quotas will be. So we're still evaluating what is the best alternate manage all this just on the examination by the European authorities.\n",
      "Travis Axelrod: All right, thank you. The next question comes from Jeff Osborne from JPMorgan. Jeff, please go ahead and unmute yourself.\n",
      "Jeff Osborne: Hi, thanks for taking my question. I wanted to ask about the potential for FSD to be used in ride-sharing services. You mentioned that it's not a priority for you right now, but do you see any potential for FSD to be used in ride-sharing services? And if so, what kind of partnerships or collaborations would you need to make that happen?\n",
      "Elon Musk: Yes. I think there is a lot of potential for FSD to be used in ride-sharing services. In fact, we're already working with some of the major ride-sharing companies on this. And I think it's going to be a very important part of our future. We're also working with some of the major ride-sharing companies on this. And I think, we're still in the process of building our own factory in Europe. So, we're not going to be able to get rid of the import completely. But we're trying to minimize it as much as possible.\n",
      "We were adjusting our import strategy out of China into Europe. However, we started building right-hand models from a model-wise approach in Berlin and delivered them in the UK. We are still importing Model 3s into Europe, but we will keep adjusting as needed. We are still evaluating what is the best alternate manage all this just on the examination by the European authorities. Like I said, we're still in the process of building our own factory in Europe. So, we're not going to be able to get rid of the import completely. But we're trying to minimize it as much as possible. self-driving car program in 2019. And I'm just curious, what are the key differences between the regulatory environment in the U.S. and Europe that would allow Tesla to deploy a self-driving car in Europe, but not in the U.S.?\n",
      "Elon Musk: Well, I think the main difference is that in Europe, there is a lot more regulation around autonomous vehicles. And so, it's easier for us to get regulatory approval for our vehicles. In the U.S., there is a lot less regulation around autonomous vehicles. And so, it's harder for us to get regulatory approval for our vehicles. But I think that's changing. I think that as we continue to deploy and demonstrate the safety of our vehicles, regulators will become more comfortable with allowing us to deploy them on the road.\n",
      "Travis Axelrod: Thank you. Next question comes from David from JPMorgan. David, please go ahead and unmute yourself.\n",
      "David Tamberrino: Hi, everyone. Thanks for taking my questions. I was just curious, what are the key differences between the regulatory environment in the U.S. and Europe that would allow Tesla to deploy a self-driving car in Europe, but not and they're still selling cars in Europe. So, I'm curious, what's the difference between the regulatory environment in Europe versus the US?\n",
      "Travis Axelrod: Sure, George. The regulatory environment for autonomous vehicles can vary significantly between countries. In the United States, there are a number of different agencies that regulate autonomous vehicle technology, including the National Highway Traffic Safety Administration (NHTSA) and the Federal Aviation Administration (FAA). These agencies work together to develop regulations and standards for autonomous vehicle technology.\n",
      "\n",
      "In Europe, there is also a regulatory framework in place for autonomous vehicles, but it varies by country. For example, in Germany, the Federal Ministry of Transport and Digital Infrastructure is responsible for regulating autonomous vehicle technology. In the United Kingdom, the Department for Transport is responsible for regulating autonomous vehicle technology.\n",
      "\n",
      "The regulatory environment can have a significant impact on the development and deployment of autonomous vehicle technology. For example, different countries may have different requirements for testing and certification of autonomous vehicles, which can affect the pace at which these technologies are adopted.\n",
      "\n",
      "Travis Axelrod: And to answer your question, George, the regulatory environment in Europe is generally more supportive of autonomous vehicle technology than it is in the United States. update, we can just deploy a fleet of cars that are able to do this. And then, you know, we'll have to work with local governments and regulators to make sure that they're comfortable with it. But it's not like we're going to be doing this in the middle of nowhere. We're going to be doing this in places where there's a lot of traffic and a lot of people. And so, you know, we'll have to work with local governments to make sure that they're comfortable with it. But it's not like we're going to be doing this in the middle of nowhere. We're going to be doing this in places where there's a lot of traffic and a lot of people. And so, you know, we'll have to work with local governments to make sure that they're comfortable with it. But it's not like we're going to be doing this in the middle of nowhere. We're going to be doing this in places where there's a lot of traffic and a lot of people. And so, you know, we'll have to work with local governments to make sure that they're comfortable with it. But update, you can have a fleet of cars that are all operating at the same time, and they can be in different places, and they can be in different states, and they can be in different countries, and they can be in different cities, and they can be in different neighborhoods, and they can be in different buildings, and they can be in different rooms, and they can be in different floors, and they can be in different rooms, and they can be in different floors, and they can be in different rooms, and they can be in different floors, and they can be in different rooms, and they can be in different floors, and they can be in different rooms, and they can be in different floors, and they can be in different rooms, and they can be in different floors, and they can be in different rooms, and they can be in different floors, and they can be in different rooms, and they can be in different floors, and they can be in different rooms, and they can be in different floors, and they can be in different rooms, and they can be in different floors, and they can be in different rooms, and they can be in different floors, and they can be in different rooms, and they can be in different So, we're seeing a lot of growth in that space.\n",
      "Travis Axelrod: Thank you. Our next question comes from John from Goldman Sachs. John, please unmute yourself.\n",
      "John: Hi, thanks for taking my question. I wanted to follow up on the question about the -- the potential for the company to be a platform for other companies to build on top of. Can you talk about how you're thinking about that and what kind of partnerships you might be looking to form in that space?\n",
      "Elon Musk: Yeah, so we're definitely looking at building out a platform. And I think it's going to be a very important part of our business. We're already seeing a lot of interest from other companies who want to build on top of our platform. And we're also seeing a lot of interest from other companies who want to use our platform as a way to get access to our customers and our network. So, we're definitely looking at building out a platform.\n",
      "Vaibhav Taneja: Yes, and I think the key thing is that we're not just building a platform for other companies to build on top of. We're also building a platform for ourselves to build So, it's not just about the number of GPUs, but also the amount of new pipeline that's being added to the data centers.\n",
      "Collapse this content: \n",
      "Vaibhav Taneja: Yes, I mean just even on the AI computer side, right? These GPUs are really powerful already and the amount of new pipeline, which we're getting for people for data center backup and things like that is increasing at a pretty large scale. So, it's not just about the number of GPUs, but also the amount of new pipeline that's being added to the data centers.\n",
      "Collapse this content: \n",
      "Vaibhav Taneja: Yes, I mean just even on the AI computer side, right? These GPUs are really powerful already and the amount of new pipeline, which we're getting for people for data center backup and things like that is increasing at a pretty large scale. So, it's not just about the number of GPUs, but also the amount of new pipeline that's being added to the data centers.\n",
      "Collapse this content: \n",
      "Vaibhav Taneja: Yes, I mean just even on the AI computer side, right? These GPUs are really powerful already and focus on autonomy. We're working on it. We're making progress. And I think that's going to be the key to our success in the long run.\n",
      "Travis Axelrod: Thank you, Colin. The next question comes from Jefferies. Jefferies, please unmute yourself.\n",
      "Jefferies Analyst: Hi, thank you for taking my questions. Can you talk about the progress you're making on the battery front and how that's impacting your overall cost structure? And then, can you talk about the potential for a new factory in Europe and what that might mean for your production capacity?\n",
      "Elon Musk: Yes, Jefferies. So, on the battery front, we are making progress. We have a lot of things going on there. We're working on a lot of different battery technologies. We're working on a lot of different battery chemistries. We're working on a lot of different battery cell architectures. And we're working on a lot of different battery manufacturing processes. So, we're making progress on all of those fronts. And we're also working on a lot of different battery technologies, including solid state batteries, which are much more efficient than the current lithium-ion batteries that focus on the cost structure. We want to make sure that we are not just growing for growth's sake. We want to make sure that we are growing in a healthy way. And that is what we are focused on.\n",
      "We want our business to grow in a healthy and sustainable manner without relying on subsidies or external support. This approach has been the guiding principle for our decision-making processes, including battery costs. We strive to operate efficiently and effectively even in challenging economic conditions. Our focus on cost structure and sustainability is reflected in our commitment to responsible growth. Thank you, Travis."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4371 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the technology gets better and better, you're going to see more and more trucks on the road. And I think that's going to be a big driver of adoption. And then there are also things like regulatory factors. So, as governments start to mandate the use of autonomous trucks, you're going to see more and more adoption.\n",
      "So, I think all of those things will drive adoption over the next few years. And I think on top of. And so, we're going to be able to use our own data and insights to improve our own products and services.\n",
      "Travis Axelrod: Thank you. Our next question comes from David from JPMorgan. David, please unmute yourself.\n",
      "David: Hi, thanks for taking my question. I wanted to follow up on the question about the potential for the company to be a platform for other companies to build on top of. Can you talk about how you're thinking about that and what kind of partnerships you might be looking to form in that space?\n",
      "Elon Musk: Yeah, so we're definitely looking at building out a platform. And I think it's going to be a very important part of our business. We're already seeing a lot of interest from other companies who want to build on top of our platform. And we're also seeing a lot of interest from other companies who want to use our platform as a way to get access to our customers and The next question comes from Goldman Sachs. Goldman Sachs, please unmute yourself.\n",
      "Goldman Sachs Analyst: Hi, thank you for taking my questions. Can you talk about the progress you're making on the battery front and how that's impacting your overall cost structure? And then, can you talk about the potential for a new factory in Europe and what that might mean for your production capacity?\n",
      "Elon Musk: Yes, Goldman Sachs. So, on the battery front, we are making progress. We have a lot of things going on there. We're working on a lot of different battery technologies. We're working on a lot of different battery chemistries. We're working on a lot of different battery cell architectures. And we're working on a lot of different battery manufacturing processes. So, we're making progress on all of those fronts. And we're also working on a lot of different battery technologies, including solid state batteries, which are much more efficient than the current lithium-ion batteries that\n",
      "\n",
      " focus on the cost structure. We want to make sure that we are not just growing for growth's sake. We want to make sure that we are growing in a healthy way. And that is what we are focused on\n",
      "Based on the provided text, here's a summary of the main challenges and successes for Tesla:\n",
      "\n",
      "* Challenges:\n",
      "\t+ High costs and low profitability\n",
      "\t+ Dependence on government subsidies\n",
      "\t+ Competition from traditional automakers\n",
      "\t+ Difficulty in scaling production\n",
      "\t+ Complex regulatory environment\n",
      "\t+ High employee turnover rate\n",
      "* Successes:\n",
      "\t+ Strong brand recognition and customer loyalty\n",
      "\t+ Rapid growth in sales and revenue\n",
      "\t+ Expansion into new markets and product lines\n",
      "\t+ Innovative technology and features\n",
      "\t+ Strong financial performance and profitability\n",
      "\t+ Successful launch of the Model 3 and Model Y\n",
      "\t+ Strong partnerships with suppliers and partners\n",
      "\t+ Strong focus on sustainability and environmental responsibility\n",
      "\n",
      "Note: The above summary is based on a general understanding of the provided text and may not be comprehensive or accurate."
     ]
    }
   ],
   "source": [
    "output = chain.invoke(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/finance/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.4213]), tensor([0.5115]), tensor([0.4621]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bert_score import BERTScorer\n",
    "from rouge import Rouge\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = BERTScorer(model_type='bert-base-uncased')\n",
    "\n",
    "orig = result['documents'][0][0]\n",
    "output = open('output_raw_llm.txt','r').read()\n",
    "P, R, F1 = scorer.score([orig], [output]) \n",
    "P, R, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.421326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.511537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.462069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "Precision  0.421326\n",
       "Recall     0.511537\n",
       "F1         0.462069"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'Precision':P, 'Recall':R, 'F1':F1}).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 Precision: 0.8246, Recall: 0.0109, F1: 0.0214\n",
      "ROUGE-2 Precision: 0.2212, Recall: 0.0029, F1: 0.0057\n",
      "ROUGE-L Precision: 0.5000, Recall: 0.0066, F1: 0.0130\n"
     ]
    }
   ],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(orig, output)\n",
    "print(f\"ROUGE-1 Precision: {scores['rouge1'].precision:.4f}, Recall: {scores['rouge1'].recall:.4f}, F1: {scores['rouge1'].fmeasure:.4f}\")\n",
    "print(f\"ROUGE-2 Precision: {scores['rouge2'].precision:.4f}, Recall: {scores['rouge2'].recall:.4f}, F1: {scores['rouge2'].fmeasure:.4f}\")\n",
    "print(f\"ROUGE-L Precision: {scores['rougeL'].precision:.4f}, Recall: {scores['rougeL'].recall:.4f}, F1: {scores['rougeL'].fmeasure:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary 1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rouge-1 (r)</th>\n",
       "      <td>0.528736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge-1 (p)</th>\n",
       "      <td>0.023650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge-1 (f)</th>\n",
       "      <td>0.045276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge-2 (r)</th>\n",
       "      <td>0.116667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge-2 (p)</th>\n",
       "      <td>0.002231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge-2 (f)</th>\n",
       "      <td>0.004379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge-l (r)</th>\n",
       "      <td>0.482759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge-l (p)</th>\n",
       "      <td>0.021594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge-l (f)</th>\n",
       "      <td>0.041339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Summary 1\n",
       "Metric                \n",
       "rouge-1 (r)   0.528736\n",
       "rouge-1 (p)   0.023650\n",
       "rouge-1 (f)   0.045276\n",
       "rouge-2 (r)   0.116667\n",
       "rouge-2 (p)   0.002231\n",
       "rouge-2 (f)   0.004379\n",
       "rouge-l (r)   0.482759\n",
       "rouge-l (p)   0.021594\n",
       "rouge-l (f)   0.041339"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to calculate the Rouge score\n",
    "def get_rouge_scores(text1, text2):\n",
    "    rouge = Rouge()\n",
    "    return rouge.get_scores(text1, text2)\n",
    "\n",
    "\n",
    "rouge_scores_out = []\n",
    "\n",
    "# Calculate the ROUGE scores for both summaries using reference\n",
    "eval_1_rouge = get_rouge_scores(orig, output)\n",
    "\n",
    "for metric in [\"rouge-1\", \"rouge-2\", \"rouge-l\"]:\n",
    "    for label in ['r', 'p', 'f']:\n",
    "        eval_1_score = eval_1_rouge[0][metric][label[0].lower()]\n",
    "\n",
    "        row = {\n",
    "            \"Metric\": f\"{metric} ({label})\",\n",
    "            \"Summary 1\": eval_1_score,\n",
    "        }\n",
    "        rouge_scores_out.append(row)\n",
    "\n",
    "\n",
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return [\n",
    "        \"background-color: lightgreen\" if v else \"background-color: white\"\n",
    "        for v in is_max\n",
    "    ]\n",
    "\n",
    "\n",
    "rouge_scores_out = (\n",
    "    pd.DataFrame(rouge_scores_out)\n",
    "    .set_index(\"Metric\")\n",
    ")\n",
    "\n",
    "rouge_scores_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'r': 0.5287356321839081,\n",
       "   'p': 0.02365038560411311,\n",
       "   'f': 0.045275589731544316},\n",
       "  'rouge-2': {'r': 0.11666666666666667,\n",
       "   'p': 0.0022314313037934334,\n",
       "   'f': 0.004379105043015708},\n",
       "  'rouge-l': {'r': 0.4827586206896552,\n",
       "   'p': 0.021593830334190232,\n",
       "   'f': 0.041338581857528575}}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_1_rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation prompt template based on G-Eval\n",
    "EVALUATION_PROMPT_TEMPLATE = \"\"\"\n",
    "You will be given one summary written for an article. Your task is to rate the summary on one metric.\n",
    "Please make sure you read and understand these instructions very carefully. \n",
    "Please keep this document open while reviewing, and refer to it as needed.\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "{criteria}\n",
    "\n",
    "Evaluation Steps:\n",
    "\n",
    "{steps}\n",
    "\n",
    "Example:\n",
    "\n",
    "Source Text:\n",
    "\n",
    "{document}\n",
    "\n",
    "Summary:\n",
    "\n",
    "{summary}\n",
    "\n",
    "Evaluation Form (scores ONLY):\n",
    "\n",
    "- {metric_name}\n",
    "\"\"\"\n",
    "\n",
    "# Metric 1: Relevance\n",
    "\n",
    "RELEVANCY_SCORE_CRITERIA = \"\"\"\n",
    "Relevance(1-5) - selection of important content from the source. \\\n",
    "The summary should include only important information from the source document. \\\n",
    "Annotators were instructed to penalize summaries which contained redundancies and excess information.\n",
    "\"\"\"\n",
    "\n",
    "RELEVANCY_SCORE_STEPS = \"\"\"\n",
    "1. Read the summary and the source document carefully.\n",
    "2. Compare the summary to the source document and identify the main points of the article.\n",
    "3. Assess how well the summary covers the main points of the article, and how much irrelevant or redundant information it contains.\n",
    "4. Assign a relevance score from 1 to 5.\n",
    "\"\"\"\n",
    "\n",
    "# Metric 2: Coherence\n",
    "\n",
    "COHERENCE_SCORE_CRITERIA = \"\"\"\n",
    "Coherence(1-5) - the collective quality of all sentences. \\\n",
    "We align this dimension with the DUC quality question of structure and coherence \\\n",
    "whereby \"the summary should be well-structured and well-organized. \\\n",
    "The summary should not just be a heap of related information, but should build from sentence to a\\\n",
    "coherent body of information about a topic.\"\n",
    "\"\"\"\n",
    "\n",
    "COHERENCE_SCORE_STEPS = \"\"\"\n",
    "1. Read the article carefully and identify the main topic and key points.\n",
    "2. Read the summary and compare it to the article. Check if the summary covers the main topic and key points of the article,\n",
    "and if it presents them in a clear and logical order.\n",
    "3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.\n",
    "\"\"\"\n",
    "\n",
    "# Metric 3: Consistency\n",
    "\n",
    "CONSISTENCY_SCORE_CRITERIA = \"\"\"\n",
    "Consistency(1-5) - the factual alignment between the summary and the summarized source. \\\n",
    "A factually consistent summary contains only statements that are entailed by the source document. \\\n",
    "Annotators were also asked to penalize summaries that contained hallucinated facts.\n",
    "\"\"\"\n",
    "\n",
    "CONSISTENCY_SCORE_STEPS = \"\"\"\n",
    "1. Read the article carefully and identify the main facts and details it presents.\n",
    "2. Read the summary and compare it to the article. Check if the summary contains any factual errors that are not supported by the article.\n",
    "3. Assign a score for consistency based on the Evaluation Criteria.\n",
    "\"\"\"\n",
    "\n",
    "# Metric 4: Fluency\n",
    "\n",
    "FLUENCY_SCORE_CRITERIA = \"\"\"\n",
    "Fluency(1-3): the quality of the summary in terms of grammar, spelling, punctuation, word choice, and sentence structure.\n",
    "1: Poor. The summary has many errors that make it hard to understand or sound unnatural.\n",
    "2: Fair. The summary has some errors that affect the clarity or smoothness of the text, but the main points are still comprehensible.\n",
    "3: Good. The summary has few or no errors and is easy to read and follow.\n",
    "\"\"\"\n",
    "\n",
    "FLUENCY_SCORE_STEPS = \"\"\"\n",
    "Read the summary and evaluate its fluency based on the given criteria. Assign a fluency score from 1 to 3.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_geval_score(\n",
    "    criteria: str, steps: str, document: str, summary: str, metric_name: str\n",
    "):\n",
    "    prompt = EVALUATION_PROMPT_TEMPLATE.format(\n",
    "        criteria=criteria,\n",
    "        steps=steps,\n",
    "        metric_name=metric_name,\n",
    "        document=document,\n",
    "        summary=summary,\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=5,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "evaluation_metrics = {\n",
    "    \"Relevance\": (RELEVANCY_SCORE_CRITERIA, RELEVANCY_SCORE_STEPS),\n",
    "    \"Coherence\": (COHERENCE_SCORE_CRITERIA, COHERENCE_SCORE_STEPS),\n",
    "    \"Consistency\": (CONSISTENCY_SCORE_CRITERIA, CONSISTENCY_SCORE_STEPS),\n",
    "    \"Fluency\": (FLUENCY_SCORE_CRITERIA, FLUENCY_SCORE_STEPS),\n",
    "}\n",
    "\n",
    "summaries = {\"Summary 1\": eval_summary_1, \"Summary 2\": eval_summary_2}\n",
    "\n",
    "data = {\"Evaluation Type\": [], \"Summary Type\": [], \"Score\": []}\n",
    "\n",
    "for eval_type, (criteria, steps) in evaluation_metrics.items():\n",
    "    for summ_type, summary in summaries.items():\n",
    "        data[\"Evaluation Type\"].append(eval_type)\n",
    "        data[\"Summary Type\"].append(summ_type)\n",
    "        result = get_geval_score(criteria, steps, excerpt, summary, eval_type)\n",
    "        score_num = int(result.strip())\n",
    "        data[\"Score\"].append(score_num)\n",
    "\n",
    "pivot_df = pd.DataFrame(data, index=None).pivot(\n",
    "    index=\"Evaluation Type\", columns=\"Summary Type\", values=\"Score\"\n",
    ")\n",
    "styled_pivot_df = pivot_df.style.apply(highlight_max, axis=1)\n",
    "display(styled_pivot_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
